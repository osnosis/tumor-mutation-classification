{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tumor Mutation Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will analyze the dataset which contains tumor gene mutations and their risk category, which corresponds to their risk of malignancy in the human condition. The dataset was hand-labeled and released by the team of clinical pathologists at Memorial Sloan Kettering in 2018. Our objective of this project is to fit the dataset into our machine learning models to predict the risk category while accounting for highly unbalanced classes. Several methods for text feature generation will be explored and the resulting features will be reduced using principle component analysis (PCA). We will then use the synthetic minority over-sampling technique (SMOTE) to resample the dataset to make the numbers of categories more even. The last step is to compare the machine learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "#from spacy.lemmatizer import Lemmatizer\n",
    "#lemmatizer = Lemmatizer()\n",
    "#nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
    "nlp = spacy.blank('en')\n",
    "#nlp.add_pipe(lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In this section we will load in all our data and format them to the appropriate data types. The text also needs to be cleaned of all non-legitimate words, such as figure references and parentheses. This will be accomplished using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up packages for loading in data\n",
    "client = boto3.client('s3') #low-level functional API\n",
    "\n",
    "resource = boto3.resource('s3') #high-level object-oriented API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in training data labels\n",
    "obj = client.get_object(Bucket='thinkful-capstone', Key='training_variants')\n",
    "stream = io.BytesIO(obj['Body'].read())\n",
    "training_variants = pd.read_csv(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID    Gene             Variation  Class\n",
      "0   0  FAM58A  Truncating Mutations      1\n",
      "1   1     CBL                 W802*      2\n",
      "2   2     CBL                 Q249E      2\n",
      "3   3     CBL                 N454D      3\n",
      "4   4     CBL                 L399V      4\n",
      "(3321, 4)\n"
     ]
    }
   ],
   "source": [
    "print(training_variants.head())\n",
    "print(training_variants.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID||Text\n",
      "0||Cyclin-dependent kinases (CDKs) regulate a variety of fundamental cellular processes. CDK10 stands out as one of the last orphan CDKs for which no activating cyclin has been identified and no kinase activity revealed. Previous work has shown that CDK10 silencing increases ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2)-driven activation of the MAPK pathway, which confers tamoxifen resistance to breast cancer cells. The precise mechanisms by which CDK10 modulates ETS2 activity, and more generally the functions of CDK10, remain elusive. Here we demonstrate that CDK10 is a cyclin-dependent kinase by identifying cyclin M as an activating cyclin. Cyclin M, an orphan cyclin, is the product of FAM58A, whose mutations cause STAR syndrome, a human developmental anomaly whose features include toe syndactyly, telecanthus, and anogenital and renal malformations. We show that STAR syndrome-associated cyclin M mutants are unable to interact with CDK10. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and in conferring tamoxifen resistance to breast cancer cells. CDK10/cyclin M phosphorylates ETS2 in vitro, and in cells it positively controls ETS2 degradation by the proteasome. ETS2 protein levels are increased in cells derived from a STAR patient, and this increase is attributable to decreased cyclin M levels. Altogether, our results reveal an additional regulatory mechanism for ETS2, which plays key roles in cancer and development. They also shed light on the molecular mechanisms underlying STAR syndrome.Cyclin-dependent kinases (CDKs) play a pivotal role in the control of a number of fundamental cellular processes (1). The human genome contains 21 genes encoding proteins that can be considered as members of the CDK family owing to their sequence similarity with bona fide CDKs, those known to be activated by cyclins (2). Although discovered almost 20 y ago (3, 4), CDK10 remains one of the two CDKs without an identified cyclin partner. This knowledge gap has largely impeded the exploration of its biological functions. CDK10 can act as a positive cell cycle regulator in some cells (5, 6) or as a tumor suppressor in others (7, 8). CDK10 interacts with the ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2) transcription factor and inhibits its transcriptional activity through an unknown mechanism (9). CDK10 knockdown derepresses ETS2, which increases the expression of the c-Raf protein kinase, activates the MAPK pathway, and induces resistance of MCF7 cells to tamoxifen (6).Here, we deorphanize CDK10 by identifying cyclin M, the product of FAM58A, as a binding partner. Mutations in this gene that predict absence or truncation of cyclin M are associated with STAR syndrome, whose features include toe syndactyly, telecanthus, and anogenital and renal malformations in heterozygous females (10). However, both the functions of cyclin M and the pathogenesis of STAR syndrome remain unknown. We show that a recombinant CDK10/cyclin M heterodimer is an active protein kinase that phosphorylates ETS2 in vitro. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and phospho-ERK expression levels and in inducing tamoxifen resistance in estrogen receptor (ER)+ breast cancer cells. We show that CDK10/cyclin M positively controls ETS2 degradation by the proteasome, through the phosphorylation of two neighboring serines. Finally, we detect an increased ETS2 expression level in cells derived from a STAR patient, and we demonstrate that it is attributable to the decreased cyclin M expression level observed in these cells.Previous SectionNext SectionResultsA yeast two-hybrid (Y2H) screen unveiled an interaction signal between CDK10 and a mouse protein whose C-terminal half presents a strong sequence homology with the human FAM58A gene product [whose proposed name is cyclin M (11)]. We thus performed Y2H mating assays to determine whether human CDK10 interacts with human cyclin M (Fig. 1 A–C). The longest CDK10 isoform (P1) expressed as a bait protein produced a strong interaction phenotype with full-length cyclin M (expressed as a prey protein) but no detectable phenotype with cyclin D1, p21 (CIP1), and Cdi1 (KAP), which are known binding partners of other CDKs (Fig. 1B). CDK1 and CDK3 also produced Y2H signals with cyclin M, albeit notably weaker than that observed with CDK10 (Fig. 1B). An interaction phenotype was also observed between full-length cyclin M and CDK10 proteins expressed as bait and prey, respectively (Fig. S1A). We then tested different isoforms of CDK10 and cyclin M originating from alternative gene splicing, and two truncated cyclin M proteins corresponding to the hypothetical products of two mutated FAM58A genes found in STAR syndrome patients (10). None of these shorter isoforms produced interaction phenotypes (Fig. 1 A and C and Fig. S1A).Fig. 1.In a new window Download PPTFig. 1.CDK10 and cyclin M form an interaction complex. (A) Schematic representation of the different protein isoforms analyzed by Y2H assays. Amino acid numbers are indicated. Black boxes indicate internal deletions. The red box indicates a differing amino acid sequence compared with CDK10 P1. (B) Y2H assay between a set of CDK proteins expressed as baits (in fusion to the LexA DNA binding domain) and CDK interacting proteins expressed as preys (in fusion to the B42 transcriptional activator). pEG202 and pJG4-5 are the empty bait and prey plasmids expressing LexA and B42, respectively. lacZ was used as a reporter gene, and blue yeast are indicative of a Y2H interaction phenotype. (C) Y2H assay between the different CDK10 and cyclin M isoforms. The amino-terminal region of ETS2, known to interact with CDK10 (9), was also assayed. (D) Western blot analysis of Myc-CDK10 (wt or kd) and CycM-V5-6His expression levels in transfected HEK293 cells. (E) Western blot analysis of Myc-CDK10 (wt or kd) immunoprecipitates obtained using the anti-Myc antibody. “Inputs” correspond to 10 μg total lysates obtained from HEK293 cells coexpressing Myc-CDK10 (wt or kd) and CycM-V5-6His. (F) Western blot analysis of immunoprecipitates obtained using the anti-CDK10 antibody or a control goat antibody, from human breast cancer MCF7 cells. “Input” corresponds to 30 μg MCF7 total cell lysates. The lower band of the doublet observed on the upper panel comigrates with the exogenously expressed untagged CDK10 and thus corresponds to endogenous CDK10. The upper band of the doublet corresponds to a nonspecific signal, as demonstrated by it insensitivity to either overexpression of CDK10 (as seen on the left lane) or silencing of CDK10 (Fig. S2B). Another experiment with a longer gel migration is shown in Fig. S1D.Next we examined the ability of CDK10 and cyclin M to interact when expressed in human cells (Fig. 1 D and E). We tested wild-type CDK10 (wt) and a kinase dead (kd) mutant bearing a D181A amino acid substitution that abolishes ATP binding (12). We expressed cyclin M-V5-6His and/or Myc-CDK10 (wt or kd) in a human embryonic kidney cell line (HEK293). The expression level of cyclin M-V5-6His was significantly increased upon coexpression with Myc-CDK10 (wt or kd) and, to a lesser extent, that of Myc-CDK10 (wt or kd) was increased upon coexpression with cyclin M-V5-6His (Fig. 1D). We then immunoprecipitated Myc-CDK10 proteins and detected the presence of cyclin M in the CDK10 (wt) and (kd) immunoprecipitates only when these proteins were coexpressed pair-wise (Fig. 1E). We confirmed these observations by detecting the presence of Myc-CDK10 in cyclin M-V5-6His immunoprecipitates (Fig. S1B). These experiments confirmed the lack of robust interaction between the CDK10.P2 isoform and cyclin M (Fig. S1C). To detect the interaction between endogenous proteins, we performed immunoprecipitations on nontransfected MCF7 cells derived from a human breast cancer. CDK10 and cyclin M antibodies detected their cognate endogenous proteins by Western blotting. We readily detected cyclin M in immunoprecipitates obtained with the CDK10 antibody but not with a control antibody (Fig. 1F). These results confirm the physical interaction between CDK10 and cyclin M in human cells.To unveil a hypothesized CDK10/cyclin M protein kinase activity, we produced GST-CDK10 and StrepII-cyclin M fusion proteins in insect cells, either individually or in combination. We observed that GST-CDK10 and StrepII-cyclin M copurified, thus confirming their interaction in yet another cellular model (Fig. 2A). We then performed in vitro kinase assays with purified proteins, using histone H1 as a generic substrate. Histone H1 phosphorylation was detected only from lysates of cells coexpressing GST-CDK10 and StrepII-cyclin M. No phosphorylation was detected when GST-CDK10 or StrepII-cyclin M were expressed alone, or when StrepII-cyclin M was coexpressed with GST-CDK10(kd) (Fig. 2A). Next we investigated whether ETS2, which is known to interact with CDK10 (9) (Fig. 1C), is a phosphorylation substrate of CDK10/cyclin M. We detected strong phosphorylation of ETS2 by the GST-CDK10/StrepII-cyclin M purified heterodimer, whereas no phosphorylation was detected using GST-CDK10 alone or GST-CDK10(kd)/StrepII-cyclin M heterodimer (Fig. 2B).Fig. 2.In a new window Download PPTFig. 2.CDK10 is a cyclin M-dependent protein kinase. (A) In vitro protein kinase assay on histone H1. Lysates from insect cells expressing different proteins were purified on a glutathione Sepharose matrix to capture GST-CDK10(wt or kd) fusion proteins alone, or in complex with STR-CycM fusion protein. Purified protein expression levels were analyzed by Western blots (Top and Upper Middle). The kinase activity was determined by autoradiography of histone H1, whose added amounts were visualized by Coomassie staining (Lower Middle and Bottom). (B) Same as in A, using purified recombinant 6His-ETS2 as a substrate.CDK10 silencing has been shown to increase ETS2-driven c-RAF transcription and to activate the MAPK \n"
     ]
    }
   ],
   "source": [
    "# Load in training data text articles\n",
    "obj = client.get_object(Bucket=\"thinkful-capstone\",Key=\"training_text\")\n",
    "raw_training_text = obj[\"Body\"].read()\n",
    "training_text = raw_training_text.decode('utf-8')\n",
    "print(training_text[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211296707\n",
      "205598350\n",
      "ID||Text\n",
      "0||Cyclin-dependent kinases (CDKs) regulate a variety of fundamental cellular processes. CDK10 stands out as one of the last orphan CDKs for which no activating cyclin has been identified and no kinase activity revealed. Previous work has shown that CDK10 silencing increases ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2)-driven activation of the MAPK pathway, which confers tamoxifen resistance to breast cancer cells. The precise mechanisms by which CDK10 modulates ETS2 activity, and more generally the functions of CDK10, remain elusive. Here we demonstrate that CDK10 is a cyclin-dependent kinase by identifying cyclin M as an activating cyclin. Cyclin M, an orphan cyclin, is the product of FAM58A, whose mutations cause STAR syndrome, a human developmental anomaly whose features include toe syndactyly, telecanthus, and anogenital and renal malformations. We show that STAR syndrome-associated cyclin M mutants are unable to interact with CDK10. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and in conferring tamoxifen resistance to breast cancer cells. CDK10/cyclin M phosphorylates ETS2 in vitro, and in cells it positively controls ETS2 degradation by the proteasome. ETS2 protein levels are increased in cells derived from a STAR patient, and this increase is attributable to decreased cyclin M levels. Altogether, our results reveal an additional regulatory mechanism for ETS2, which plays key roles in cancer and development. They also shed light on the molecular mechanisms underlying STAR syndrome.Cyclin-dependent kinases (CDKs) play a pivotal role in the control of a number of fundamental cellular processes. The human genome contains 21 genes encoding proteins that can be considered as members of the CDK family owing to their sequence similarity with bona fide CDKs, those known to be activated by cyclins. Although discovered almost 20 y ago, CDK10 remains one of the two CDKs without an identified cyclin partner. This knowledge gap has largely impeded the exploration of its biological functions. CDK10 can act as a positive cell cycle regulator in some cells or as a tumor suppressor in others. CDK10 interacts with the ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2) transcription factor and inhibits its transcriptional activity through an unknown mechanism. CDK10 knockdown derepresses ETS2, which increases the expression of the c-Raf protein kinase, activates the MAPK pathway, and induces resistance of MCF7 cells to tamoxifen.Here, we deorphanize CDK10 by identifying cyclin M, the product of FAM58A, as a binding partner. Mutations in this gene that predict absence or truncation of cyclin M are associated with STAR syndrome, whose features include toe syndactyly, telecanthus, and anogenital and renal malformations in heterozygous females. However, both the functions of cyclin M and the pathogenesis of STAR syndrome remain unknown. We show that a recombinant CDK10/cyclin M heterodimer is an active protein kinase that phosphorylates ETS2 in vitro. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and phospho-ERK expression levels and in inducing tamoxifen resistance in estrogen receptor (ER)+ breast cancer cells. We show that CDK10/cyclin M positively controls ETS2 degradation by the proteasome, through the phosphorylation of two neighboring serines. Finally, we detect an increased ETS2 expression level in cells derived from a STAR patient, and we demonstrate that it is attributable to the decreased cyclin M expression level observed in these cells.Previous SectionNext SectionResultsA yeast two-hybrid (Y2H) screen unveiled an interaction signal between CDK10 and a mouse protein whose C-terminal half presents a strong sequence homology with the human FAM58A gene product [whose proposed name is cyclin M]. We thus performed Y2H mating assays to determine whether human CDK10 interacts with human cyclin M. The longest CDK10 isoform (P1) expressed as a bait protein produced a\n"
     ]
    }
   ],
   "source": [
    "# Eliminate references and abbreviations within parentheses\n",
    "print(len(training_text))\n",
    "training_text = re.sub(' \\(Fig \\d+.+?\\)', '', training_text)\n",
    "training_text = re.sub(' \\(Fig\\. \\d+.+?\\)', '', training_text)\n",
    "training_text = re.sub(' \\(\\d.*?\\)', '', training_text)\n",
    "training_text = re.sub(' \\([A-Z]\\)', '', training_text)\n",
    "print(len(training_text))\n",
    "print(training_text[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3321\n"
     ]
    }
   ],
   "source": [
    "# Split text file into list of documents\n",
    "training_list = training_text.split('||')\n",
    "training_list = training_list[2:]\n",
    "print(len(training_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID    Gene             Variation  Class  \\\n",
      "0   0  FAM58A  Truncating Mutations      1   \n",
      "1   1     CBL                 W802*      2   \n",
      "2   2     CBL                 Q249E      2   \n",
      "3   3     CBL                 N454D      3   \n",
      "4   4     CBL                 L399V      4   \n",
      "\n",
      "                                                text  \n",
      "0  Cyclin-dependent kinases (CDKs) regulate a var...  \n",
      "1   Abstract Background  Non-small cell lung canc...  \n",
      "2   Abstract Background  Non-small cell lung canc...  \n",
      "3  Recent evidence has demonstrated that acquired...  \n",
      "4  Oncogenic mutations in the monomeric Casitas B...  \n",
      "(3321, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load training text list into dataframe\n",
    "texts_df = pd.DataFrame(training_list, columns = ['text'])\n",
    "\n",
    "# Merge text dataframe with labels dataframe\n",
    "train = pd.concat([training_variants, texts_df], axis=1)\n",
    "print(train.head())\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Temporary cell to reduce data size\n",
    "#train = train[:400]\n",
    "#print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is loaded into a dataframe, let's do some preliminary data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7    953\n",
      "4    686\n",
      "1    568\n",
      "2    452\n",
      "6    275\n",
      "5    242\n",
      "3     89\n",
      "9     37\n",
      "8     19\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3321 total datapoints to work with, and it looks like we are dealing with significant class imbalance. Class 7 has 953 datapoints, while Class 8 has only 19. We will have to address this class imbalance with our experiment design. Additionally, the labels have been anonymized, which means we cannot draw any insight about what these classes might signify. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Design\n",
    "\n",
    "The prevalence of class imbalance has serious implications for our analysis. First and foremost, we must establish our scoring metric. The purpose here is to use the relevant clinical texts to predict the mutation category for each gene/mutation pair. While we want the predictions to be as accurate as possible, simple classification accuracy is not a representative way to judge models that are built on class imbalance, as they may achieve high accuracy by simply predicting the most common class every time. <br>\n",
    "\n",
    "# Fix this\n",
    "\n",
    "Given that we are working with a multi-label classifier, the most appropriate scoring metric is Cohen's Kappa coefficient (K). This statistic measures inter-rater agreement for categorical items. Contrary to conventional classification accuracy, the K statistic accounts for the possibility of the agreement occurring by chance. <br>\n",
    "\n",
    "The Kappa statistic varies from 0 to 1, where 0 = agreement equivalent to chance, and 1 = perfect agreement. We will be choosing models with Kappa statistics closer to 1. We will also look at the precision and recall via the F1 score. Though these are not optimized for multi-label classification, they will be interesting to consider. <br>\n",
    "\n",
    "We will also try oversampling the lesser-represented categories and apply our machine learning models on the oversampled datasets, judging by their Kappa statistic. Oversampling can be achieved by generating duplicate datapoints or by generating new synthetic datapoints via SMOTE, the Synthetic Minority Oversampling Technique. <br>\n",
    "\n",
    "I will use various methods of feature generation including classic NLP techniques such as bag-of-words, tf-idf, and n-grams. These methods of feature generation will be applied to both the original and oversampled datasets. They will then be subjected to various machine learning models. Decision trees are known to perform well on unbalanced datasets, so this model may prevail on the original data. However, Naive Bayes is known to perform well on natural langauge, so once the dataset is oversampled it is possible that Naive Bayes will perform the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is relatively clean already, and contains no NaN values. It needs to be tokenized so it can be processed into readable pieces of data. We will use spaCy to tokenize the data and create a new column with a list of the tokens for each row. Furthermore, we will convert all tokens that are not stop words or punctuation to lemmas to reduce the noise from unnecessary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'under', 'more', 'do', 'too', 'both', \"shan't\", 'hadn', 'their', 'has', 'o', \"wasn't\", 'after', 'should', 'to', 'such', 'y', 'into', 'but', 'up', 'we', 'before', 'am', 'out', 'above', 'on', 'it', 'further', 'or', 'can', 'about', 'own', 'myself', \"needn't\", 'through', 'him', 'been', 'these', 'now', \"isn't\", 'm', 'how', 'each', 'then', 'doing', 'than', 'theirs', 'did', 'and', 'isn', 'this', 'mustn', 'same', 'other', 'ain', 'weren', 'wasn', 'me', 'needn', \"shouldn't\", 'with', 'just', \"that'll\", 'i', 'during', 'haven', 'hers', \"doesn't\", 'aren', 'an', 'won', \"wouldn't\", 'what', 'those', 'from', 'itself', \"didn't\", 'be', 'her', 'a', 'at', 'didn', 'you', 't', 'shan', 'wouldn', \"you've\", \"hadn't\", 'don', 'only', \"haven't\", 'by', 'between', 'll', 'they', \"you'll\", \"mustn't\", \"mightn't\", 'were', 'are', 'couldn', 'why', 'being', 'doesn', 'd', 'because', 'so', 'once', \"couldn't\", 'yours', \"you're\", 'of', 'in', 'some', 'yourself', 'will', 'your', 'again', 'here', \"you'd\", 'if', \"aren't\", \"hasn't\", 'that', 'his', 've', 'himself', 'which', 'themselves', 'he', 'ours', 'not', \"should've\", 'herself', 'few', \"it's\", 'down', 'having', 'as', 'for', 'shouldn', 'she', 'over', 'whom', 'no', 'had', 'most', 'my', 's', 'is', 'yourselves', 're', \"weren't\", \"she's\", 'was', 'off', 'hasn', \"don't\", 'the', 'against', 'nor', 'very', 'them', 'ourselves', 'mightn', 'all', 'when', 'who', 'have', 'does', 'its', 'ma', 'below', 'any', 'our', 'there', \"won't\", 'until', 'while', 'where'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning lemmatization\n",
      "   ID    Gene             Variation  Class  \\\n",
      "0   0  FAM58A  Truncating Mutations      1   \n",
      "1   1     CBL                 W802*      2   \n",
      "2   2     CBL                 Q249E      2   \n",
      "3   3     CBL                 N454D      3   \n",
      "4   4     CBL                 L399V      4   \n",
      "\n",
      "                                                text  \\\n",
      "0  Cyclin-dependent kinases (CDKs) regulate a var...   \n",
      "1   Abstract Background  Non-small cell lung canc...   \n",
      "2   Abstract Background  Non-small cell lung canc...   \n",
      "3  Recent evidence has demonstrated that acquired...   \n",
      "4  Oncogenic mutations in the monomeric Casitas B...   \n",
      "\n",
      "                                        spacy_tokens  \\\n",
      "0  (Cyclin, -, dependent, kinases, (, CDKs, ), re...   \n",
      "1  ( , Abstract, Background,  , Non, -, small, ce...   \n",
      "2  ( , Abstract, Background,  , Non, -, small, ce...   \n",
      "3  (Recent, evidence, has, demonstrated, that, ac...   \n",
      "4  (Oncogenic, mutations, in, the, monomeric, Cas...   \n",
      "\n",
      "                                              lemmas  \n",
      "0  [cyclin, dependent, kinase, cdks, regulate, va...  \n",
      "1  [abstract, background, non, small, cell, lung,...  \n",
      "2  [abstract, background, non, small, cell, lung,...  \n",
      "3  [recent, evidence, demonstrate, acquire, unipa...  \n",
      "4  [oncogenic, mutation, monomeric, casitas, b, l...  \n"
     ]
    }
   ],
   "source": [
    "print('beginning lemmatization')\n",
    "train['spacy_tokens'] = train['text'].apply(lambda x: nlp(x))\n",
    "\n",
    "def lemmatize(x):\n",
    "    intermediate_lemmas = [token.lemma_.lower() for token in x\n",
    "            if not token.is_punct]\n",
    "    return [lemma for lemma in intermediate_lemmas\n",
    "           if lemma not in stop_words\n",
    "           and lemma != \"-PRON-\"\n",
    "           and lemma != \" \"\n",
    "           ]\n",
    "\n",
    "train['lemmas'] = train['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# idea: export lemmatized to file to speed up future runs\n",
    "\n",
    "Now that we have our lemmatized datapoints, we must convert them into a list of strings to feed to our feature generators. We will then identify the 10,000 most common words in each document in order to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Establishing our test datasets\n",
    "X = train['lemmas']\n",
    "Y = train['Class']\n",
    "\n",
    "X_lemma_documents = [\n",
    "    ' '.join([str(word) for word in text])\n",
    "    for text in X.values.tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4232\n",
      "1189\n"
     ]
    }
   ],
   "source": [
    "# Finding example length of document word list\n",
    "words = re.findall(r'\\w+', X_lemma_documents[0])\n",
    "lower_words = [word.lower() for word in words]\n",
    "print(len(lower_words))\n",
    "lower_words_set = set(lower_words)\n",
    "print(len(lower_words_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given that there are only 1189 unique words for the first document, what is the appropriate number to set my N to? (currently at 10,000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3321\n"
     ]
    }
   ],
   "source": [
    "# Creates a list of the N most common words per document to reduce unnecessary input to vectorizer\n",
    "# should be across all documents, not each document\n",
    "common_X_lemma_documents = []\n",
    "\n",
    "for document in X_lemma_documents: \n",
    "    N = 10000\n",
    "    words = re.findall(r'\\w+', document)\n",
    "    #lower_words = [word.lower() for word in words]\n",
    "    word_counts = Counter(words).most_common(N)\n",
    "    \n",
    "    counter = Counter()\n",
    "    counter.update(new_document)\n",
    "\n",
    "    common_words = [word_tuple[0] for word_tuple in word_counts]\n",
    "    common_X_lemma_documents.append(common_words)\n",
    "    \n",
    "print(len(common_X_lemma_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a dictionary of words and frequencies\n",
    "counter = Counter()\n",
    "\n",
    "for document in X_lemma_documents:\n",
    "    words = document.split(' ')\n",
    "    counter.update(words)\n",
    "\n",
    "#common_X_lemma_documents = counter.most_common(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mutation', 326668], ['cell', 297261]]\n"
     ]
    }
   ],
   "source": [
    "# Converts term/frequency dictionary to sorted list\n",
    "word_frequencies = sorted(\n",
    "                        [[key, value] for key, value in counter.items()],\n",
    "                        key = lambda x: x[1],\n",
    "                        reverse = True\n",
    "                        )\n",
    "\n",
    "print(word_frequencies[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228309\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff2ed0da90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot term frequency, trim list to most common words for better visualization\n",
    "print(len(word_frequencies))\n",
    "trim_length = 2000\n",
    "trim_frequencies = word_frequencies[:trim_length]\n",
    "frequency_X = list(range(len(trim_frequencies)))\n",
    "frequency_Y = [frequency_pair[1] for frequency_pair in trim_frequencies]\n",
    "plt.plot(frequency_X, frequency_Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the term frequencies drop off after the first 250 words. After 250, the term frequencies become very sparse and will likely add no value to our model. For this reason, we will extract only the 1000 most common words for input to our feature generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "common_length = 1000\n",
    "common_words = [frequency_pair[0] for frequency_pair in word_frequencies[:common_length]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to maintain the format of our dataframe, we must reduce each individual document to these 1000 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduced_documents = []\n",
    "\n",
    "for document in X_lemma_documents:\n",
    "    words = document.split(' ')\n",
    "    reduced_document = [word for word in words\n",
    "                       if word in common_words]\n",
    "    reduced_document = ' '.join(reduced_document)\n",
    "    reduced_documents.append(reduced_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "\n",
    "Beyond using singular words or lemmas as features for classification, we can also use groupings of words that appear together, as they may convey more meaning than each word isolated by itself. We will create a dataset of bigrams (2 consecutive words) as groupings larger than 2 words in a dataset this small would likely create unnecessary noise without adding any insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ngrams(input_text, n):\n",
    "  input_text = input_text.split(' ')\n",
    "  output = []\n",
    "  for i in range(len(input_text)-n+1):\n",
    "    output.append(input_text[i:i+n])\n",
    "  return output\n",
    "\n",
    "joined_bigrams = []\n",
    "joined_bigram_list = []\n",
    "\n",
    "for document in X_lemma_documents:\n",
    "    document_bigrams = ngrams(document, 2)\n",
    "    \n",
    "    for bigram in document_bigrams[:10]:\n",
    "        joined_bigram = bigram[0] + ' ' + bigram[1]\n",
    "        joined_bigrams.append(joined_bigram)\n",
    "    \n",
    "    joined_bigram_list.append(joined_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search: Feature Generation, Dimensionality Reduction, Oversampling and Machine Learning\n",
    "\n",
    "In this cell, we will create a pipeline that iterates through several different parameters and models. For feature generation, we will attempt both tf-idf vectorization and bag-of-words vectorization, which will convert our list of strings into sparse dataframes. Bag-of-words is a simplistic representation of text that considers only term frequency and assumes that position doesn't matter. Tf-idf is a more nuanced representation of text that considers the term frequency as well the inverse document frequency, which gives more information about how significant each word is. For example, a common word such as \"the\" may score high in term frequency, but having high term frequency in every document would diminish its meaning and thus its tf-idf score. Both of bag-of-words and tf-idf have several parameters that will be optimized through the gridsearch. For brevity's sake, we are only tuning the max_df and min_df, which represent the maximum and minimum number of times a term should appear in any given document.\n",
    "\n",
    "We will likely need a way to reduce dimensionality, as dimensionality reduction reduces the storage and computation requirements of our model.  We will implement some permutations of the pipeline with Truncated Singular Value Decomposition which works well with sparse data. This transformation is termed Latent Semantic Analysis, a technique that is known to combat polysemy and synonymy. For the permutations that use Truncated SVD, I chose to reduce to 500 components based on a scree plot that was generated outside of this notebook. In the scree plot, 500 components appeared to capture roughly 85% of the variance of the dataset, a well-known standard for dimensionality reduction.\n",
    "\n",
    "Source: http://scikit-learn.org/stable/modules/decomposition.html#truncated-singular-value-decomposition-and-latent-semantic-analysis\n",
    "\n",
    "As mentioned before, we will also implement oversampling via SMOTE to combat the class imbalance. SMOTE (Synthetic Minority Oversampling Technique) uses the K Neighbors algorithm to synthesize new datapoints that resemble the existing classes. The default number of neighbors to use is 5. In an effort to reduce computational complexity, we will hold off on tuning other parameters such as \n",
    "\n",
    "Finally, we will apply several different machine learning models and evaluate their performance using cross-validation with log-loss scoring. First, we will try a multinomial Naive Bayes model, which naively assumes that all features are independent from one another. The multinomial model is commonly used for NLP problems and works under the bag-of-words assumption that position doesn't matter. The sklearn MultinomialNB documentation states that \"the multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\"\n",
    "\n",
    "Source: https://web.stanford.edu/class/cs124/lec/naivebayes.pdf, http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "We will also try random forests, as ensemble models tend to be robust against overfitting. Random forest language models have been shown to generalize well to unseen data, which is important in predictive modeling. The random forest has several parameters that will be tuned with the high dimensionality of the dataset in mind.\n",
    "\n",
    "Source: http://www.aclweb.org/anthology/W04-3242\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 5)),\n",
    "    ('machine', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Feature generation paramters\n",
    "maxdf = [0.25, 0.5, 0.75]\n",
    "mindf = [0.01, 0.05, 0.1]\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "\n",
    "# Machine learning model parameters \n",
    "C_values = [1e-3, 1e-1, 1, 100, 1000]\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    # Tfidf permutations first, running through pipeline with and without dimensionality reduction/oversampling\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    # Bag of words permutations next\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  22.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   36.4s remaining:    0.0s\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  22.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  22.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  24.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  24.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  24.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  24.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  24.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  23.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  25.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  26.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  25.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  25.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  25.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  25.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  33.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  33.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  33.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  37.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  34.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  35.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  47.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  45.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  48.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  55.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  50.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  50.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  23.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  22.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  22.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  24.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  24.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  25.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  24.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  24.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  24.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  26.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  26.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  26.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  25.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  25.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  25.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  33.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  32.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  34.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  35.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  35.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  36.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  51.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  45.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  49.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  51.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  51.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  51.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  23.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  23.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  23.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  25.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  25.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  25.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  24.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  24.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  24.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  26.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  26.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  26.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  25.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  25.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  25.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  33.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  32.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  33.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  35.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  35.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  36.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  48.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  44.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  47.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  53.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  46.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  50.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  24.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  24.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  23.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  25.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  25.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  25.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  25.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  25.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  25.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  26.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  26.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  26.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  26.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  28.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  28.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  28.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  35.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  34.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  35.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  36.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  36.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  38.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  50.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  49.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  50.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  59.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  49.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  53.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  24.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  24.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  24.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  27.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  26.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  27.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  29.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  29.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  28.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  34.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  34.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  36.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  36.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  36.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  37.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  48.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  46.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  47.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  52.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  48.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  52.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  24.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  24.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  24.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  26.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  26.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  26.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  25.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  25.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  25.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  26.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  26.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  27.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  28.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  28.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  29.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  35.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  34.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  34.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  36.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  36.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  37.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  51.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  47.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  50.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  48.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  48.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  52.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  25.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  25.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  24.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  26.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  26.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  26.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  26.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  26.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  28.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  28.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  28.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  27.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  27.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  27.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  29.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  29.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  29.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  35.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  35.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  37.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  38.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  37.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  37.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  53.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  48.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  53.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  56.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  50.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  54.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  25.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  25.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  24.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  26.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  26.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  26.3s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  26.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  28.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  28.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  28.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  28.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  28.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  28.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  29.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  29.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  29.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  35.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  35.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  36.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  38.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  37.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  39.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  46.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  48.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  50.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  50.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  48.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, reduce_dim__n_components=500, total=  50.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  25.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  25.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, reduce_dim__n_components=500, total=  25.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, reduce_dim__n_components=500, total=  27.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  26.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  26.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, reduce_dim__n_components=500, total=  26.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  28.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  28.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  28.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  28.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  28.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, reduce_dim__n_components=500, total=  28.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  30.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  30.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, reduce_dim__n_components=500, total=  30.7s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  35.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  36.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, reduce_dim__n_components=500, total=  36.5s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  38.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  37.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, reduce_dim__n_components=500, total=  38.4s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  48.6s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500, total=  49.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.75, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, total=  21.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, total=  21.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, total=  21.1s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, total=  20.8s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, total=  20.8s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, total=  21.1s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, total=  21.1s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, total=  21.1s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, total=  21.3s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, total=  21.4s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, total=  21.3s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, total=  21.1s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, total=  21.3s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, total=  21.1s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, total=  21.3s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, total=  21.1s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, total=  20.9s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.1, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, total=  20.8s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, total=  21.3s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, total=  21.1s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, total=  21.3s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, total=  21.1s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, total=  20.5s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, total=  21.1s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, total=  21.1s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, total=  20.9s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, total=  21.4s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, total=  20.9s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, total=  21.1s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, total=  21.3s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, total=  21.2s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, total=  21.4s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, total=  21.1s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.01, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=borderline1, total=  21.1s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, total=  21.1s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=regular, total=  21.1s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, total=  21.3s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, total=  20.8s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.001, oversample__kind=borderline1, total=  20.9s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, total=  20.7s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, total=  20.7s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=regular, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, total=  20.7s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, total=  20.8s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, oversample__kind=borderline1, total=  20.8s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, total=  20.8s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, total=  20.7s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=regular, total=  20.8s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, total=  20.7s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, total=  20.8s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, oversample__kind=borderline1, total=  20.8s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, total=  20.7s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, total=  20.7s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=regular, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, total=  20.6s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, total=  21.0s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, oversample__kind=borderline1, total=  20.7s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular, total=  20.7s\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=0.05,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.5, feat_gen__min_df=0.05, machine=LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, oversample__kind=regular \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, reduce_dim__n_components=500, total= 1.3min\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=0.1, reduce_dim__n_components=500, total= 1.3min\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, reduce_dim__n_components=500, total= 1.9min\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, reduce_dim__n_components=500, total= 2.1min\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1, reduce_dim__n_components=500, total= 1.8min\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, reduce_dim__n_components=500, total= 2.4min\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, reduce_dim__n_components=500, total= 2.4min\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=100, reduce_dim__n_components=500, total= 2.9min\n",
      "[CV] feat_gen=CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=1000, min_df=0.01,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=1, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), machine__C=1000, reduce_dim__n_components=500 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-6233e6f2272e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_lemma_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'best score: \\n {grid.best_score_}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'best params:\\n {grid.best_params_}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             train_scores = _score(estimator, X_train, y_train, scorer,\n\u001b[0;32m--> 492\u001b[0;31m                                   is_multimetric)\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \"\"\"\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[0;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, clf, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    136\u001b[0m         super(_ProbaScorer, self).__call__(clf, X, y,\n\u001b[1;32m    137\u001b[0m                                            sample_weight=sample_weight)\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             return self._sign * self._score_func(y, y_pred,\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mtoken_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid.fit(X_lemma_documents, Y)\n",
    "print(f'best score: \\n {grid.best_score_}')\n",
    "print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 5)),\n",
    "    # add configuration without oversampling/dim red\n",
    "    ('machine', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Feature generation paramters\n",
    "maxdf = [0.25, 0.5, 0.75]\n",
    "mindf = [0.01, 0.05, 0.1]\n",
    "\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "\n",
    "# Machine learning model parameters \n",
    "rf_max_depth = [30, 60, 100]\n",
    "#rf_max_features = [10, 20, 30]\n",
    "#rf_min_samples_split = [10, 20, 30]\n",
    "# look into parameter choices for RF for high dimensionality. consider higher values\n",
    "rf_n_estimators = [20, 50]\n",
    "\n",
    "param_grid = [\n",
    "    # Tfidf permutations first, running through pipeline with and without dimensionality reduction/oversampling\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [RandomForestClassifier(random_state = 1)],\n",
    "        'machine__max_depth': rf_max_depth,\n",
    "        'machine__max_features': rf_max_features,\n",
    "        'machine__min_samples_split': rf_min_samples_split,\n",
    "        'machine__n_estimators': rf_n_estimators\n",
    "    },\n",
    "    # Bag of words permutations next, running through all machine learning models\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [RandomForestClassifier(random_state = 1)],\n",
    "        'machine__max_depth': rf_max_depth,\n",
    "        'machine__max_features': rf_max_features,\n",
    "        'machine__min_samples_split': rf_min_samples_split,\n",
    "        'machine__n_estimators': rf_n_estimators\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid.fit(X_lemma_documents, Y)\n",
    "print(f'best score: \\n {grid.best_score_}')\n",
    "print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 5)),\n",
    "    # add configuration without oversampling/dim red\n",
    "    ('machine', XGBClassifier())\n",
    "])\n",
    "\n",
    "# Feature generation paramters\n",
    "maxdf = [0.25, 0.5, 0.75]\n",
    "mindf = [0.01, 0.05, 0.1]\n",
    "\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "\n",
    "# Machine learning model parameters \n",
    "xg_booster = ['gbtree','gblinear']\n",
    "xg_eta = [0.01, 0.1, 0.3]\n",
    "xg_max_depth = [5, 10]\n",
    "xg_subsample = [0.5, 1]\n",
    "\n",
    "param_grid = [\n",
    "    # Tfidf permutations first, running through pipeline with and without dimensionality reduction/oversampling\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [XGBClassifier(random_state = 1)],\n",
    "        'machine__booster': xg_booster,\n",
    "        'machine__learning_rate': xg_eta,\n",
    "        'machine__max_depth': xg_max_depth,\n",
    "        'machine__subsample': xg_subsample\n",
    "    },\n",
    "    # Bag of words permutations next, running through all machine learning models\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [XGBClassifier(random_state = 1)],\n",
    "        'machine__booster': xg_booster,\n",
    "        'machine__learning_rate': xg_eta,\n",
    "        'machine__max_depth': xg_max_depth,\n",
    "        'machine__subsample': xg_subsample\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid.fit(X_lemma_documents, Y)\n",
    "print(f'best score: \\n {grid.best_score_}')\n",
    "print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduced version of pipeline for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 1)),\n",
    "    # increase k neighbors to default\n",
    "    # add configuration without oversampling/dim red\n",
    "    ('machine', GaussianNB())\n",
    "    # why gaussian?\n",
    "])\n",
    "\n",
    "# Feature generation paramters\n",
    "maxdf = [0.25, 0.5, 0.75]\n",
    "mindf = [0.01, 0.05, 0.1]\n",
    "\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "# why dim red? why 500?\n",
    "# if time, show scree plot\n",
    "\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "\n",
    "# Machine learning model parameters \n",
    "rf_max_depth = [10, 20, 30]\n",
    "rf_max_features = [10, 20, 30]\n",
    "rf_min_samples_split = [10, 20, 30]\n",
    "# look into parameter choices for RF for high dimensionality. consider higher values\n",
    "rf_n_estimators = [10, 20]\n",
    "xg_booster = ['gbtree','gblinear']\n",
    "xg_eta = [0.01, 0.1, 0.3]\n",
    "xg_max_depth = [5, 10]\n",
    "xg_subsample = [0.5, 1]\n",
    "\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    # Tfidf permutations first, running through all machine learning models\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer()],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'machine': [GaussianNB()] \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=GaussianNB(priors=None) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=GaussianNB(priors=None), total=  20.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=GaussianNB(priors=None) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   35.5s remaining:    0.0s\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=GaussianNB(priors=None), total=  20.9s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=GaussianNB(priors=None) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.01, machine=GaussianNB(priors=None), total=  21.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=GaussianNB(priors=None) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=GaussianNB(priors=None), total=  21.0s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=None, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=GaussianNB(priors=None) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/metrics/scorer.py:137: DeprecationWarning: Scoring method log_loss was renamed to neg_log_loss in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=None, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=GaussianNB(priors=None), total=  21.2s\n",
      "[CV] feat_gen=TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=None, min_df=0.05,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), feat_gen__max_df=0.25, feat_gen__min_df=0.05, machine=GaussianNB(priors=None) \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-bf9a9a498c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_lemma_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'best params:\\n {grid.best_params_}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \"\"\"\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    199\u001b[0m                     Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    200\u001b[0m                         \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                         **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    202\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     Xt, yt, fitted_transformer = fit_sample_one_cached(\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    591\u001b[0m                        **fit_params):\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mtoken_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid.fit(X_lemma_documents, Y)\n",
    "print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to figure out how I came up with my 500 PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import datetime\n",
    "\n",
    "def export_gridsearch_to_csv(gs_clf, export_file):\n",
    "    with open(export_file, 'w') as outfile:\n",
    "        csvwriter = csv.writer(outfile, delimiter=',')\n",
    "\n",
    "        # Create the header using the parameter names \n",
    "        header = [\"mean\",\"std\", \"params\"]\n",
    "\n",
    "        csvwriter.writerow(header)\n",
    "        \n",
    "        sorted_by_score = sorted(gs_clf.grid_scores_, key = itemgetter(1), reverse=True)\n",
    "        \n",
    "        for config in sorted_by_score:\n",
    "            # Get mean and standard deviation\n",
    "            mean = np.abs(config[1])\n",
    "            std = np.std(config[2])\n",
    "            row = [mean,std, str(config[0])]\n",
    "\n",
    "            csvwriter.writerow(row)\n",
    "            \n",
    "filename = datetime.datetime.utcnow().strftime('gridresults_%Y%m%d_%H:%M.csv')       \n",
    "export_gridsearch_to_csv(grid, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "filename = datetime.datetime.utcnow().strftime('gridresults_%Y%m%d_%H%M.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original custom tfidf vectorizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 82153\n"
     ]
    }
   ],
   "source": [
    "def tfidf_vectorizer(X,Y):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                                 min_df=2,\n",
    "                                 stop_words='english', \n",
    "                                 lowercase=True,\n",
    "                                 norm=u'l2',\n",
    "                                 smooth_idf=True,\n",
    "                                )\n",
    "\n",
    "    sparse_tfidf_matrix=vectorizer.fit_transform(X)\n",
    "    print(f'Number of features: {sparse_tfidf_matrix.get_shape()[1]}')\n",
    "    \n",
    "    # Densify matrix so we can convert it to a conventional dataframe to extract X/Y\n",
    "    dense_tfidf_matrix = sparse_tfidf_matrix.todense()\n",
    "    df_tfidf = pd.DataFrame(dense_tfidf_matrix)\n",
    "    df_tfidf['lemma_tokens'] = X\n",
    "    df_tfidf['class'] = Y\n",
    "    \n",
    "    return df_tfidf\n",
    "\n",
    "tfidf_unigram = tfidf_vectorizer(X_lemma_documents,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d38e5d0d8ebf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_unigram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msklearn_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msklearn_pca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m print(\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         X = check_array(X, dtype=[np.float64, np.float32], ensure_2d=True,\n\u001b[0;32m--> 370\u001b[0;31m                         copy=self.copy)\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;31m# Handle n_components==None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/lib.pyx\u001b[0m in \u001b[0;36mpandas.lib.values_from_object (pandas/lib.c:4652)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mget_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m         \u001b[0;34m\"\"\"same as values (but handles sparseness conversions)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2940\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2942\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_dtype_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mas_matrix\u001b[0;34m(self, columns)\u001b[0m\n\u001b[1;32m   2905\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2906\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2907\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2908\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mas_matrix\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m   3401\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3402\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3403\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_interleave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3410\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_interleaved_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3412\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3414\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "X = tfidf_unigram\n",
    "sklearn_pca = PCA()\n",
    "sklearn_pca.fit(X)\n",
    "\n",
    "print(\n",
    "    'The percentage of total variance in the dataset explained by each',\n",
    "    'component from Sklearn PCA.\\n',\n",
    "    sklearn_pca.explained_variance_ratio_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_lemma_documents[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_bigram = tfidf_vectorizer(bigrams, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning Methods\n",
    "\n",
    "Here, we will attempt to classify the training data using several different machine learning classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(tX_train, tY_train)\n",
    "test_pred = gnb.predict(tX_test)\n",
    "print(f'Testing Accuracy: {accuracy_score(tY_test, test_pred)}')\n",
    "print(f'Cross Val Score: {cross_val_score(gnb, tX_train, tY_train, cv=3).mean()}')\n",
    "print(pd.crosstab(tY_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer(\n",
    "                            stop_words='english', \n",
    "                            lowercase=True,\n",
    "                            norm=u'l2',\n",
    "                            smooth_idf=True\n",
    "                        )\n",
    "nb = GaussianNB()\n",
    "\n",
    "pipe = Pipeline(steps=[('tfidf', tfidf), ('nb', nb)])\n",
    "\n",
    "tfidf_maxdf = [0.25, 0.5, 0.75]\n",
    "tfidf_mindf = [2, 5, 20]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'tfidf__max_df': tfidf_maxdf,\n",
    "        'tfidf__min_df': tfidf_mindf,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, cv=5, n_jobs=1, param_grid=param_grid)\n",
    "grid.fit(X_train, Y_train)\n",
    "#print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, decomposition, datasets\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feat_gen', tfidf_vectorizer()),\n",
    "    ('reduce_dim', decomposition.PCA())\n",
    "    ('classify', GaussianNB())\n",
    "])\n",
    "\n",
    "tfidf_maxdf = [0.25, 0.5, 0.75]\n",
    "tfidf_mindf = [2, 5, 20]\n",
    "\n",
    "param_grid_1 = [\n",
    "    {\n",
    "        'feat_gen': [tfidf_vectorizer(                            \n",
    "                                        stop_words='english', \n",
    "                                        lowercase=True,\n",
    "                                        norm=u'l2',\n",
    "                                        smooth_idf=True\n",
    "                                    )],\n",
    "        'feat_gen__max_df': tfidf_maxdf,\n",
    "        'feat_gen__min_df': tfidf_mindf,\n",
    "        'classify__C': C_OPTIONS\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [SelectKBest(chi2)],\n",
    "        'feat_gen__k': N_FEATURES_OPTIONS,\n",
    "        'classify__C': C_OPTIONS\n",
    "    },\n",
    "]\n",
    "\n",
    "param_grid_2 = dict(feat_gen=[tfidf_vectorizer(), count_vectorizer()],\n",
    "                    reduce_dim = [PCA(), tSNE()]\n",
    "                    clf = [GaussianNB()])\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, n_jobs=1, param_grid=param_grid)\n",
    "grid.fit(X, Y)\n",
    "\n",
    "##### needs to use a real class not a custom function\n",
    "##### input must be X list of strings (each string corresponds to a single document) and Y of same size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
