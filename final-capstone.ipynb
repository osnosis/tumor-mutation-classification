{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tumor Mutation Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will analyze a dataset of tumor gene mutations and their associated 'risk category', which indicates the risk of malignancy for that mutation in the human condition. The dataset was hand-labeled and released by the team of clinical pathologists at Memorial Sloan Kettering in 2018. Our objective of this project is to fit the dataset into various machine learning models to predict the risk category while accounting for highly unbalanced classes. Several methods for text feature generation will be explored. We will then use the synthetic minority over-sampling technique (SMOTE) to resample the dataset to make the numbers of categories more even. The last step is to compare machine learning methods on our feature sets and tune our models to achieve maximum performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In this section we will load in all of our data. The text also needs to be cleaned of all non-legitimate words, such as figure references and parentheses. This will be accomplished using regular expressions. We have been provided a training dataset of several thousand labeled peer-reviewed journal articles. We have also been provided a testing dataset for external validation, but since this dataset is not labeled so we will not be able to use it at this time to assist with tuning our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up packages for loading in data\n",
    "client = boto3.client('s3') #low-level functional API\n",
    "\n",
    "resource = boto3.resource('s3') #high-level object-oriented API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in training data labels\n",
    "obj = client.get_object(Bucket='thinkful-capstone', Key='training_variants')\n",
    "stream = io.BytesIO(obj['Body'].read())\n",
    "training_variants = pd.read_csv(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID    Gene             Variation  Class\n",
      "0   0  FAM58A  Truncating Mutations      1\n",
      "1   1     CBL                 W802*      2\n",
      "2   2     CBL                 Q249E      2\n",
      "3   3     CBL                 N454D      3\n",
      "4   4     CBL                 L399V      4\n",
      "(3321, 4)\n"
     ]
    }
   ],
   "source": [
    "print(training_variants.head())\n",
    "print(training_variants.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID||Text\n",
      "0||Cyclin-dependent kinases (CDKs) regulate a variety of fundamental cellular processes. CDK10 stands out as one of the last orphan CDKs for which no activating cyclin has been identified and no kinase activity revealed. Previous work has shown that CDK10 silencing increases ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2)-driven activation of the MAPK pathway, which confers tamoxifen resistance to breast cancer cells. The precise mechanisms by which CDK10 modulates ETS2 activity, and more generally the functions of CDK10, remain elusive. Here we demonstrate that CDK10 is a cyclin-dependent kinase by identifying cyclin M as an activating cyclin. Cyclin M, an orphan cyclin, is the product of FAM58A, whose mutations cause STAR syndrome, a human developmental anomaly whose features include toe syndactyly, telecanthus, and anogenital and renal malformations. We show that STAR syndrome-associated cyclin M mutants are unable to interact with CDK10. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and in conferring tamoxifen resistance to breast cancer cells. CDK10/cyclin M phosphorylates ETS2 in vitro, and in cells it positively controls ETS2 degradation by the proteasome. ETS2 protein levels are increased in cells derived from a STAR patient, and this increase is attributable to decreased cyclin M levels. Altogether, our results reveal an additional regulatory mechanism for ETS2, which plays key roles in cancer and development. They also shed light on the molecular mechanisms underlying STAR syndrome.Cyclin-dependent kinases (CDKs) play a pivotal role in the control of a number of fundamental cellular processes (1). The human genome contains 21 genes encoding proteins that can be considered as members of the CDK family owing to their sequence similarity with bona fide CDKs, those known to be activated by cyclins (2). Although discovered almost 20 y ago (3, 4), CDK10 remains one of the two CDKs without an identified cyclin partner. This knowledge gap has largely impeded the exploration of its biological functions. CDK10 can act as a positive cell cycle regulator in some cells (5, 6) or as a tumor suppressor in others (7, 8). CDK10 interacts with the ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2) transcription factor and inhibits its transcriptional activity through an unknown mechanism (9). CDK10 knockdown derepresses ETS2, which increases the expression of the c-Raf protein kinase, activates the MAPK pathway, and induces resistance of MCF7 cells to tamoxifen (6).Here, we deorphanize CDK10 by identifying cyclin M, the product of FAM58A, as a binding partner. Mutations in this gene that predict absence or truncation of cyclin M are associated with STAR syndrome, whose features include toe syndactyly, telecanthus, and anogenital and renal malformations in heterozygous females (10). However, both the functions of cyclin M and the pathogenesis of STAR syndrome remain unknown. We show that a recombinant CDK10/cyclin M heterodimer is an active protein kinase that phosphorylates ETS2 in vitro. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and phospho-ERK expression levels and in inducing tamoxifen resistance in estrogen receptor (ER)+ breast cancer cells. We show that CDK10/cyclin M positively controls ETS2 degradation by the proteasome, through the phosphorylation of two neighboring serines. Finally, we detect an increased ETS2 expression level in cells derived from a STAR patient, and we demonstrate that it is attributable to the decreased cyclin M expression level observed in these cells.Previous SectionNext SectionResultsA yeast two-hybrid (Y2H) screen unveiled an interaction signal between CDK10 and a mouse protein whose C-terminal half presents a strong sequence homology with the human FAM58A gene product [whose proposed name is cyclin M (11)]. We thus performed Y2H mating assays to determine whether human CDK10 interacts with human cyclin M (Fig. 1 A–C). The longest CDK10 isoform (P1) expressed as a bait protein produced a strong interaction phenotype with full-length cyclin M (expressed as a prey protein) but no detectable phenotype with cyclin D1, p21 (CIP1), and Cdi1 (KAP), which are known binding partners of other CDKs (Fig. 1B). CDK1 and CDK3 also produced Y2H signals with cyclin M, albeit notably weaker than that observed with CDK10 (Fig. 1B). An interaction phenotype was also observed between full-length cyclin M and CDK10 proteins expressed as bait and prey, respectively (Fig. S1A). We then tested different isoforms of CDK10 and cyclin M originating from alternative gene splicing, and two truncated cyclin M proteins corresponding to the hypothetical products of two mutated FAM58A genes found in STAR syndrome patients (10). None of these shorter isoforms produced interaction phenotypes (Fig. 1 A and C and Fig. S1A).Fig. 1.In a new window Download PPTFig. 1.CDK10 and cyclin M form an interaction complex. (A) Schematic representation of the different protein isoforms analyzed by Y2H assays. Amino acid numbers are indicated. Black boxes indicate internal deletions. The red box indicates a differing amino acid sequence compared with CDK10 P1. (B) Y2H assay between a set of CDK proteins expressed as baits (in fusion to the LexA DNA binding domain) and CDK interacting proteins expressed as preys (in fusion to the B42 transcriptional activator). pEG202 and pJG4-5 are the empty bait and prey plasmids expressing LexA and B42, respectively. lacZ was used as a reporter gene, and blue yeast are indicative of a Y2H interaction phenotype. (C) Y2H assay between the different CDK10 and cyclin M isoforms. The amino-terminal region of ETS2, known to interact with CDK10 (9), was also assayed. (D) Western blot analysis of Myc-CDK10 (wt or kd) and CycM-V5-6His expression levels in transfected HEK293 cells. (E) Western blot analysis of Myc-CDK10 (wt or kd) immunoprecipitates obtained using the anti-Myc antibody. “Inputs” correspond to 10 μg total lysates obtained from HEK293 cells coexpressing Myc-CDK10 (wt or kd) and CycM-V5-6His. (F) Western blot analysis of immunoprecipitates obtained using the anti-CDK10 antibody or a control goat antibody, from human breast cancer MCF7 cells. “Input” corresponds to 30 μg MCF7 total cell lysates. The lower band of the doublet observed on the upper panel comigrates with the exogenously expressed untagged CDK10 and thus corresponds to endogenous CDK10. The upper band of the doublet corresponds to a nonspecific signal, as demonstrated by it insensitivity to either overexpression of CDK10 (as seen on the left lane) or silencing of CDK10 (Fig. S2B). Another experiment with a longer gel migration is shown in Fig. S1D.Next we examined the ability of CDK10 and cyclin M to interact when expressed in human cells (Fig. 1 D and E). We tested wild-type CDK10 (wt) and a kinase dead (kd) mutant bearing a D181A amino acid substitution that abolishes ATP binding (12). We expressed cyclin M-V5-6His and/or Myc-CDK10 (wt or kd) in a human embryonic kidney cell line (HEK293). The expression level of cyclin M-V5-6His was significantly increased upon coexpression with Myc-CDK10 (wt or kd) and, to a lesser extent, that of Myc-CDK10 (wt or kd) was increased upon coexpression with cyclin M-V5-6His (Fig. 1D). We then immunoprecipitated Myc-CDK10 proteins and detected the presence of cyclin M in the CDK10 (wt) and (kd) immunoprecipitates only when these proteins were coexpressed pair-wise (Fig. 1E). We confirmed these observations by detecting the presence of Myc-CDK10 in cyclin M-V5-6His immunoprecipitates (Fig. S1B). These experiments confirmed the lack of robust interaction between the CDK10.P2 isoform and cyclin M (Fig. S1C). To detect the interaction between endogenous proteins, we performed immunoprecipitations on nontransfected MCF7 cells derived from a human breast cancer. CDK10 and cyclin M antibodies detected their cognate endogenous proteins by Western blotting. We readily detected cyclin M in immunoprecipitates obtained with the CDK10 antibody but not with a control antibody (Fig. 1F). These results confirm the physical interaction between CDK10 and cyclin M in human cells.To unveil a hypothesized CDK10/cyclin M protein kinase activity, we produced GST-CDK10 and StrepII-cyclin M fusion proteins in insect cells, either individually or in combination. We observed that GST-CDK10 and StrepII-cyclin M copurified, thus confirming their interaction in yet another cellular model (Fig. 2A). We then performed in vitro kinase assays with purified proteins, using histone H1 as a generic substrate. Histone H1 phosphorylation was detected only from lysates of cells coexpressing GST-CDK10 and StrepII-cyclin M. No phosphorylation was detected when GST-CDK10 or StrepII-cyclin M were expressed alone, or when StrepII-cyclin M was coexpressed with GST-CDK10(kd) (Fig. 2A). Next we investigated whether ETS2, which is known to interact with CDK10 (9) (Fig. 1C), is a phosphorylation substrate of CDK10/cyclin M. We detected strong phosphorylation of ETS2 by the GST-CDK10/StrepII-cyclin M purified heterodimer, whereas no phosphorylation was detected using GST-CDK10 alone or GST-CDK10(kd)/StrepII-cyclin M heterodimer (Fig. 2B).Fig. 2.In a new window Download PPTFig. 2.CDK10 is a cyclin M-dependent protein kinase. (A) In vitro protein kinase assay on histone H1. Lysates from insect cells expressing different proteins were purified on a glutathione Sepharose matrix to capture GST-CDK10(wt or kd) fusion proteins alone, or in complex with STR-CycM fusion protein. Purified protein expression levels were analyzed by Western blots (Top and Upper Middle). The kinase activity was determined by autoradiography of histone H1, whose added amounts were visualized by Coomassie staining (Lower Middle and Bottom). (B) Same as in A, using purified recombinant 6His-ETS2 as a substrate.CDK10 silencing has been shown to increase ETS2-driven c-RAF transcription and to activate the MAPK \n"
     ]
    }
   ],
   "source": [
    "# Load in training data text articles\n",
    "obj = client.get_object(Bucket=\"thinkful-capstone\",Key=\"training_text\")\n",
    "raw_training_text = obj[\"Body\"].read()\n",
    "training_text = raw_training_text.decode('utf-8')\n",
    "print(training_text[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211296707\n",
      "205598350\n",
      "ID||Text\n",
      "0||Cyclin-dependent kinases (CDKs) regulate a variety of fundamental cellular processes. CDK10 stands out as one of the last orphan CDKs for which no activating cyclin has been identified and no kinase activity revealed. Previous work has shown that CDK10 silencing increases ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2)-driven activation of the MAPK pathway, which confers tamoxifen resistance to breast cancer cells. The precise mechanisms by which CDK10 modulates ETS2 activity, and more generally the functions of CDK10, remain elusive. Here we demonstrate that CDK10 is a cyclin-dependent kinase by identifying cyclin M as an activating cyclin. Cyclin M, an orphan cyclin, is the product of FAM58A, whose mutations cause STAR syndrome, a human developmental anomaly whose features include toe syndactyly, telecanthus, and anogenital and renal malformations. We show that STAR syndrome-associated cyclin M mutants are unable to interact with CDK10. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and in conferring tamoxifen resistance to breast cancer cells. CDK10/cyclin M phosphorylates ETS2 in vitro, and in cells it positively controls ETS2 degradation by the proteasome. ETS2 protein levels are increased in cells derived from a STAR patient, and this increase is attributable to decreased cyclin M levels. Altogether, our results reveal an additional regulatory mechanism for ETS2, which plays key roles in cancer and development. They also shed light on the molecular mechanisms underlying STAR syndrome.Cyclin-dependent kinases (CDKs) play a pivotal role in the control of a number of fundamental cellular processes. The human genome contains 21 genes encoding proteins that can be considered as members of the CDK family owing to their sequence similarity with bona fide CDKs, those known to be activated by cyclins. Although discovered almost 20 y ago, CDK10 remains one of the two CDKs without an identified cyclin partner. This knowledge gap has largely impeded the exploration of its biological functions. CDK10 can act as a positive cell cycle regulator in some cells or as a tumor suppressor in others. CDK10 interacts with the ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2) transcription factor and inhibits its transcriptional activity through an unknown mechanism. CDK10 knockdown derepresses ETS2, which increases the expression of the c-Raf protein kinase, activates the MAPK pathway, and induces resistance of MCF7 cells to tamoxifen.Here, we deorphanize CDK10 by identifying cyclin M, the product of FAM58A, as a binding partner. Mutations in this gene that predict absence or truncation of cyclin M are associated with STAR syndrome, whose features include toe syndactyly, telecanthus, and anogenital and renal malformations in heterozygous females. However, both the functions of cyclin M and the pathogenesis of STAR syndrome remain unknown. We show that a recombinant CDK10/cyclin M heterodimer is an active protein kinase that phosphorylates ETS2 in vitro. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and phospho-ERK expression levels and in inducing tamoxifen resistance in estrogen receptor (ER)+ breast cancer cells. We show that CDK10/cyclin M positively controls ETS2 degradation by the proteasome, through the phosphorylation of two neighboring serines. Finally, we detect an increased ETS2 expression level in cells derived from a STAR patient, and we demonstrate that it is attributable to the decreased cyclin M expression level observed in these cells.Previous SectionNext SectionResultsA yeast two-hybrid (Y2H) screen unveiled an interaction signal between CDK10 and a mouse protein whose C-terminal half presents a strong sequence homology with the human FAM58A gene product [whose proposed name is cyclin M]. We thus performed Y2H mating assays to determine whether human CDK10 interacts with human cyclin M. The longest CDK10 isoform (P1) expressed as a bait protein produced a\n"
     ]
    }
   ],
   "source": [
    "# Eliminate references and abbreviations within parentheses\n",
    "print(len(training_text))\n",
    "training_text = re.sub(' \\(Fig \\d+.+?\\)', '', training_text)\n",
    "training_text = re.sub(' \\(Fig\\. \\d+.+?\\)', '', training_text)\n",
    "training_text = re.sub(' \\(\\d.*?\\)', '', training_text)\n",
    "training_text = re.sub(' \\([A-Z]\\)', '', training_text)\n",
    "print(len(training_text))\n",
    "print(training_text[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3321\n"
     ]
    }
   ],
   "source": [
    "# Split text file into list of documents\n",
    "training_list = training_text.split('||')\n",
    "training_list = training_list[2:]\n",
    "print(len(training_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID    Gene             Variation  Class  \\\n",
      "0   0  FAM58A  Truncating Mutations      1   \n",
      "1   1     CBL                 W802*      2   \n",
      "2   2     CBL                 Q249E      2   \n",
      "3   3     CBL                 N454D      3   \n",
      "4   4     CBL                 L399V      4   \n",
      "\n",
      "                                                text  \n",
      "0  Cyclin-dependent kinases (CDKs) regulate a var...  \n",
      "1   Abstract Background  Non-small cell lung canc...  \n",
      "2   Abstract Background  Non-small cell lung canc...  \n",
      "3  Recent evidence has demonstrated that acquired...  \n",
      "4  Oncogenic mutations in the monomeric Casitas B...  \n",
      "(3321, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load training text list into dataframe\n",
    "texts_df = pd.DataFrame(training_list, columns = ['text'])\n",
    "\n",
    "# Merge text dataframe with labels dataframe\n",
    "train = pd.concat([training_variants, texts_df], axis=1)\n",
    "print(train.head())\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Temporary cell to reduce data size\n",
    "#train = train[:400]\n",
    "#print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is loaded into a dataframe, let's do some preliminary data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7    953\n",
      "4    686\n",
      "1    568\n",
      "2    452\n",
      "6    275\n",
      "5    242\n",
      "3     89\n",
      "9     37\n",
      "8     19\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3321 total datapoints to work with, and it looks like we are dealing with significant class imbalance. Class 7 has 953 datapoints, while Class 8 has only 19. We will have to address this class imbalance with our experiment design. Additionally, the labels have been anonymized, which means we cannot draw any insight about what these classes might signify. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Design\n",
    "\n",
    "The prevalence of class imbalance has serious implications for our analysis. First and foremost, we must establish our scoring metric. The purpose here is to use the relevant clinical texts to predict the mutation category for each gene/mutation pair. While we want the predictions to be as accurate as possible, simple classification accuracy is not a representative way to judge models that are built on class imbalance, as they may achieve high accuracy by simply predicting the most common class every time. <br>\n",
    "\n",
    "Given that we are working with a multi-label classifier, the most appropriate scoring metric is log loss. Log loss quantifies the accuracy of a classifier by penalising false classifications, and heavily penalises classifiers that are confident about an incorrect classification. For this reason, it is more suitable than traditional accuracy for datasets with class imbalance. <br>\n",
    "\n",
    "When feeding our data into the predictive models, each resulting prediction is associated with a probability, and each probability is multiplied by one another to get the overall probability that all of those outcomes occurred together. As each event gets multiplied in, the final number gets smaller and smaller. So, we take the log to put the number in a more accessible range. The number is then multiplied by -1 to maintain the convention that a lower loss score is better. We will be choosing models with log loss scores closer to 0. We will also look at the precision and recall via the F1 score. Though these are not optimized for multi-label classification, they will be interesting to consider. <br>\n",
    "\n",
    "Source: https://datawookie.netlify.com/blog/2015/12/making-sense-of-logarithmic-loss/, https://www.kaggle.com/dansbecker/what-is-log-loss\n",
    "\n",
    "We will also try oversampling the lesser-represented categories and apply our machine learning models on the oversampled datasets, judging by their log loss scores. Oversampling can be achieved by generating duplicate datapoints or by generating new synthetic datapoints via SMOTE, the Synthetic Minority Oversampling Technique. We will use SMOTE, a feature of the imbalanced learn package. <br>\n",
    "\n",
    "I will use various methods of feature generation including classic NLP techniques such as bag-of-words, tf-idf, and n-grams. These methods of feature generation will be applied to both the original and oversampled datasets. They will then be subjected to various machine learning models. Decision trees are known to perform well on unbalanced datasets, so this model may prevail on the original data. However, Naive Bayes is known to perform well on natural langauge, so once the dataset is oversampled it is possible that Naive Bayes will perform the best. All machine learning models will be run with a variety of hyperparameters on a variety of datasets, and the permutation with the best log-loss score will be chosen to move forward.\n",
    "\n",
    "It is important to note that by convention, grid search always tries to maximize its score so loss functions like log loss will have to be negated such that the lowest log loss score will be the highest negated score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is relatively clean already, and contains no NaN values. It needs to be tokenized so it can be processed into readable pieces of data. We will use spaCy to tokenize the data and create a new column with a list of the tokens for each row. Furthermore, we will convert all tokens that are not stop words or punctuation to lemmas to reduce the noise from unnecessary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'off', 's', 'ain', \"needn't\", 'couldn', 'all', 'her', 'has', 'of', 'ma', 'before', 'during', 'over', 'which', 're', 'yours', \"don't\", 'our', 'them', 'y', 'each', 'that', 'wasn', \"mustn't\", 'very', 'not', 'from', 'up', 'when', 'd', 'until', 'now', 'below', 'she', 'doing', 'down', 'am', 'hadn', 'wouldn', \"won't\", 'herself', 'the', 'me', \"couldn't\", 'between', 'on', \"you're\", 'myself', 'haven', 'won', 'through', 'only', \"you've\", 'then', 'no', 'having', 'don', 'what', 'being', 'yourselves', 'shouldn', 'him', 'than', 'where', 'll', 'at', 'own', 'if', \"didn't\", 'were', 'while', 'most', 'who', \"hadn't\", 'an', \"wouldn't\", 'for', 'under', 'its', 'doesn', \"isn't\", 'is', 'and', 'mightn', 'any', 'against', \"hasn't\", 'because', 'to', 've', 'it', 'mustn', 'shan', 'so', 'they', 'isn', 'their', 'whom', \"doesn't\", 'further', \"shouldn't\", 'have', 'by', 'needn', \"should've\", 'does', 'or', 'after', \"aren't\", 'did', 'there', 'you', 'had', \"you'll\", 'too', 'as', 'will', 'those', 'be', 'about', 'nor', 'few', \"weren't\", 'themselves', 'such', 'some', \"shan't\", \"wasn't\", 'once', 'weren', 'ours', 'been', \"that'll\", 't', \"you'd\", \"haven't\", 'o', 'do', 'are', 'but', 'above', 'ourselves', 'these', 'was', 'more', 'aren', 'itself', 'theirs', 'other', 'same', 'yourself', 'this', 'with', 'i', 'my', 'just', 'a', 'his', 'here', 'both', \"mightn't\", 'we', 'your', 'should', 'again', 'can', 'into', 'in', 'm', 'hasn', \"it's\", 'why', \"she's\", 'how', 'out', 'didn', 'himself', 'he', 'hers'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning lemmatization\n",
      "   ID    Gene             Variation  Class  \\\n",
      "0   0  FAM58A  Truncating Mutations      1   \n",
      "1   1     CBL                 W802*      2   \n",
      "2   2     CBL                 Q249E      2   \n",
      "3   3     CBL                 N454D      3   \n",
      "4   4     CBL                 L399V      4   \n",
      "\n",
      "                                                text  \\\n",
      "0  Cyclin-dependent kinases (CDKs) regulate a var...   \n",
      "1   Abstract Background  Non-small cell lung canc...   \n",
      "2   Abstract Background  Non-small cell lung canc...   \n",
      "3  Recent evidence has demonstrated that acquired...   \n",
      "4  Oncogenic mutations in the monomeric Casitas B...   \n",
      "\n",
      "                                        spacy_tokens  \\\n",
      "0  (Cyclin, -, dependent, kinases, (, CDKs, ), re...   \n",
      "1  ( , Abstract, Background,  , Non, -, small, ce...   \n",
      "2  ( , Abstract, Background,  , Non, -, small, ce...   \n",
      "3  (Recent, evidence, has, demonstrated, that, ac...   \n",
      "4  (Oncogenic, mutations, in, the, monomeric, Cas...   \n",
      "\n",
      "                                              lemmas  \n",
      "0  [cyclin, dependent, kinase, cdks, regulate, va...  \n",
      "1  [abstract, background, non, small, cell, lung,...  \n",
      "2  [abstract, background, non, small, cell, lung,...  \n",
      "3  [recent, evidence, demonstrate, acquire, unipa...  \n",
      "4  [oncogenic, mutation, monomeric, casitas, b, l...  \n"
     ]
    }
   ],
   "source": [
    "print('beginning lemmatization')\n",
    "train['spacy_tokens'] = train['text'].apply(lambda x: nlp(x))\n",
    "\n",
    "def lemmatize(x):\n",
    "    intermediate_lemmas = [token.lemma_.lower() for token in x\n",
    "            if not token.is_punct]\n",
    "    return [lemma for lemma in intermediate_lemmas\n",
    "           if lemma not in stop_words\n",
    "           and lemma != \"-PRON-\"\n",
    "           and lemma != \" \"\n",
    "           ]\n",
    "\n",
    "train['lemmas'] = train['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our lemmatized datapoints, we must convert them into a list of strings to feed to our feature generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_lemma_documents = [\n",
    "    ' '.join([str(word) for word in text])\n",
    "    for text in train['lemmas'].values.tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update training dataframe with lemmatized documents\n",
    "train['lemmatized'] = joined_lemma_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reserve 20% of training dataset for external validation\n",
    "train, test = train_test_split(train,\n",
    "                                test_size = 0.2,\n",
    "                                random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce unnecessary input to the vectorizer, we want to cut each document to its most important features. But how will we know how many features to keep? First, we will create a dictionary of all the words in the corpora and their frequencies using Counter. Then we will graph the words by frequency to see how many words make up the majority of our corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of 1000 most common words per training set to reduce features in testing set\n",
    "# Create a dictionary of words and frequencies\n",
    "counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iterate through each training document, split into words and add words/frequencies to Counter\n",
    "for document in train['lemmatized']:\n",
    "    words = document.split(' ')\n",
    "    counter.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mutation', 264384], ['cell', 241150]]\n"
     ]
    }
   ],
   "source": [
    "# Convert term/frequency dictionary to list sorted by frequency\n",
    "word_frequencies = sorted(\n",
    "                        [[key, value] for key, value in counter.items()],\n",
    "                        key = lambda x: x[1],\n",
    "                        reverse = True\n",
    "                        )\n",
    "\n",
    "print(word_frequencies[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of terms: 205136\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng9gl1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbb8cd89550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot term frequency, trim list to 2000 most common words for easier visualization\n",
    "print('Total number of terms:', len(word_frequencies))\n",
    "trim_length = 2000\n",
    "trim_frequencies = word_frequencies[:trim_length]\n",
    "frequency_X = list(range(len(trim_frequencies)))\n",
    "frequency_Y = [frequency_pair[1] for frequency_pair in trim_frequencies]\n",
    "plt.plot(frequency_X, frequency_Y)\n",
    "plt.xlabel('Rank of Frequency')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the term frequencies drop off after the first 250 words. After 250, the term frequencies become very sparse and will likely add no value to our model. Still, we will keep the 1000 most common words for input to our feature generator to ensure that we are not losing any potentially valuable features. We will utilize the max_feature hyperparameter in the tf-idf and count vectorizer, which will allow us to reduce the dataset to these 1000 most common words in one simple step.\n",
    "\n",
    "However, when we run our external validation set (reserved 20%) we cannot verify it unless our validation set has the same features as our training set. For this reason, we will extract the 1000 most common words from our training set and reduce our validation documents to these 1000 words. While this significantly changes our testing data, it is the only way to evaluate the legitimacy of our chosen model. This is a common problem in natural language processing, and typically handled by either ignoring all unseen words in the testing document or lumping them into a single 'unknown' type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of n most common words per training document\n",
    "common_words = [frequency_pair[0] for frequency_pair in word_frequencies[:1000]]\n",
    "\n",
    "reduced_documents = []\n",
    "# Reducing test documents to 1000 most common words per training set\n",
    "for document in test['lemmatized'].values.tolist():\n",
    "    words = document.split(' ')\n",
    "    reduced_document = [word for word in words\n",
    "                       if word in common_words]\n",
    "    reduced_document = ' '.join(reduced_document)\n",
    "    reduced_documents.append(reduced_document)\n",
    "    \n",
    "test['lemmas'] = reduced_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exporting training/testing dataframes to csv'\n",
    "train.to_csv('lemmatized_train.csv')\n",
    "test.to_csv('lemmatized_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search: Feature Generation, Dimensionality Reduction, Oversampling and Machine Learning\n",
    "\n",
    "In this cell, we will create a pipeline that iterates through several different parameters and models. For feature generation, we will attempt both tf-idf vectorization and bag-of-words vectorization, which will convert our list of strings into sparse dataframes. Bag-of-words is a simplistic representation of text that considers only term frequency and assumes that position doesn't matter. Tf-idf is a more nuanced representation of text that considers the term frequency as well the inverse document frequency, which gives more information about how significant each word is. For example, a common word such as \"the\" may score high in term frequency, but having high term frequency in every document would diminish its meaning and thus its tf-idf score. Both bag-of-words and tf-idf have several parameters that will be optimized through the gridsearch. For brevity's sake, we are only tuning the max_df and min_df, which represent the maximum and minimum number of times a term should appear in any given document.\n",
    "\n",
    "Beyond using singular words or lemmas as features for classification, we can also use groupings of words that appear together, as they may convey more meaning than each word isolated by itself. We will use the native ngram_range hyperparameter to broaden our dataset to include common bigrams as well. Bigrams (2 consecutive words) are preferable as groupings larger than 2 words in a dataset this small would likely create unnecessary noise without adding any insight.\n",
    "\n",
    "We will likely need a way to reduce dimensionality, as dimensionality reduction reduces the storage and computation requirements of our model.  We will implement some permutations of the pipeline with Truncated Singular Value Decomposition which works well with sparse data. This transformation is termed Latent Semantic Analysis, a technique that is known to combat polysemy and synonymy. For the permutations that use Truncated SVD, I chose to reduce to 500 components based on a scree plot that was generated outside of this notebook. In the scree plot, 500 components appeared to capture roughly 85% of the variance of the dataset, a well-known standard for dimensionality reduction.\n",
    "\n",
    "Source: http://scikit-learn.org/stable/modules/decomposition.html#truncated-singular-value-decomposition-and-latent-semantic-analysis\n",
    "\n",
    "As mentioned before, we will also implement oversampling via SMOTE to combat the class imbalance. SMOTE (Synthetic Minority Oversampling Technique) uses the K Neighbors algorithm to synthesize new datapoints that resemble the existing classes. The default number of neighbors to use is 5. In an effort to reduce computational complexity, we will hold off on tuning other parameters.\n",
    "\n",
    "Finally, we will apply several different machine learning models and evaluate their performance using cross-validation with log-loss scoring. First, we will try a multinomial Naive Bayes model, which naively assumes that all features are independent from one another. The multinomial model is commonly used for NLP problems and works under the bag-of-words assumption that position doesn't matter. The sklearn MultinomialNB documentation states that \"the multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\"\n",
    "\n",
    "Source: https://web.stanford.edu/class/cs124/lec/naivebayes.pdf, http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "We will also try random forests, as ensemble models tend to be robust against overfitting. Random forest language models have been shown to generalize well to unseen data, which is important in predictive modeling. The random forest has several parameters that will be tuned with the high dimensionality of the dataset in mind.\n",
    "\n",
    "Source: http://www.aclweb.org/anthology/W04-3242\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in training dataframe\n",
    "train = pd.read_csv('~/tumor-mutation-classification/lemmatized_train.csv')\n",
    "X = train['lemmatized']\n",
    "Y = train['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "#    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 5)),\n",
    "    ('machine', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Feature generation parameters\n",
    "maxdf = [0.25, 0.5, 0.75]\n",
    "mindf = [0.01, 0.05, 0.1]\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "# Machine learning model parameters \n",
    "C_values = [1e-3, 1e-1, 1, 100]\n",
    "\n",
    "param_grid = [\n",
    "    # Tfidf permutations first\n",
    "    # With and without oversampling, ngrams\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    # Bag of words permutations next\n",
    "    # With and without oversampling, ngrams\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 500)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 500)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run grid search with cross validation\n",
    "print('running grid search with logistic regression...')\n",
    "grid = GridSearchCVProgressBar(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')\n",
    "grid.fit(X, Y)\n",
    "print(f'best score: \\n {grid.best_score_}')\n",
    "print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have run the grid search, we will export our findings to a csv so that we can access them without having to run the expensive grid search every time. We will create a function that can be re-used throughout the notebook to export all the grid search findings from each machine learning model type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort and export all results for grid search to csv\n",
    "def export_gridsearch_to_csv(gs_clf, export_file):\n",
    "    with open(export_file, 'w') as outfile:\n",
    "        csvwriter = csv.writer(outfile, delimiter=',')\n",
    "\n",
    "        # Create the header using the parameter names \n",
    "        header = [\"mean\",\"std\", \"params\"]\n",
    "\n",
    "        csvwriter.writerow(header)\n",
    "\n",
    "        sorted_by_score = sorted(gs_clf.grid_scores_, key = itemgetter(1), reverse=True)\n",
    "\n",
    "        for config in sorted_by_score:\n",
    "            # Get mean and standard deviation\n",
    "            mean = np.abs(config[1])\n",
    "            std = np.std(config[2])\n",
    "            row = [mean,std, str(config[0])]\n",
    "\n",
    "            csvwriter.writerow(row)\n",
    "\n",
    "filename = datetime.datetime.utcnow().strftime('logistic_gridresults_%Y%m%d_%H:%M.csv')       \n",
    "export_gridsearch_to_csv(grid, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "#    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 5)),\n",
    "    ('machine', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Feature generation parameters\n",
    "maxdf = [0.25, 0.5, 0.75]\n",
    "mindf = [0.01, 0.05, 0.1]\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "# Machine learning model parameters \n",
    "rf_max_depth = [30, 60, 100]\n",
    "rf_min_samples_split = [430]\t# 430 represents 5% of the dataset after oversampling\n",
    "rf_n_estimators = [10]\n",
    "\n",
    "param_grid = [\n",
    "    # Tfidf permutations with and without n-grams\n",
    "    # Removed permutations with CountVectorizer after logistic regression demonstrated poor log loss scores\n",
    "    # Removed permutations without oversampling after logistic regression demonstrated poor log loss scores\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        #'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [RandomForestClassifier(random_state = 1)],\n",
    "        'machine__max_depth': rf_max_depth,\n",
    "        'machine__min_samples_split': rf_min_samples_split,\n",
    "        'machine__n_estimators': rf_n_estimators\n",
    "    },\n",
    "        {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500, ngram_range=(2,2))],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        #'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [RandomForestClassifier(random_state = 1)],\n",
    "        'machine__max_depth': rf_max_depth,\n",
    "        'machine__min_samples_split': rf_min_samples_split,\n",
    "        'machine__n_estimators': rf_n_estimators\n",
    "    }\n",
    "]\n",
    "# Run grid search with cross validation\n",
    "print('running grid search with random forest...')\n",
    "grid = GridSearchCVProgressBar(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')\n",
    "grid.fit(X, Y)\n",
    "print(f'best score: \\n {grid.best_score_}')\n",
    "print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = datetime.datetime.utcnow().strftime('rf_gridresults_%Y%m%d_%H:%M.csv')       \n",
    "export_gridsearch_to_csv(grid, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "#    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 5)),\n",
    "    ('machine', XGBClassifier())\n",
    "])\n",
    "\n",
    "# Feature generation parameters\n",
    "# max_df and min_df values reduced to best-performing values from logistic/random forest grid search\n",
    "maxdf = [0.75]\n",
    "mindf = [0.05]\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "# Machine learning model parameters \n",
    "xg_booster = ['gbtree','gblinear']\n",
    "xg_learning_rate = [0.01, 0.1, 0.3]\n",
    "xg_max_depth = [30, 60, 100]\n",
    "xg_subsample = [0.5, 1]\n",
    "\n",
    "param_grid = [\n",
    "# Tfidf permutations with and without n-grams\n",
    "# Removed permutations with CountVectorizer after logistic regression demonstrated poor log loss scores\n",
    "# Removed permutations without oversampling after logistic regression demonstrated poor log loss scores\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        #'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [XGBClassifier(random_state = 1)],\n",
    "        'machine__booster': xg_booster,\n",
    "        'machine__learning_rate': xg_learning_rate,\n",
    "        'machine__max_depth': xg_max_depth,\n",
    "        'machine__subsample': xg_subsample\n",
    "    },\n",
    "        {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500, ngram_range=(2,2))],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        #'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine__booster': xg_booster,\n",
    "        'machine__learning_rate': xg_learning_rate,\n",
    "        'machine__max_depth': xg_max_depth,\n",
    "        'machine__subsample': xg_subsample\n",
    "    }\n",
    "]\n",
    "# Run grid search with cross validation\n",
    "print('running grid search with XGBoost...')\n",
    "grid = GridSearchCVProgressBar(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')\n",
    "grid.fit(X, Y)\n",
    "print(f'best score: \\n {grid.best_score_}')\n",
    "print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = datetime.datetime.utcnow().strftime('xg_gridresults_%Y%m%d_%H:%M.csv')       \n",
    "export_gridsearch_to_csv(grid, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Pipeline (Multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "#    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 5)),\n",
    "    ('machine', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Feature generation parameters\n",
    "# max_df and min_df values reduced to best-performing values from logistic/random forest grid search\n",
    "maxdf = [0.75]\n",
    "mindf = [0.05]\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "# Default machine learning model parameters \n",
    "\n",
    "param_grid = [\n",
    "# Tfidf permutations with and without n-grams\n",
    "# Removed permutations with CountVectorizer after logistic regression demonstrated poor log loss scores\n",
    "# Removed permutations without oversampling after logistic regression demonstrated poor log loss scores\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        #'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [MultinomialNB()],\n",
    "    },\n",
    "        {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500, ngram_range=(2,2))],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        #'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [MultinomialNB()],\n",
    "    }\n",
    "]\n",
    "# Run grid search with cross validation\n",
    "print('running grid search with Naive Bayes...')\n",
    "grid = GridSearchCVProgressBar(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')\n",
    "grid.fit(X, Y)\n",
    "print(f'best score: \\n {grid.best_score_}')\n",
    "print(f'best params:\\n {grid.best_params_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = datetime.datetime.utcnow().strftime('nb_gridresults_%Y%m%d_%H:%M.csv')       \n",
    "export_gridsearch_to_csv(grid, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all of our preliminary pipelines have been run, we can take a look at the resulting log loss scores and judge which machine learning model was the most effective on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating grid search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_results = pd.read_csv('scripts/logistic_gridresults_20180629_00:02.csv')\n",
    "nb_results = pd.read_csv('scripts/nb_gridresults_20180703_18:04.csv')\n",
    "rf_results = pd.read_csv('scripts/rf_gridresults_20180701_22:50.csv')\n",
    "xg_results = pd.read_csv('scripts/xg_gridresults_20180630_17:46.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      classifier      mean       std  \\\n",
      "2  random forest  1.832495  0.011121   \n",
      "1    naive bayes  1.839022  0.044531   \n",
      "3        xgboost  1.863806  0.055248   \n",
      "0       logistic  1.888993  0.061842   \n",
      "\n",
      "                                              params  \n",
      "2  {'feat_gen': TfidfVectorizer(analyzer='word', ...  \n",
      "1  {'feat_gen': TfidfVectorizer(analyzer='word', ...  \n",
      "3  {'feat_gen': TfidfVectorizer(analyzer='word', ...  \n",
      "0  {'feat_gen': TfidfVectorizer(analyzer='word', ...  \n"
     ]
    }
   ],
   "source": [
    "best_logistic = pd.Series(['logistic', logistic_results['mean'][0], logistic_results['std'][0], logistic_results['params'][0]])\n",
    "best_nb = pd.Series(['naive bayes', nb_results['mean'][0], nb_results['std'][0], nb_results['params'][0]])\n",
    "best_rf = pd.Series(['random forest', rf_results['mean'][0], rf_results['std'][0], rf_results['params'][0]])\n",
    "best_xg = pd.Series(['xgboost', xg_results['mean'][0], xg_results['std'][0], xg_results['params'][0]])\n",
    "best_gridresults = pd.DataFrame().append([best_logistic, best_nb, best_rf, best_xg], ignore_index=True)\n",
    "best_gridresults.columns = ['classifier','mean','std','params']\n",
    "best_gridresults = best_gridresults.sort_values(by=['mean'], ascending=True)\n",
    "print(best_gridresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n"
     ]
    }
   ],
   "source": [
    "print(best_gridresults['params'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that with the lowest log loss score of 1.83, the random forest classifier performed the best on our training dataset. Decision trees/random forests are known to perform well on imbalanced datasets, which we are currently working with. This model used bigrams as features with max_depth = 30 and min_samples_split = 430. One option now is to use the hyperparameters determined from this grid search to test new methods of feature generation such as word2vec, a common unsupervised neural network approach for NLP. The resulting log loss scores from other methods of feature generation will be compared to this baseline score of 1.83."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "requires sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    large e3 ligase subfamily cul3 bind btb domain...\n",
      "1    gene rearrangement form intragenic deletion pr...\n",
      "Name: lemmatized, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-15996f4e3ce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m \u001b[0;34m,\u001b[0m  \u001b[0;31m# Penalize frequent words.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#size=300,      # Word vector length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m           \u001b[0;31m# Use hierarchical softmax.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_train_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m             self.train(\n\u001b[1;32m    337\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m             trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    256\u001b[0m                 \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                 queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0mtrained_word_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrained_word_count_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mraw_word_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mraw_word_count_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    225\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    226\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             report_delay=report_delay)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(\n",
    "    X,\n",
    "    workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=10,  # Minimum word count threshold.\n",
    "    window=6,      # Number of words around target word to consider.\n",
    "    sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    #size=300,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.vocab)\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
