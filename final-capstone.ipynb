{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tumor Mutation Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will analyze the dataset which contains tumor gene mutations and their risk category, which corresponds to their risk of malignancy in the human condition. The dataset was hand-labeled and released by the team of clinical pathologists at Memorial Sloan Kettering in 2018. Our objective of this project is to fit the dataset into our machine learning models to predict the risk category while accounting for highly unbalanced classes. Several methods for text feature generation will be explored and the resulting features will be reduced using principle component analysis (PCA). We will then use the synthetic minority over-sampling technique (SMOTE) to resample the dataset to make the numbers of categories more even. The last step is to compare the machine learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['Booster', 'DMatrix', 'VERSION_FILE', 'XGBClassifier', 'XGBModel', 'XGBRegressor', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'absolute_import', 'callback', 'compat', 'core', 'cv', 'f', 'libpath', 'os', 'plot_importance', 'plot_tree', 'plotting', 'rabit', 'sklearn', 'to_graphviz', 'train', 'training']\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "import xgboost\n",
    "print(dir(xgboost))\n",
    "print('done')\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In this section we will load in all our data and format them to the appropriate data types. The text also needs to be cleaned of all non-legitimate words, such as figure references and parentheses. This will be accomplished using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up packages for loading in data\n",
    "client = boto3.client('s3') #low-level functional API\n",
    "\n",
    "resource = boto3.resource('s3') #high-level object-oriented API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in training data labels\n",
    "obj = client.get_object(Bucket='thinkful-capstone', Key='training_variants')\n",
    "stream = io.BytesIO(obj['Body'].read())\n",
    "training_variants = pd.read_csv(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID    Gene             Variation  Class\n",
      "0   0  FAM58A  Truncating Mutations      1\n",
      "1   1     CBL                 W802*      2\n",
      "2   2     CBL                 Q249E      2\n",
      "3   3     CBL                 N454D      3\n",
      "4   4     CBL                 L399V      4\n",
      "(3321, 4)\n"
     ]
    }
   ],
   "source": [
    "print(training_variants.head())\n",
    "print(training_variants.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID||Text\n",
      "0||Cyclin-dependent kinases (CDKs) regulate a variety of fundamental cellular processes. CDK10 stands out as one of the last orphan CDKs for which no activating cyclin has been identified and no kinase activity revealed. Previous work has shown that CDK10 silencing increases ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2)-driven activation of the MAPK pathway, which confers tamoxifen resistance to breast cancer cells. The precise mechanisms by which CDK10 modulates ETS2 activity, and more generally the functions of CDK10, remain elusive. Here we demonstrate that CDK10 is a cyclin-dependent kinase by identifying cyclin M as an activating cyclin. Cyclin M, an orphan cyclin, is the product of FAM58A, whose mutations cause STAR syndrome, a human developmental anomaly whose features include toe syndactyly, telecanthus, and anogenital and renal malformations. We show that STAR syndrome-associated cyclin M mutants are unable to interact with CDK10. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and in conferring tamoxifen resistance to breast cancer cells. CDK10/cyclin M phosphorylates ETS2 in vitro, and in cells it positively controls ETS2 degradation by the proteasome. ETS2 protein levels are increased in cells derived from a STAR patient, and this increase is attributable to decreased cyclin M levels. Altogether, our results reveal an additional regulatory mechanism for ETS2, which plays key roles in cancer and development. They also shed light on the molecular mechanisms underlying STAR syndrome.Cyclin-dependent kinases (CDKs) play a pivotal role in the control of a number of fundamental cellular processes (1). The human genome contains 21 genes encoding proteins that can be considered as members of the CDK family owing to their sequence similarity with bona fide CDKs, those known to be activated by cyclins (2). Although discovered almost 20 y ago (3, 4), CDK10 remains one of the two CDKs without an identified cyclin partner. This knowledge gap has largely impeded the exploration of its biological functions. CDK10 can act as a positive cell cycle regulator in some cells (5, 6) or as a tumor suppressor in others (7, 8). CDK10 interacts with the ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2) transcription factor and inhibits its transcriptional activity through an unknown mechanism (9). CDK10 knockdown derepresses ETS2, which increases the expression of the c-Raf protein kinase, activates the MAPK pathway, and induces resistance of MCF7 cells to tamoxifen (6).Here, we deorphanize CDK10 by identifying cyclin M, the product of FAM58A, as a binding partner. Mutations in this gene that predict absence or truncation of cyclin M are associated with STAR syndrome, whose features include toe syndactyly, telecanthus, and anogenital and renal malformations in heterozygous females (10). However, both the functions of cyclin M and the pathogenesis of STAR syndrome remain unknown. We show that a recombinant CDK10/cyclin M heterodimer is an active protein kinase that phosphorylates ETS2 in vitro. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and phospho-ERK expression levels and in inducing tamoxifen resistance in estrogen receptor (ER)+ breast cancer cells. We show that CDK10/cyclin M positively controls ETS2 degradation by the proteasome, through the phosphorylation of two neighboring serines. Finally, we detect an increased ETS2 expression level in cells derived from a STAR patient, and we demonstrate that it is attributable to the decreased cyclin M expression level observed in these cells.Previous SectionNext SectionResultsA yeast two-hybrid (Y2H) screen unveiled an interaction signal between CDK10 and a mouse protein whose C-terminal half presents a strong sequence homology with the human FAM58A gene product [whose proposed name is cyclin M (11)]. We thus performed Y2H mating assays to determine whether human CDK10 interacts with human cyclin M (Fig. 1 A–C). The longest CDK10 isoform (P1) expressed as a bait protein produced a strong interaction phenotype with full-length cyclin M (expressed as a prey protein) but no detectable phenotype with cyclin D1, p21 (CIP1), and Cdi1 (KAP), which are known binding partners of other CDKs (Fig. 1B). CDK1 and CDK3 also produced Y2H signals with cyclin M, albeit notably weaker than that observed with CDK10 (Fig. 1B). An interaction phenotype was also observed between full-length cyclin M and CDK10 proteins expressed as bait and prey, respectively (Fig. S1A). We then tested different isoforms of CDK10 and cyclin M originating from alternative gene splicing, and two truncated cyclin M proteins corresponding to the hypothetical products of two mutated FAM58A genes found in STAR syndrome patients (10). None of these shorter isoforms produced interaction phenotypes (Fig. 1 A and C and Fig. S1A).Fig. 1.In a new window Download PPTFig. 1.CDK10 and cyclin M form an interaction complex. (A) Schematic representation of the different protein isoforms analyzed by Y2H assays. Amino acid numbers are indicated. Black boxes indicate internal deletions. The red box indicates a differing amino acid sequence compared with CDK10 P1. (B) Y2H assay between a set of CDK proteins expressed as baits (in fusion to the LexA DNA binding domain) and CDK interacting proteins expressed as preys (in fusion to the B42 transcriptional activator). pEG202 and pJG4-5 are the empty bait and prey plasmids expressing LexA and B42, respectively. lacZ was used as a reporter gene, and blue yeast are indicative of a Y2H interaction phenotype. (C) Y2H assay between the different CDK10 and cyclin M isoforms. The amino-terminal region of ETS2, known to interact with CDK10 (9), was also assayed. (D) Western blot analysis of Myc-CDK10 (wt or kd) and CycM-V5-6His expression levels in transfected HEK293 cells. (E) Western blot analysis of Myc-CDK10 (wt or kd) immunoprecipitates obtained using the anti-Myc antibody. “Inputs” correspond to 10 μg total lysates obtained from HEK293 cells coexpressing Myc-CDK10 (wt or kd) and CycM-V5-6His. (F) Western blot analysis of immunoprecipitates obtained using the anti-CDK10 antibody or a control goat antibody, from human breast cancer MCF7 cells. “Input” corresponds to 30 μg MCF7 total cell lysates. The lower band of the doublet observed on the upper panel comigrates with the exogenously expressed untagged CDK10 and thus corresponds to endogenous CDK10. The upper band of the doublet corresponds to a nonspecific signal, as demonstrated by it insensitivity to either overexpression of CDK10 (as seen on the left lane) or silencing of CDK10 (Fig. S2B). Another experiment with a longer gel migration is shown in Fig. S1D.Next we examined the ability of CDK10 and cyclin M to interact when expressed in human cells (Fig. 1 D and E). We tested wild-type CDK10 (wt) and a kinase dead (kd) mutant bearing a D181A amino acid substitution that abolishes ATP binding (12). We expressed cyclin M-V5-6His and/or Myc-CDK10 (wt or kd) in a human embryonic kidney cell line (HEK293). The expression level of cyclin M-V5-6His was significantly increased upon coexpression with Myc-CDK10 (wt or kd) and, to a lesser extent, that of Myc-CDK10 (wt or kd) was increased upon coexpression with cyclin M-V5-6His (Fig. 1D). We then immunoprecipitated Myc-CDK10 proteins and detected the presence of cyclin M in the CDK10 (wt) and (kd) immunoprecipitates only when these proteins were coexpressed pair-wise (Fig. 1E). We confirmed these observations by detecting the presence of Myc-CDK10 in cyclin M-V5-6His immunoprecipitates (Fig. S1B). These experiments confirmed the lack of robust interaction between the CDK10.P2 isoform and cyclin M (Fig. S1C). To detect the interaction between endogenous proteins, we performed immunoprecipitations on nontransfected MCF7 cells derived from a human breast cancer. CDK10 and cyclin M antibodies detected their cognate endogenous proteins by Western blotting. We readily detected cyclin M in immunoprecipitates obtained with the CDK10 antibody but not with a control antibody (Fig. 1F). These results confirm the physical interaction between CDK10 and cyclin M in human cells.To unveil a hypothesized CDK10/cyclin M protein kinase activity, we produced GST-CDK10 and StrepII-cyclin M fusion proteins in insect cells, either individually or in combination. We observed that GST-CDK10 and StrepII-cyclin M copurified, thus confirming their interaction in yet another cellular model (Fig. 2A). We then performed in vitro kinase assays with purified proteins, using histone H1 as a generic substrate. Histone H1 phosphorylation was detected only from lysates of cells coexpressing GST-CDK10 and StrepII-cyclin M. No phosphorylation was detected when GST-CDK10 or StrepII-cyclin M were expressed alone, or when StrepII-cyclin M was coexpressed with GST-CDK10(kd) (Fig. 2A). Next we investigated whether ETS2, which is known to interact with CDK10 (9) (Fig. 1C), is a phosphorylation substrate of CDK10/cyclin M. We detected strong phosphorylation of ETS2 by the GST-CDK10/StrepII-cyclin M purified heterodimer, whereas no phosphorylation was detected using GST-CDK10 alone or GST-CDK10(kd)/StrepII-cyclin M heterodimer (Fig. 2B).Fig. 2.In a new window Download PPTFig. 2.CDK10 is a cyclin M-dependent protein kinase. (A) In vitro protein kinase assay on histone H1. Lysates from insect cells expressing different proteins were purified on a glutathione Sepharose matrix to capture GST-CDK10(wt or kd) fusion proteins alone, or in complex with STR-CycM fusion protein. Purified protein expression levels were analyzed by Western blots (Top and Upper Middle). The kinase activity was determined by autoradiography of histone H1, whose added amounts were visualized by Coomassie staining (Lower Middle and Bottom). (B) Same as in A, using purified recombinant 6His-ETS2 as a substrate.CDK10 silencing has been shown to increase ETS2-driven c-RAF transcription and to activate the MAPK \n"
     ]
    }
   ],
   "source": [
    "# Load in training data text articles\n",
    "obj = client.get_object(Bucket=\"thinkful-capstone\",Key=\"training_text\")\n",
    "raw_training_text = obj[\"Body\"].read()\n",
    "training_text = raw_training_text.decode('utf-8')\n",
    "print(training_text[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211296707\n",
      "205598350\n",
      "ID||Text\n",
      "0||Cyclin-dependent kinases (CDKs) regulate a variety of fundamental cellular processes. CDK10 stands out as one of the last orphan CDKs for which no activating cyclin has been identified and no kinase activity revealed. Previous work has shown that CDK10 silencing increases ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2)-driven activation of the MAPK pathway, which confers tamoxifen resistance to breast cancer cells. The precise mechanisms by which CDK10 modulates ETS2 activity, and more generally the functions of CDK10, remain elusive. Here we demonstrate that CDK10 is a cyclin-dependent kinase by identifying cyclin M as an activating cyclin. Cyclin M, an orphan cyclin, is the product of FAM58A, whose mutations cause STAR syndrome, a human developmental anomaly whose features include toe syndactyly, telecanthus, and anogenital and renal malformations. We show that STAR syndrome-associated cyclin M mutants are unable to interact with CDK10. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and in conferring tamoxifen resistance to breast cancer cells. CDK10/cyclin M phosphorylates ETS2 in vitro, and in cells it positively controls ETS2 degradation by the proteasome. ETS2 protein levels are increased in cells derived from a STAR patient, and this increase is attributable to decreased cyclin M levels. Altogether, our results reveal an additional regulatory mechanism for ETS2, which plays key roles in cancer and development. They also shed light on the molecular mechanisms underlying STAR syndrome.Cyclin-dependent kinases (CDKs) play a pivotal role in the control of a number of fundamental cellular processes. The human genome contains 21 genes encoding proteins that can be considered as members of the CDK family owing to their sequence similarity with bona fide CDKs, those known to be activated by cyclins. Although discovered almost 20 y ago, CDK10 remains one of the two CDKs without an identified cyclin partner. This knowledge gap has largely impeded the exploration of its biological functions. CDK10 can act as a positive cell cycle regulator in some cells or as a tumor suppressor in others. CDK10 interacts with the ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2) transcription factor and inhibits its transcriptional activity through an unknown mechanism. CDK10 knockdown derepresses ETS2, which increases the expression of the c-Raf protein kinase, activates the MAPK pathway, and induces resistance of MCF7 cells to tamoxifen.Here, we deorphanize CDK10 by identifying cyclin M, the product of FAM58A, as a binding partner. Mutations in this gene that predict absence or truncation of cyclin M are associated with STAR syndrome, whose features include toe syndactyly, telecanthus, and anogenital and renal malformations in heterozygous females. However, both the functions of cyclin M and the pathogenesis of STAR syndrome remain unknown. We show that a recombinant CDK10/cyclin M heterodimer is an active protein kinase that phosphorylates ETS2 in vitro. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and phospho-ERK expression levels and in inducing tamoxifen resistance in estrogen receptor (ER)+ breast cancer cells. We show that CDK10/cyclin M positively controls ETS2 degradation by the proteasome, through the phosphorylation of two neighboring serines. Finally, we detect an increased ETS2 expression level in cells derived from a STAR patient, and we demonstrate that it is attributable to the decreased cyclin M expression level observed in these cells.Previous SectionNext SectionResultsA yeast two-hybrid (Y2H) screen unveiled an interaction signal between CDK10 and a mouse protein whose C-terminal half presents a strong sequence homology with the human FAM58A gene product [whose proposed name is cyclin M]. We thus performed Y2H mating assays to determine whether human CDK10 interacts with human cyclin M. The longest CDK10 isoform (P1) expressed as a bait protein produced a\n"
     ]
    }
   ],
   "source": [
    "# Eliminate references and abbreviations within parentheses\n",
    "print(len(training_text))\n",
    "training_text = re.sub(' \\(Fig \\d+.+?\\)', '', training_text)\n",
    "training_text = re.sub(' \\(Fig\\. \\d+.+?\\)', '', training_text)\n",
    "training_text = re.sub(' \\(\\d.*?\\)', '', training_text)\n",
    "training_text = re.sub(' \\([A-Z]\\)', '', training_text)\n",
    "print(len(training_text))\n",
    "print(training_text[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3321\n"
     ]
    }
   ],
   "source": [
    "# Split text file into list of documents\n",
    "training_list = training_text.split('||')\n",
    "training_list = training_list[2:]\n",
    "print(len(training_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID    Gene             Variation  Class  \\\n",
      "0   0  FAM58A  Truncating Mutations      1   \n",
      "1   1     CBL                 W802*      2   \n",
      "2   2     CBL                 Q249E      2   \n",
      "3   3     CBL                 N454D      3   \n",
      "4   4     CBL                 L399V      4   \n",
      "\n",
      "                                                text  \n",
      "0  Cyclin-dependent kinases (CDKs) regulate a var...  \n",
      "1   Abstract Background  Non-small cell lung canc...  \n",
      "2   Abstract Background  Non-small cell lung canc...  \n",
      "3  Recent evidence has demonstrated that acquired...  \n",
      "4  Oncogenic mutations in the monomeric Casitas B...  \n",
      "(3321, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load training text list into dataframe\n",
    "texts_df = pd.DataFrame(training_list, columns = ['text'])\n",
    "\n",
    "# Merge text dataframe with labels dataframe\n",
    "train = pd.concat([training_variants, texts_df], axis=1)\n",
    "print(train.head())\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Temporary cell to reduce data size\n",
    "#train = train[:400]\n",
    "#print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is loaded into a dataframe, let's do some preliminary data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7    953\n",
      "4    686\n",
      "1    568\n",
      "2    452\n",
      "6    275\n",
      "5    242\n",
      "3     89\n",
      "9     37\n",
      "8     19\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3321 total datapoints to work with, and it looks like we are dealing with significant class imbalance. Class 7 has 953 datapoints, while Class 8 has only 19. We will have to address this class imbalance with our experiment design. Additionally, the labels have been anonymized, which means we cannot draw any insight about what these classes might signify. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Design\n",
    "\n",
    "The prevalence of class imbalance has serious implications for our analysis. First and foremost, we must establish our scoring metric. The purpose here is to use the relevant clinical texts to predict the mutation category for each gene/mutation pair. While we want the predictions to be as accurate as possible, simple classification accuracy is not a representative way to judge models that are built on class imbalance, as they may achieve high accuracy by simply predicting the most common class every time. <br>\n",
    "\n",
    "Given that we are working with a multi-label classifier, the most appropriate scoring metric is log loss. Log loss quantifies the accuracy of a classifier by penalising false classifications, and heavily penalises classifiers that are confident about an incorrect classification. For this reason, it is more suitable than traditional accuracy for datasets with class imbalance. <br>\n",
    "\n",
    "When feeding our data into the predictive models, each resulting prediction is associated with a probability, and each probability is multiplied by one another to get the overall probability that all of those outcomes occurred together. As each event gets multiplied in, the final number gets smaller and smaller. So, we take the log to put the number in a more accessible range. The number is then multiplied by -1 to maintain the convention that a lower loss score is better. We will be choosing models with log loss scores closer to 0. We will also look at the precision and recall via the F1 score. Though these are not optimized for multi-label classification, they will be interesting to consider. <br>\n",
    "\n",
    "Source: https://datawookie.netlify.com/blog/2015/12/making-sense-of-logarithmic-loss/, https://www.kaggle.com/dansbecker/what-is-log-loss\n",
    "\n",
    "We will also try oversampling the lesser-represented categories and apply our machine learning models on the oversampled datasets, judging by their log loss scores. Oversampling can be achieved by generating duplicate datapoints or by generating new synthetic datapoints via SMOTE, the Synthetic Minority Oversampling Technique. We will use SMOTE, a feature of the imbalanced learn package. <br>\n",
    "\n",
    "I will use various methods of feature generation including classic NLP techniques such as bag-of-words, tf-idf, and n-grams. These methods of feature generation will be applied to both the original and oversampled datasets. They will then be subjected to various machine learning models. Decision trees are known to perform well on unbalanced datasets, so this model may prevail on the original data. However, Naive Bayes is known to perform well on natural langauge, so once the dataset is oversampled it is possible that Naive Bayes will perform the best. All machine learning models will be run with a variety of hyperparameters on a variety of datasets, and the permutation with the best log-loss score will be chosen.\n",
    "\n",
    "It is important to note that by convention, grid search always tries to maximize its score so loss functions like log loss will have to be negated such that the lowest log loss score will be the highest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is relatively clean already, and contains no NaN values. It needs to be tokenized so it can be processed into readable pieces of data. We will use spaCy to tokenize the data and create a new column with a list of the tokens for each row. Furthermore, we will convert all tokens that are not stop words or punctuation to lemmas to reduce the noise from unnecessary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'o', 'his', 'whom', 'having', 'off', 'over', 'did', 'now', 'doing', 'needn', 'between', 'same', 'other', 'haven', 'wasn', 'most', \"you've\", 'a', 'myself', \"weren't\", 'isn', 'had', 'been', 'was', \"it's\", 'each', 'so', 'will', 'ain', 'an', 's', 'have', 'hadn', 'weren', 'down', 'or', \"shan't\", 'yourself', 'these', 'there', 'once', \"won't\", 'against', 'both', 'hers', 'no', 'own', 'does', \"should've\", 'won', \"isn't\", 'shan', 'were', 'you', 'what', 'don', 'more', 'themselves', \"didn't\", 'ourselves', 'some', 've', 'she', 'why', 'for', 'couldn', 'here', \"that'll\", 'such', \"hadn't\", 'if', 'where', \"haven't\", 'doesn', 'hasn', 'and', 'during', \"hasn't\", 'from', 'out', 'itself', \"doesn't\", 'into', 'him', 'which', 'further', 'is', 'himself', 'then', 'while', 'them', 'up', \"you're\", \"don't\", 't', 'are', 'he', 'under', 'can', 'has', \"shouldn't\", \"you'd\", 'theirs', 'before', 'm', 'all', 'few', 'wouldn', 'll', 'my', 'too', 'until', \"mustn't\", 'that', 'the', 'with', 'very', 'they', 'their', 'am', 'just', 'about', 'those', 'when', \"couldn't\", 'me', 'd', 'your', \"you'll\", 'aren', 'yourselves', 'at', 'yours', 'because', 'we', \"mightn't\", 'but', \"wouldn't\", 'herself', 'as', 'on', 'to', 'nor', 'be', 'above', 'after', 'her', 're', 'shouldn', \"wasn't\", 'in', 'of', 'below', \"aren't\", 'ours', 'only', 'do', 'mustn', \"needn't\", 'mightn', 'its', 'ma', \"she's\", 'should', 'not', 'by', 'didn', 'again', 'through', 'this', 'it', 'than', 'i', 'any', 'y', 'who', 'our', 'how', 'being'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning lemmatization\n",
      "   ID    Gene             Variation  Class  \\\n",
      "0   0  FAM58A  Truncating Mutations      1   \n",
      "1   1     CBL                 W802*      2   \n",
      "2   2     CBL                 Q249E      2   \n",
      "3   3     CBL                 N454D      3   \n",
      "4   4     CBL                 L399V      4   \n",
      "\n",
      "                                                text  \\\n",
      "0  Cyclin-dependent kinases (CDKs) regulate a var...   \n",
      "1   Abstract Background  Non-small cell lung canc...   \n",
      "2   Abstract Background  Non-small cell lung canc...   \n",
      "3  Recent evidence has demonstrated that acquired...   \n",
      "4  Oncogenic mutations in the monomeric Casitas B...   \n",
      "\n",
      "                                        spacy_tokens  \\\n",
      "0  (Cyclin, -, dependent, kinases, (, CDKs, ), re...   \n",
      "1  ( , Abstract, Background,  , Non, -, small, ce...   \n",
      "2  ( , Abstract, Background,  , Non, -, small, ce...   \n",
      "3  (Recent, evidence, has, demonstrated, that, ac...   \n",
      "4  (Oncogenic, mutations, in, the, monomeric, Cas...   \n",
      "\n",
      "                                              lemmas  \n",
      "0  [cyclin, dependent, kinase, cdks, regulate, va...  \n",
      "1  [abstract, background, non, small, cell, lung,...  \n",
      "2  [abstract, background, non, small, cell, lung,...  \n",
      "3  [recent, evidence, demonstrate, acquire, unipa...  \n",
      "4  [oncogenic, mutation, monomeric, casitas, b, l...  \n"
     ]
    }
   ],
   "source": [
    "print('beginning lemmatization')\n",
    "train['spacy_tokens'] = train['text'].apply(lambda x: nlp(x))\n",
    "\n",
    "def lemmatize(x):\n",
    "    intermediate_lemmas = [token.lemma_.lower() for token in x\n",
    "            if not token.is_punct]\n",
    "    return [lemma for lemma in intermediate_lemmas\n",
    "           if lemma not in stop_words\n",
    "           and lemma != \"-PRON-\"\n",
    "           and lemma != \" \"\n",
    "           ]\n",
    "\n",
    "train['lemmas'] = train['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our lemmatized datapoints, we must convert them into a list of strings to feed to our feature generators.\n",
    "\n",
    "# make sure to run grid search in tmux pane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Establishing our test datasets\n",
    "X = train['lemmas']\n",
    "Y = train['Class']\n",
    "\n",
    "X_lemma_documents = [\n",
    "    ' '.join([str(word) for word in text])\n",
    "    for text in X.values.tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reduce documents to their N most common words in order to reduce unnecessary input to vectorizer. First, we will create a dictionary of all the words in the copora and their frequencies using Counter. Then we will graph the words by frequency to see how many words make up the majority of our corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of words and frequencies\n",
    "counter = Counter()\n",
    "\n",
    "# Iterate through each document, split into words and add words/frequencies to Counter\n",
    "for document in X_lemma_documents:\n",
    "    words = document.split(' ')\n",
    "    counter.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mutation', 326668], ['cell', 297261]]\n"
     ]
    }
   ],
   "source": [
    "# Convert term/frequency dictionary to list sorted by frequency\n",
    "word_frequencies = sorted(\n",
    "                        [[key, value] for key, value in counter.items()],\n",
    "                        key = lambda x: x[1],\n",
    "                        reverse = True\n",
    "                        )\n",
    "\n",
    "print(word_frequencies[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of terms: 228309\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nu8p/VQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9a2cf9e9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot term frequency, trim list to 2000 most common words for easier visualization\n",
    "print('Total number of terms:', len(word_frequencies))\n",
    "trim_length = 2000\n",
    "trim_frequencies = word_frequencies[:trim_length]\n",
    "frequency_X = list(range(len(trim_frequencies)))\n",
    "frequency_Y = [frequency_pair[1] for frequency_pair in trim_frequencies]\n",
    "plt.plot(frequency_X, frequency_Y)\n",
    "plt.xlabel('Rank of Frequency')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the term frequencies drop off after the first 250 words. After 250, the term frequencies become very sparse and will likely add no value to our model. Still, we will keep the 500 most common words for input to our feature generator to ensure that we are not losing any potentially valuable features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_length = 500\n",
    "common_words = [frequency_pair[0] for frequency_pair in word_frequencies[:common_length]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to maintain the format of our dataframe, we must reduce each individual document to these 1000 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_documents = []\n",
    "\n",
    "for document in X_lemma_documents:\n",
    "    words = document.split(' ')\n",
    "    reduced_document = [word for word in words\n",
    "                       if word in common_words]\n",
    "    reduced_document = ' '.join(reduced_document)\n",
    "    reduced_documents.append(reduced_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export list of complete lemmatized documents filtered down to most common words to speed up future runs\n",
    "def export_list_to_csv(document_list, csv_file):\n",
    "    with open(csv_file, \"w\") as outfile:\n",
    "        writer = csv.writer(outfile, lineterminator='\\n')\n",
    "        for val in document_list:\n",
    "            writer.writerow([val])    \n",
    "                  \n",
    "export_list_to_csv(reduced_documents, 'reduced_document_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "\n",
    "Beyond using singular words or lemmas as features for classification, we can also use groupings of words that appear together, as they may convey more meaning than each word isolated by itself. We will create a dataset of bigrams (2 consecutive words) as groupings larger than 2 words in a dataset this small would likely create unnecessary noise without adding any insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export labeled dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngrams(input_text, n):\n",
    "  input_text = input_text.split(' ')\n",
    "  words = []\n",
    "  for i in range(len(input_text)-n+1):\n",
    "    words.append(input_text[i:i+n])\n",
    "  return words\n",
    "\n",
    "joined_document_bigrams = []\n",
    "corpora_bigram_list = []\n",
    "counter = Counter()\n",
    "\n",
    "for document in reduced_documents[:1]:\n",
    "    joined_document_bigrams = ngrams(document, 2)\n",
    "    \n",
    "    for bigram in joined_document_bigrams:\n",
    "        joined_bigram = bigram[0] + ' ' + bigram[1]\n",
    "        joined_document_bigrams.append(joined_bigram)\n",
    "        \n",
    "    counter.update(joined_document_bigrams)\n",
    "    corpora_bigram_list.append(joined_document_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert term/frequency dictionary to sorted list\n",
    "word_frequencies = sorted(\n",
    "                        [[key, value] for key, value in counter.items()],\n",
    "                        key = lambda x: x[1],\n",
    "                        reverse = True\n",
    "                        )\n",
    "\n",
    "print(word_frequencies[:2])\n",
    "\n",
    "# Plot term frequency, trim list to 1000 most common words for better visualization\n",
    "print(len(word_frequencies))\n",
    "trim_length = 1000\n",
    "trim_frequencies = word_frequencies[:trim_length]\n",
    "frequency_X = list(range(len(trim_frequencies)))\n",
    "frequency_Y = [frequency_pair[1] for frequency_pair in trim_frequencies]\n",
    "plt.plot(frequency_X, frequency_Y)\n",
    "plt.xlabel('Rank of Frequency')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will look at the term frequency of each bigram to deduce how many terms we should keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for element in joined_bigram_list[:3]:\n",
    "    print(element)\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of words and frequencies\n",
    "counter = Counter()\n",
    "words = []\n",
    "\n",
    "joined_bigrams = []\n",
    "\n",
    "for document in X_lemma_documents:\n",
    "    document_bigrams = ngrams(document, 2)\n",
    "    \n",
    "    for bigram in document_bigrams[:10]:\n",
    "        joined_bigram = bigram[0] + ' ' + bigram[1]\n",
    "        joined_bigrams.append(joined_bigram)\n",
    "    \n",
    "    counter.update(joined_bigrams)\n",
    "    \n",
    "# Convert term/frequency dictionary to sorted list\n",
    "word_frequencies = sorted(\n",
    "                        [[key, value] for key, value in counter.items()],\n",
    "                        key = lambda x: x[1],\n",
    "                        reverse = True\n",
    "                        )\n",
    "\n",
    "print(word_frequencies[:2])\n",
    "\n",
    "# Plot term frequency, trim list to 1000 most common words for better visualization\n",
    "print(len(word_frequencies))\n",
    "trim_length = 1000\n",
    "trim_frequencies = word_frequencies[:trim_length]\n",
    "frequency_X = list(range(len(trim_frequencies)))\n",
    "frequency_Y = [frequency_pair[1] for frequency_pair in trim_frequencies]\n",
    "plt.plot(frequency_X, frequency_Y)\n",
    "plt.xlabel('Rank of Frequency')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search: Feature Generation, Dimensionality Reduction, Oversampling and Machine Learning\n",
    "\n",
    "In this cell, we will create a pipeline that iterates through several different parameters and models. For feature generation, we will attempt both tf-idf vectorization and bag-of-words vectorization, which will convert our list of strings into sparse dataframes. Bag-of-words is a simplistic representation of text that considers only term frequency and assumes that position doesn't matter. Tf-idf is a more nuanced representation of text that considers the term frequency as well the inverse document frequency, which gives more information about how significant each word is. For example, a common word such as \"the\" may score high in term frequency, but having high term frequency in every document would diminish its meaning and thus its tf-idf score. Both of bag-of-words and tf-idf have several parameters that will be optimized through the gridsearch. For brevity's sake, we are only tuning the max_df and min_df, which represent the maximum and minimum number of times a term should appear in any given document.\n",
    "\n",
    "We will likely need a way to reduce dimensionality, as dimensionality reduction reduces the storage and computation requirements of our model.  We will implement some permutations of the pipeline with Truncated Singular Value Decomposition which works well with sparse data. This transformation is termed Latent Semantic Analysis, a technique that is known to combat polysemy and synonymy. For the permutations that use Truncated SVD, I chose to reduce to 500 components based on a scree plot that was generated outside of this notebook. In the scree plot, 500 components appeared to capture roughly 85% of the variance of the dataset, a well-known standard for dimensionality reduction.\n",
    "\n",
    "Source: http://scikit-learn.org/stable/modules/decomposition.html#truncated-singular-value-decomposition-and-latent-semantic-analysis\n",
    "\n",
    "As mentioned before, we will also implement oversampling via SMOTE to combat the class imbalance. SMOTE (Synthetic Minority Oversampling Technique) uses the K Neighbors algorithm to synthesize new datapoints that resemble the existing classes. The default number of neighbors to use is 5. In an effort to reduce computational complexity, we will hold off on tuning other parameters such as \n",
    "\n",
    "Finally, we will apply several different machine learning models and evaluate their performance using cross-validation with log-loss scoring. First, we will try a multinomial Naive Bayes model, which naively assumes that all features are independent from one another. The multinomial model is commonly used for NLP problems and works under the bag-of-words assumption that position doesn't matter. The sklearn MultinomialNB documentation states that \"the multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\"\n",
    "\n",
    "Source: https://web.stanford.edu/class/cs124/lec/naivebayes.pdf, http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "We will also try random forests, as ensemble models tend to be robust against overfitting. Random forest language models have been shown to generalize well to unseen data, which is important in predictive modeling. The random forest has several parameters that will be tuned with the high dimensionality of the dataset in mind.\n",
    "\n",
    "Source: http://www.aclweb.org/anthology/W04-3242\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 5)),\n",
    "    ('machine', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Feature generation paramters\n",
    "maxdf = [0.25, 0.5, 0.75]\n",
    "mindf = [0.01, 0.05, 0.1]\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "\n",
    "# Machine learning model parameters \n",
    "C_values = [1e-3, 1e-1, 1, 100, 1000]\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    # Tfidf permutations first, running through pipeline with and without dimensionality reduction/oversampling\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    # Bag of words permutations next\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid.fit(X_lemma_documents, Y)\n",
    "print(f'best score: \\n {grid.best_score_}')\n",
    "print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 5)),\n",
    "    # add configuration without oversampling/dim red\n",
    "    ('machine', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Feature generation paramters\n",
    "maxdf = [0.25, 0.5, 0.75]\n",
    "mindf = [0.01, 0.05, 0.1]\n",
    "\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "\n",
    "# Machine learning model parameters \n",
    "rf_max_depth = [30, 60, 100]\n",
    "#rf_max_features = [10, 20, 30]\n",
    "#rf_min_samples_split = [10, 20, 30]\n",
    "# look into parameter choices for RF for high dimensionality. consider higher values\n",
    "rf_n_estimators = [20, 50]\n",
    "\n",
    "param_grid = [\n",
    "    # Tfidf permutations first, running through pipeline with and without dimensionality reduction/oversampling\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [RandomForestClassifier(random_state = 1)],\n",
    "        'machine__max_depth': rf_max_depth,\n",
    "        'machine__max_features': rf_max_features,\n",
    "        'machine__min_samples_split': rf_min_samples_split,\n",
    "        'machine__n_estimators': rf_n_estimators\n",
    "    },\n",
    "    # Bag of words permutations next, running through all machine learning models\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [RandomForestClassifier(random_state = 1)],\n",
    "        'machine__max_depth': rf_max_depth,\n",
    "        'machine__max_features': rf_max_features,\n",
    "        'machine__min_samples_split': rf_min_samples_split,\n",
    "        'machine__n_estimators': rf_n_estimators\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid.fit(X_lemma_documents, Y)\n",
    "print(f'best score: \\n {grid.best_score_}')\n",
    "print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 5)),\n",
    "    # add configuration without oversampling/dim red\n",
    "    ('machine', XGBClassifier())\n",
    "])\n",
    "\n",
    "# Feature generation paramters\n",
    "maxdf = [0.25, 0.5, 0.75]\n",
    "mindf = [0.01, 0.05, 0.1]\n",
    "\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "\n",
    "# Machine learning model parameters \n",
    "xg_booster = ['gbtree','gblinear']\n",
    "xg_eta = [0.01, 0.1, 0.3]\n",
    "xg_max_depth = [5, 10]\n",
    "xg_subsample = [0.5, 1]\n",
    "\n",
    "param_grid = [\n",
    "    # Tfidf permutations first, running through pipeline with and without dimensionality reduction/oversampling\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [XGBClassifier(random_state = 1)],\n",
    "        'machine__booster': xg_booster,\n",
    "        'machine__learning_rate': xg_eta,\n",
    "        'machine__max_depth': xg_max_depth,\n",
    "        'machine__subsample': xg_subsample\n",
    "    },\n",
    "    # Bag of words permutations next, running through all machine learning models\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 1000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [XGBClassifier(random_state = 1)],\n",
    "        'machine__booster': xg_booster,\n",
    "        'machine__learning_rate': xg_eta,\n",
    "        'machine__max_depth': xg_max_depth,\n",
    "        'machine__subsample': xg_subsample\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid.fit(X_lemma_documents, Y)\n",
    "print(f'best score: \\n {grid.best_score_}')\n",
    "print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduced version of pipeline for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 1)),\n",
    "    # increase k neighbors to default\n",
    "    # add configuration without oversampling/dim red\n",
    "    ('machine', GaussianNB())\n",
    "    # why gaussian?\n",
    "])\n",
    "\n",
    "# Feature generation paramters\n",
    "maxdf = [0.25, 0.5, 0.75]\n",
    "mindf = [0.01, 0.05, 0.1]\n",
    "\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "# why dim red? why 500?\n",
    "# if time, show scree plot\n",
    "\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "\n",
    "# Machine learning model parameters \n",
    "rf_max_depth = [10, 20, 30]\n",
    "rf_max_features = [10, 20, 30]\n",
    "rf_min_samples_split = [10, 20, 30]\n",
    "# look into parameter choices for RF for high dimensionality. consider higher values\n",
    "rf_n_estimators = [10, 20]\n",
    "xg_booster = ['gbtree','gblinear']\n",
    "xg_eta = [0.01, 0.1, 0.3]\n",
    "xg_max_depth = [5, 10]\n",
    "xg_subsample = [0.5, 1]\n",
    "\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "    # Tfidf permutations first, running through all machine learning models\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer()],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'machine': [GaussianNB()] \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid.fit(X_lemma_documents, Y)\n",
    "print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to figure out how I came up with my 500 PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import datetime\n",
    "\n",
    "def export_gridsearch_to_csv(gs_clf, export_file):\n",
    "    with open(export_file, 'w') as outfile:\n",
    "        csvwriter = csv.writer(outfile, delimiter=',')\n",
    "\n",
    "        # Create the header using the parameter names \n",
    "        header = [\"mean\",\"std\", \"params\"]\n",
    "\n",
    "        csvwriter.writerow(header)\n",
    "        \n",
    "        sorted_by_score = sorted(gs_clf.grid_scores_, key = itemgetter(1), reverse=True)\n",
    "        \n",
    "        for config in sorted_by_score:\n",
    "            # Get mean and standard deviation\n",
    "            mean = np.abs(config[1])\n",
    "            std = np.std(config[2])\n",
    "            row = [mean,std, str(config[0])]\n",
    "\n",
    "            csvwriter.writerow(row)\n",
    "            \n",
    "filename = datetime.datetime.utcnow().strftime('gridresults_%Y%m%d_%H:%M.csv')       \n",
    "export_gridsearch_to_csv(grid, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "filename = datetime.datetime.utcnow().strftime('gridresults_%Y%m%d_%H%M.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original custom tfidf vectorizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(X,Y):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                                 min_df=2,\n",
    "                                 stop_words='english', \n",
    "                                 lowercase=True,\n",
    "                                 norm=u'l2',\n",
    "                                 smooth_idf=True,\n",
    "                                )\n",
    "\n",
    "    sparse_tfidf_matrix=vectorizer.fit_transform(X)\n",
    "    print(f'Number of features: {sparse_tfidf_matrix.get_shape()[1]}')\n",
    "    \n",
    "    # Densify matrix so we can convert it to a conventional dataframe to extract X/Y\n",
    "    dense_tfidf_matrix = sparse_tfidf_matrix.todense()\n",
    "    df_tfidf = pd.DataFrame(dense_tfidf_matrix)\n",
    "    df_tfidf['lemma_tokens'] = X\n",
    "    df_tfidf['class'] = Y\n",
    "    \n",
    "    return df_tfidf\n",
    "\n",
    "tfidf_unigram = tfidf_vectorizer(X_lemma_documents,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "X = tfidf_unigram\n",
    "sklearn_pca = PCA()\n",
    "sklearn_pca.fit(X)\n",
    "\n",
    "print(\n",
    "    'The percentage of total variance in the dataset explained by each',\n",
    "    'component from Sklearn PCA.\\n',\n",
    "    sklearn_pca.explained_variance_ratio_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_lemma_documents[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_bigram = tfidf_vectorizer(bigrams, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning Methods\n",
    "\n",
    "Here, we will attempt to classify the training data using several different machine learning classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(tX_train, tY_train)\n",
    "test_pred = gnb.predict(tX_test)\n",
    "print(f'Testing Accuracy: {accuracy_score(tY_test, test_pred)}')\n",
    "print(f'Cross Val Score: {cross_val_score(gnb, tX_train, tY_train, cv=3).mean()}')\n",
    "print(pd.crosstab(tY_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer(\n",
    "                            stop_words='english', \n",
    "                            lowercase=True,\n",
    "                            norm=u'l2',\n",
    "                            smooth_idf=True\n",
    "                        )\n",
    "nb = GaussianNB()\n",
    "\n",
    "pipe = Pipeline(steps=[('tfidf', tfidf), ('nb', nb)])\n",
    "\n",
    "tfidf_maxdf = [0.25, 0.5, 0.75]\n",
    "tfidf_mindf = [2, 5, 20]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'tfidf__max_df': tfidf_maxdf,\n",
    "        'tfidf__min_df': tfidf_mindf,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, cv=5, n_jobs=1, param_grid=param_grid)\n",
    "grid.fit(X_train, Y_train)\n",
    "#print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, decomposition, datasets\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feat_gen', tfidf_vectorizer()),\n",
    "    ('reduce_dim', decomposition.PCA())\n",
    "    ('classify', GaussianNB())\n",
    "])\n",
    "\n",
    "tfidf_maxdf = [0.25, 0.5, 0.75]\n",
    "tfidf_mindf = [2, 5, 20]\n",
    "\n",
    "param_grid_1 = [\n",
    "    {\n",
    "        'feat_gen': [tfidf_vectorizer(                            \n",
    "                                        stop_words='english', \n",
    "                                        lowercase=True,\n",
    "                                        norm=u'l2',\n",
    "                                        smooth_idf=True\n",
    "                                    )],\n",
    "        'feat_gen__max_df': tfidf_maxdf,\n",
    "        'feat_gen__min_df': tfidf_mindf,\n",
    "        'classify__C': C_OPTIONS\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [SelectKBest(chi2)],\n",
    "        'feat_gen__k': N_FEATURES_OPTIONS,\n",
    "        'classify__C': C_OPTIONS\n",
    "    },\n",
    "]\n",
    "\n",
    "param_grid_2 = dict(feat_gen=[tfidf_vectorizer(), count_vectorizer()],\n",
    "                    reduce_dim = [PCA(), tSNE()]\n",
    "                    clf = [GaussianNB()])\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, n_jobs=1, param_grid=param_grid)\n",
    "grid.fit(X, Y)\n",
    "\n",
    "##### needs to use a real class not a custom function\n",
    "##### input must be X list of strings (each string corresponds to a single document) and Y of same size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
