{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tumor Mutation Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will analyze a dataset of tumor gene mutations and their associated 'risk category', which indicates the risk of malignancy for that mutation in the human condition. The dataset was hand-labeled and released by the team of clinical pathologists at Memorial Sloan Kettering in 2018. Our objective of this project is to fit the dataset into various machine learning models to predict the risk category while accounting for highly unbalanced classes. Several methods for text feature generation will be explored. We will then use the synthetic minority over-sampling technique (SMOTE) to resample the dataset to make the numbers of categories more even. The last step is to compare machine learning methods on our feature sets and tune our models to achieve maximum performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import linear_model, decomposition, datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from gensim import models\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In this section we will load in all of our data. The text also needs to be cleaned of all non-legitimate words, such as figure references and parentheses. This will be accomplished using regular expressions. We have been provided a training dataset of several thousand labeled peer-reviewed journal articles. We have also been provided a testing dataset for external validation, but this dataset is not labeled so we will not be able to use it at this time to assist with tuning our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up packages for loading in data\n",
    "client = boto3.client('s3') #low-level functional API\n",
    "\n",
    "resource = boto3.resource('s3') #high-level object-oriented API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in training data labels\n",
    "obj = client.get_object(Bucket='thinkful-capstone', Key='training_variants')\n",
    "stream = io.BytesIO(obj['Body'].read())\n",
    "training_variants = pd.read_csv(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID    Gene             Variation  Class\n",
      "0   0  FAM58A  Truncating Mutations      1\n",
      "1   1     CBL                 W802*      2\n",
      "2   2     CBL                 Q249E      2\n",
      "3   3     CBL                 N454D      3\n",
      "4   4     CBL                 L399V      4\n",
      "(3321, 4)\n"
     ]
    }
   ],
   "source": [
    "print(training_variants.head())\n",
    "print(training_variants.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID||Text\n",
      "0||Cyclin-dependent kinases (CDKs) regulate a variety of fundamental cellular processes. CDK10 stands out as one of the last orphan CDKs for which no activating cyclin has been identified and no kinase activity revealed. Previous work has shown that CDK10 silencing increases ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2)-driven activation of the MAPK pathway, which confers tamoxifen resistance to breast cancer cells. The precise mechanisms by which CDK10 modulates ETS2 activity, and more generally the functions of CDK10, remain elusive. Here we demonstrate that CDK10 is a cyclin-dependent kinase by identifying cyclin M as an activating cyclin. Cyclin M, an orphan cyclin, is the product of FAM58A, whose mutations cause STAR syndrome, a human developmental anomaly whose features include toe syndactyly, telecanthus, and anogenital and renal malformations. We show that STAR syndrome-associated cyclin M mutants are unable to interact with CDK10. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and in conferring tamoxifen resistance to breast cancer cells. CDK10/cyclin M phosphorylates ETS2 in vitro, and in cells it positively controls ETS2 degradation by the proteasome. ETS2 protein levels are increased in cells derived from a STAR patient, and this increase is attributable to decreased cyclin M levels. Altogether, our results reveal an additional regulatory mechanism for ETS2, which plays key roles in cancer and development. They also shed light on the molecular mechanisms underlying STAR syndrome.Cyclin-dependent kinases (CDKs) play a pivotal role in the control of a number of fundamental cellular processes (1). The human genome contains 21 genes encoding proteins that can be considered as members of the CDK family owing to their sequence similarity with bona fide CDKs, those known to be activated by cyclins (2). Although discovered almost 20 y ago (3, 4), CDK10 remains one of the two CDKs without an identified cyclin partner. This knowledge gap has largely impeded the exploration of its biological functions. CDK10 can act as a positive cell cycle regulator in some cells (5, 6) or as a tumor suppressor in others (7, 8). CDK10 interacts with the ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2) transcription factor and inhibits its transcriptional activity through an unknown mechanism (9). CDK10 knockdown derepresses ETS2, which increases the expression of the c-Raf protein kinase, activates the MAPK pathway, and induces resistance of MCF7 cells to tamoxifen (6).Here, we deorphanize CDK10 by identifying cyclin M, the product of FAM58A, as a binding partner. Mutations in this gene that predict absence or truncation of cyclin M are associated with STAR syndrome, whose features include toe syndactyly, telecanthus, and anogenital and renal malformations in heterozygous females (10). However, both the functions of cyclin M and the pathogenesis of STAR syndrome remain unknown. We show that a recombinant CDK10/cyclin M heterodimer is an active protein kinase that phosphorylates ETS2 in vitro. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and phospho-ERK expression levels and in inducing tamoxifen resistance in estrogen receptor (ER)+ breast cancer cells. We show that CDK10/cyclin M positively controls ETS2 degradation by the proteasome, through the phosphorylation of two neighboring serines. Finally, we detect an increased ETS2 expression level in cells derived from a STAR patient, and we demonstrate that it is attributable to the decreased cyclin M expression level observed in these cells.Previous SectionNext SectionResultsA yeast two-hybrid (Y2H) screen unveiled an interaction signal between CDK10 and a mouse protein whose C-terminal half presents a strong sequence homology with the human FAM58A gene product [whose proposed name is cyclin M (11)]. We thus performed Y2H mating assays to determine whether human CDK10 interacts with human cyclin M (Fig. 1 A–C). The longest CDK10 isoform (P1) expressed as a bait protein produced a strong interaction phenotype with full-length cyclin M (expressed as a prey protein) but no detectable phenotype with cyclin D1, p21 (CIP1), and Cdi1 (KAP), which are known binding partners of other CDKs (Fig. 1B). CDK1 and CDK3 also produced Y2H signals with cyclin M, albeit notably weaker than that observed with CDK10 (Fig. 1B). An interaction phenotype was also observed between full-length cyclin M and CDK10 proteins expressed as bait and prey, respectively (Fig. S1A). We then tested different isoforms of CDK10 and cyclin M originating from alternative gene splicing, and two truncated cyclin M proteins corresponding to the hypothetical products of two mutated FAM58A genes found in STAR syndrome patients (10). None of these shorter isoforms produced interaction phenotypes (Fig. 1 A and C and Fig. S1A).Fig. 1.In a new window Download PPTFig. 1.CDK10 and cyclin M form an interaction complex. (A) Schematic representation of the different protein isoforms analyzed by Y2H assays. Amino acid numbers are indicated. Black boxes indicate internal deletions. The red box indicates a differing amino acid sequence compared with CDK10 P1. (B) Y2H assay between a set of CDK proteins expressed as baits (in fusion to the LexA DNA binding domain) and CDK interacting proteins expressed as preys (in fusion to the B42 transcriptional activator). pEG202 and pJG4-5 are the empty bait and prey plasmids expressing LexA and B42, respectively. lacZ was used as a reporter gene, and blue yeast are indicative of a Y2H interaction phenotype. (C) Y2H assay between the different CDK10 and cyclin M isoforms. The amino-terminal region of ETS2, known to interact with CDK10 (9), was also assayed. (D) Western blot analysis of Myc-CDK10 (wt or kd) and CycM-V5-6His expression levels in transfected HEK293 cells. (E) Western blot analysis of Myc-CDK10 (wt or kd) immunoprecipitates obtained using the anti-Myc antibody. “Inputs” correspond to 10 μg total lysates obtained from HEK293 cells coexpressing Myc-CDK10 (wt or kd) and CycM-V5-6His. (F) Western blot analysis of immunoprecipitates obtained using the anti-CDK10 antibody or a control goat antibody, from human breast cancer MCF7 cells. “Input” corresponds to 30 μg MCF7 total cell lysates. The lower band of the doublet observed on the upper panel comigrates with the exogenously expressed untagged CDK10 and thus corresponds to endogenous CDK10. The upper band of the doublet corresponds to a nonspecific signal, as demonstrated by it insensitivity to either overexpression of CDK10 (as seen on the left lane) or silencing of CDK10 (Fig. S2B). Another experiment with a longer gel migration is shown in Fig. S1D.Next we examined the ability of CDK10 and cyclin M to interact when expressed in human cells (Fig. 1 D and E). We tested wild-type CDK10 (wt) and a kinase dead (kd) mutant bearing a D181A amino acid substitution that abolishes ATP binding (12). We expressed cyclin M-V5-6His and/or Myc-CDK10 (wt or kd) in a human embryonic kidney cell line (HEK293). The expression level of cyclin M-V5-6His was significantly increased upon coexpression with Myc-CDK10 (wt or kd) and, to a lesser extent, that of Myc-CDK10 (wt or kd) was increased upon coexpression with cyclin M-V5-6His (Fig. 1D). We then immunoprecipitated Myc-CDK10 proteins and detected the presence of cyclin M in the CDK10 (wt) and (kd) immunoprecipitates only when these proteins were coexpressed pair-wise (Fig. 1E). We confirmed these observations by detecting the presence of Myc-CDK10 in cyclin M-V5-6His immunoprecipitates (Fig. S1B). These experiments confirmed the lack of robust interaction between the CDK10.P2 isoform and cyclin M (Fig. S1C). To detect the interaction between endogenous proteins, we performed immunoprecipitations on nontransfected MCF7 cells derived from a human breast cancer. CDK10 and cyclin M antibodies detected their cognate endogenous proteins by Western blotting. We readily detected cyclin M in immunoprecipitates obtained with the CDK10 antibody but not with a control antibody (Fig. 1F). These results confirm the physical interaction between CDK10 and cyclin M in human cells.To unveil a hypothesized CDK10/cyclin M protein kinase activity, we produced GST-CDK10 and StrepII-cyclin M fusion proteins in insect cells, either individually or in combination. We observed that GST-CDK10 and StrepII-cyclin M copurified, thus confirming their interaction in yet another cellular model (Fig. 2A). We then performed in vitro kinase assays with purified proteins, using histone H1 as a generic substrate. Histone H1 phosphorylation was detected only from lysates of cells coexpressing GST-CDK10 and StrepII-cyclin M. No phosphorylation was detected when GST-CDK10 or StrepII-cyclin M were expressed alone, or when StrepII-cyclin M was coexpressed with GST-CDK10(kd) (Fig. 2A). Next we investigated whether ETS2, which is known to interact with CDK10 (9) (Fig. 1C), is a phosphorylation substrate of CDK10/cyclin M. We detected strong phosphorylation of ETS2 by the GST-CDK10/StrepII-cyclin M purified heterodimer, whereas no phosphorylation was detected using GST-CDK10 alone or GST-CDK10(kd)/StrepII-cyclin M heterodimer (Fig. 2B).Fig. 2.In a new window Download PPTFig. 2.CDK10 is a cyclin M-dependent protein kinase. (A) In vitro protein kinase assay on histone H1. Lysates from insect cells expressing different proteins were purified on a glutathione Sepharose matrix to capture GST-CDK10(wt or kd) fusion proteins alone, or in complex with STR-CycM fusion protein. Purified protein expression levels were analyzed by Western blots (Top and Upper Middle). The kinase activity was determined by autoradiography of histone H1, whose added amounts were visualized by Coomassie staining (Lower Middle and Bottom). (B) Same as in A, using purified recombinant 6His-ETS2 as a substrate.CDK10 silencing has been shown to increase ETS2-driven c-RAF transcription and to activate the MAPK \n"
     ]
    }
   ],
   "source": [
    "# Load in training data text articles\n",
    "obj = client.get_object(Bucket=\"thinkful-capstone\",Key=\"training_text\")\n",
    "raw_training_text = obj[\"Body\"].read()\n",
    "training_text = raw_training_text.decode('utf-8')\n",
    "print(training_text[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211296707\n",
      "205598350\n",
      "ID||Text\n",
      "0||Cyclin-dependent kinases (CDKs) regulate a variety of fundamental cellular processes. CDK10 stands out as one of the last orphan CDKs for which no activating cyclin has been identified and no kinase activity revealed. Previous work has shown that CDK10 silencing increases ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2)-driven activation of the MAPK pathway, which confers tamoxifen resistance to breast cancer cells. The precise mechanisms by which CDK10 modulates ETS2 activity, and more generally the functions of CDK10, remain elusive. Here we demonstrate that CDK10 is a cyclin-dependent kinase by identifying cyclin M as an activating cyclin. Cyclin M, an orphan cyclin, is the product of FAM58A, whose mutations cause STAR syndrome, a human developmental anomaly whose features include toe syndactyly, telecanthus, and anogenital and renal malformations. We show that STAR syndrome-associated cyclin M mutants are unable to interact with CDK10. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and in conferring tamoxifen resistance to breast cancer cells. CDK10/cyclin M phosphorylates ETS2 in vitro, and in cells it positively controls ETS2 degradation by the proteasome. ETS2 protein levels are increased in cells derived from a STAR patient, and this increase is attributable to decreased cyclin M levels. Altogether, our results reveal an additional regulatory mechanism for ETS2, which plays key roles in cancer and development. They also shed light on the molecular mechanisms underlying STAR syndrome.Cyclin-dependent kinases (CDKs) play a pivotal role in the control of a number of fundamental cellular processes. The human genome contains 21 genes encoding proteins that can be considered as members of the CDK family owing to their sequence similarity with bona fide CDKs, those known to be activated by cyclins. Although discovered almost 20 y ago, CDK10 remains one of the two CDKs without an identified cyclin partner. This knowledge gap has largely impeded the exploration of its biological functions. CDK10 can act as a positive cell cycle regulator in some cells or as a tumor suppressor in others. CDK10 interacts with the ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2) transcription factor and inhibits its transcriptional activity through an unknown mechanism. CDK10 knockdown derepresses ETS2, which increases the expression of the c-Raf protein kinase, activates the MAPK pathway, and induces resistance of MCF7 cells to tamoxifen.Here, we deorphanize CDK10 by identifying cyclin M, the product of FAM58A, as a binding partner. Mutations in this gene that predict absence or truncation of cyclin M are associated with STAR syndrome, whose features include toe syndactyly, telecanthus, and anogenital and renal malformations in heterozygous females. However, both the functions of cyclin M and the pathogenesis of STAR syndrome remain unknown. We show that a recombinant CDK10/cyclin M heterodimer is an active protein kinase that phosphorylates ETS2 in vitro. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and phospho-ERK expression levels and in inducing tamoxifen resistance in estrogen receptor (ER)+ breast cancer cells. We show that CDK10/cyclin M positively controls ETS2 degradation by the proteasome, through the phosphorylation of two neighboring serines. Finally, we detect an increased ETS2 expression level in cells derived from a STAR patient, and we demonstrate that it is attributable to the decreased cyclin M expression level observed in these cells.Previous SectionNext SectionResultsA yeast two-hybrid (Y2H) screen unveiled an interaction signal between CDK10 and a mouse protein whose C-terminal half presents a strong sequence homology with the human FAM58A gene product [whose proposed name is cyclin M]. We thus performed Y2H mating assays to determine whether human CDK10 interacts with human cyclin M. The longest CDK10 isoform (P1) expressed as a bait protein produced a\n"
     ]
    }
   ],
   "source": [
    "# Eliminate references and abbreviations within parentheses\n",
    "print(len(training_text))\n",
    "training_text = re.sub(' \\(Fig \\d+.+?\\)', '', training_text)\n",
    "training_text = re.sub(' \\(Fig\\. \\d+.+?\\)', '', training_text)\n",
    "training_text = re.sub(' \\(\\d.*?\\)', '', training_text)\n",
    "training_text = re.sub(' \\([A-Z]\\)', '', training_text)\n",
    "print(len(training_text))\n",
    "print(training_text[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3321\n"
     ]
    }
   ],
   "source": [
    "# Split text file into list of documents\n",
    "training_list = training_text.split('||')\n",
    "training_list = training_list[2:]\n",
    "print(len(training_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID    Gene             Variation  Class  \\\n",
      "0   0  FAM58A  Truncating Mutations      1   \n",
      "1   1     CBL                 W802*      2   \n",
      "2   2     CBL                 Q249E      2   \n",
      "3   3     CBL                 N454D      3   \n",
      "4   4     CBL                 L399V      4   \n",
      "\n",
      "                                                text  \n",
      "0  Cyclin-dependent kinases (CDKs) regulate a var...  \n",
      "1   Abstract Background  Non-small cell lung canc...  \n",
      "2   Abstract Background  Non-small cell lung canc...  \n",
      "3  Recent evidence has demonstrated that acquired...  \n",
      "4  Oncogenic mutations in the monomeric Casitas B...  \n",
      "(3321, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load training text list into dataframe\n",
    "texts_df = pd.DataFrame(training_list, columns = ['text'])\n",
    "\n",
    "# Merge text dataframe with labels dataframe\n",
    "train = pd.concat([training_variants, texts_df], axis=1)\n",
    "print(train.head())\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Temporary cell to reduce data size\n",
    "#train = train[:400]\n",
    "#print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is loaded into a dataframe, let's do some preliminary data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7    953\n",
      "4    686\n",
      "1    568\n",
      "2    452\n",
      "6    275\n",
      "5    242\n",
      "3     89\n",
      "9     37\n",
      "8     19\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3321 total datapoints to work with, and it looks like we are dealing with significant class imbalance. Class 7 has 953 datapoints, while Class 8 has only 19. We will have to address this class imbalance with our experiment design. Additionally, the labels have been anonymized, which means we cannot draw any insight about what these classes might signify. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Design\n",
    "\n",
    "The prevalence of class imbalance has serious implications for our analysis. First and foremost, we must establish our scoring metric. The purpose here is to use the relevant clinical texts to predict the mutation category for each gene/mutation pair. While we want the predictions to be as accurate as possible, simple classification accuracy is not a representative way to judge models that are built on class imbalance, as they may achieve high accuracy by simply predicting the most common class every time. <br>\n",
    "\n",
    "Given that we are working with a multi-label classifier, the most appropriate scoring metric is log loss. Log loss quantifies the accuracy of a classifier by penalising false classifications, and heavily penalises classifiers that are confident about an incorrect classification. For this reason, it is more suitable than traditional accuracy for datasets with class imbalance. <br>\n",
    "\n",
    "When feeding our data into the predictive models, each resulting prediction is associated with a probability, and each probability is multiplied by one another to get the overall probability that all of those outcomes occurred together. As each event gets multiplied in, the final number gets smaller and smaller. So, we take the log to put the number in a more accessible range. The number is then multiplied by -1 to maintain the convention that a lower loss score is better. We will be choosing models with log loss scores closer to 0. We will also look at the precision and recall via the F1 score. Though these are not optimized for multi-label classification, they will be interesting to consider. <br>\n",
    "\n",
    "Source: https://datawookie.netlify.com/blog/2015/12/making-sense-of-logarithmic-loss/, https://www.kaggle.com/dansbecker/what-is-log-loss\n",
    "\n",
    "We will also try oversampling the lesser-represented categories and apply our machine learning models on the oversampled datasets, judging by their log loss scores. Oversampling can be achieved by generating duplicate datapoints or by generating new synthetic datapoints via SMOTE, the Synthetic Minority Oversampling Technique. We will use SMOTE, a feature of the imbalanced learn package. <br>\n",
    "\n",
    "I will use various methods of feature generation including classic NLP techniques such as bag-of-words, tf-idf, and n-grams. These methods of feature generation will be applied to both the original and oversampled datasets. They will then be subjected to various machine learning models. Decision trees are known to perform well on unbalanced datasets, so this model may prevail on the original data. However, Naive Bayes is known to perform well on natural langauge, so once the dataset is oversampled it is possible that Naive Bayes will perform the best. All machine learning models will be run with a variety of hyperparameters on a variety of datasets, and the permutation with the best log-loss score will be chosen to move forward.\n",
    "\n",
    "It is important to note that by convention, grid search always tries to maximize its score so loss functions like log loss will have to be negated such that the lowest log loss score will be the highest negated score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is relatively clean already, and contains no NaN values. It needs to be tokenized so it can be processed into readable pieces of data. We will use spaCy to tokenize the data and create a new column with a list of the tokens for each row. Furthermore, we will convert all tokens that are not stop words or punctuation to lemmas to reduce the noise from unnecessary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'off', 's', 'ain', \"needn't\", 'couldn', 'all', 'her', 'has', 'of', 'ma', 'before', 'during', 'over', 'which', 're', 'yours', \"don't\", 'our', 'them', 'y', 'each', 'that', 'wasn', \"mustn't\", 'very', 'not', 'from', 'up', 'when', 'd', 'until', 'now', 'below', 'she', 'doing', 'down', 'am', 'hadn', 'wouldn', \"won't\", 'herself', 'the', 'me', \"couldn't\", 'between', 'on', \"you're\", 'myself', 'haven', 'won', 'through', 'only', \"you've\", 'then', 'no', 'having', 'don', 'what', 'being', 'yourselves', 'shouldn', 'him', 'than', 'where', 'll', 'at', 'own', 'if', \"didn't\", 'were', 'while', 'most', 'who', \"hadn't\", 'an', \"wouldn't\", 'for', 'under', 'its', 'doesn', \"isn't\", 'is', 'and', 'mightn', 'any', 'against', \"hasn't\", 'because', 'to', 've', 'it', 'mustn', 'shan', 'so', 'they', 'isn', 'their', 'whom', \"doesn't\", 'further', \"shouldn't\", 'have', 'by', 'needn', \"should've\", 'does', 'or', 'after', \"aren't\", 'did', 'there', 'you', 'had', \"you'll\", 'too', 'as', 'will', 'those', 'be', 'about', 'nor', 'few', \"weren't\", 'themselves', 'such', 'some', \"shan't\", \"wasn't\", 'once', 'weren', 'ours', 'been', \"that'll\", 't', \"you'd\", \"haven't\", 'o', 'do', 'are', 'but', 'above', 'ourselves', 'these', 'was', 'more', 'aren', 'itself', 'theirs', 'other', 'same', 'yourself', 'this', 'with', 'i', 'my', 'just', 'a', 'his', 'here', 'both', \"mightn't\", 'we', 'your', 'should', 'again', 'can', 'into', 'in', 'm', 'hasn', \"it's\", 'why', \"she's\", 'how', 'out', 'didn', 'himself', 'he', 'hers'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning lemmatization\n",
      "   ID    Gene             Variation  Class  \\\n",
      "0   0  FAM58A  Truncating Mutations      1   \n",
      "1   1     CBL                 W802*      2   \n",
      "2   2     CBL                 Q249E      2   \n",
      "3   3     CBL                 N454D      3   \n",
      "4   4     CBL                 L399V      4   \n",
      "\n",
      "                                                text  \\\n",
      "0  Cyclin-dependent kinases (CDKs) regulate a var...   \n",
      "1   Abstract Background  Non-small cell lung canc...   \n",
      "2   Abstract Background  Non-small cell lung canc...   \n",
      "3  Recent evidence has demonstrated that acquired...   \n",
      "4  Oncogenic mutations in the monomeric Casitas B...   \n",
      "\n",
      "                                        spacy_tokens  \\\n",
      "0  (Cyclin, -, dependent, kinases, (, CDKs, ), re...   \n",
      "1  ( , Abstract, Background,  , Non, -, small, ce...   \n",
      "2  ( , Abstract, Background,  , Non, -, small, ce...   \n",
      "3  (Recent, evidence, has, demonstrated, that, ac...   \n",
      "4  (Oncogenic, mutations, in, the, monomeric, Cas...   \n",
      "\n",
      "                                              lemmas  \n",
      "0  [cyclin, dependent, kinase, cdks, regulate, va...  \n",
      "1  [abstract, background, non, small, cell, lung,...  \n",
      "2  [abstract, background, non, small, cell, lung,...  \n",
      "3  [recent, evidence, demonstrate, acquire, unipa...  \n",
      "4  [oncogenic, mutation, monomeric, casitas, b, l...  \n"
     ]
    }
   ],
   "source": [
    "print('beginning lemmatization')\n",
    "train['spacy_tokens'] = train['text'].apply(lambda x: nlp(x))\n",
    "\n",
    "def lemmatize(x):\n",
    "    intermediate_lemmas = [token.lemma_.lower() for token in x\n",
    "            if not token.is_punct]\n",
    "    return [lemma for lemma in intermediate_lemmas\n",
    "           if lemma not in stop_words\n",
    "           and lemma != \"-PRON-\"\n",
    "           and lemma != \" \"\n",
    "           ]\n",
    "\n",
    "train['lemmas'] = train['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our lemmatized datapoints, we must convert them into a list of strings to feed to our feature generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_lemma_documents = [\n",
    "    ' '.join([str(word) for word in text])\n",
    "    for text in train['lemmas'].values.tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update training dataframe with lemmatized documents\n",
    "train['lemmatized'] = joined_lemma_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reserve 20% of training dataset for external validation\n",
    "train, test = train_test_split(train,\n",
    "                                test_size = 0.2,\n",
    "                                random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce unnecessary input to the vectorizer, we want to cut each document to its most important features. But how will we know how many features to keep? First, we will create a dictionary of all the words in the corpora and their frequencies using Counter. Then we will graph the words by frequency to see how many words make up the majority of our corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of 500 most common words per training set to reduce features in testing set\n",
    "# Create a dictionary of words and frequencies\n",
    "counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Iterate through each training document, split into words and add words/frequencies to Counter\n",
    "for document in train['lemmatized']:\n",
    "    words = document.split(' ')\n",
    "    counter.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mutation', 528768], ['cell', 482300]]\n"
     ]
    }
   ],
   "source": [
    "# Convert term/frequency dictionary to list sorted by frequency\n",
    "word_frequencies = sorted(\n",
    "                        [[key, value] for key, value in counter.items()],\n",
    "                        key = lambda x: x[1],\n",
    "                        reverse = True\n",
    "                        )\n",
    "\n",
    "print(word_frequencies[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of terms: 205136\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsRd/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f13e0a3bc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot term frequency, trim list to 2000 most common words for easier visualization\n",
    "print('Total number of terms:', len(word_frequencies))\n",
    "trim_length = 2000\n",
    "trim_frequencies = word_frequencies[:trim_length]\n",
    "frequency_X = list(range(len(trim_frequencies)))\n",
    "frequency_Y = [frequency_pair[1] for frequency_pair in trim_frequencies]\n",
    "plt.plot(frequency_X, frequency_Y)\n",
    "plt.xlabel('Rank of Frequency')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the term frequencies drop off after the first 250 words. After 250, the term frequencies become very sparse and will likely add no value to our model. Still, we will keep the 500 most common words for input to our feature generator to ensure that we are not losing any potentially valuable features. We will utilize the max_feature hyperparameter in the tf-idf and count vectorizer, which will allow us to reduce the dataset to these 500 most common words in one simple step.\n",
    "\n",
    "However, when we run our external validation set (reserved 20%) we cannot verify it unless our validation set has the same features as our training set. For this reason, we will extract the 500 most common words from our training set and reduce our validation documents to these 500 words. While this significantly changes our testing data, it is the only way to evaluate the legitimacy of our chosen model. This is a common problem in natural language processing, and typically handled by either ignoring all unseen words in the testing document or lumping them into a single 'unknown' type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of n most common words per training document\n",
    "common_words = [frequency_pair[0] for frequency_pair in word_frequencies[:500]]\n",
    "\n",
    "reduced_documents = []\n",
    "# Reducing test documents to 500 most common words per training set\n",
    "for document in test['lemmatized'].values.tolist():\n",
    "    words = document.split(' ')\n",
    "    reduced_document = [word for word in words\n",
    "                       if word in common_words]\n",
    "    reduced_document = ' '.join(reduced_document)\n",
    "    reduced_documents.append(reduced_document)\n",
    "    \n",
    "test['lemmas'] = reduced_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exporting training/testing dataframes to csv'\n",
    "train.to_csv('lemmatized_train.csv')\n",
    "test.to_csv('lemmatized_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search: Feature Generation, Dimensionality Reduction, Oversampling and Machine Learning\n",
    "\n",
    "In this cell, we will create a pipeline that iterates through several different parameters and models. For feature generation, we will attempt both tf-idf vectorization and bag-of-words vectorization, which will convert our list of strings into sparse dataframes. Bag-of-words is a simplistic representation of text that considers only term frequency and assumes that position doesn't matter. Tf-idf is a more nuanced representation of text that considers the term frequency as well the inverse document frequency, which gives more information about how significant each word is. For example, a common word such as \"the\" may score high in term frequency, but having high term frequency in every document would diminish its meaning and thus its tf-idf score. Both bag-of-words and tf-idf have several parameters that will be optimized through the gridsearch. For brevity's sake, we are only tuning the max_df and min_df, which represent the maximum and minimum number of times a term should appear in any given document.\n",
    "\n",
    "Beyond using singular words or lemmas as features for classification, we can also use groupings of words that appear together, as they may convey more meaning than each word isolated by itself. We will use the native ngram_range hyperparameter to broaden our dataset to include common bigrams as well. Bigrams (2 consecutive words) are preferable as groupings larger than 2 words in a dataset this small would likely create unnecessary noise without adding any insight.\n",
    "\n",
    "We will likely need a way to reduce dimensionality, as dimensionality reduction reduces the storage and computation requirements of our model.  We will implement some permutations of the pipeline with Truncated Singular Value Decomposition which works well with sparse data. This transformation is termed Latent Semantic Analysis, a technique that is known to combat polysemy and synonymy. For the permutations that use Truncated SVD, I chose to reduce to 500 components based on a scree plot that was generated outside of this notebook. In the scree plot, 500 components appeared to capture roughly 85% of the variance of the dataset, a well-known standard for dimensionality reduction.\n",
    "\n",
    "Source: http://scikit-learn.org/stable/modules/decomposition.html#truncated-singular-value-decomposition-and-latent-semantic-analysis\n",
    "\n",
    "As mentioned before, we will also implement oversampling via SMOTE to combat the class imbalance. SMOTE (Synthetic Minority Oversampling Technique) uses the K Neighbors algorithm to synthesize new datapoints that resemble the existing classes. The default number of neighbors to use is 5. In an effort to reduce computational complexity, we will hold off on tuning other parameters.\n",
    "\n",
    "Finally, we will apply several different machine learning models and evaluate their performance using cross-validation with log-loss scoring. First, we will try a multinomial Naive Bayes model, which naively assumes that all features are independent from one another. The multinomial model is commonly used for NLP problems and works under the bag-of-words assumption that position doesn't matter. The sklearn MultinomialNB documentation states that \"the multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\"\n",
    "\n",
    "Source: https://web.stanford.edu/class/cs124/lec/naivebayes.pdf, http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "We will also try random forests, as ensemble models tend to be robust against overfitting. Random forest language models have been shown to generalize well to unseen data, which is important in predictive modeling. The random forest has several parameters that will be tuned with the high dimensionality of the dataset in mind.\n",
    "\n",
    "Source: http://www.aclweb.org/anthology/W04-3242\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in training dataframe\n",
    "train = pd.read_csv('~/tumor-mutation-classification/lemmatized_train.csv')\n",
    "X = train['lemmatized']\n",
    "Y = train['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "#    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 5)),\n",
    "    ('machine', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Feature generation parameters\n",
    "maxdf = [0.25, 0.5, 0.75]\n",
    "mindf = [0.01, 0.05, 0.1]\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "# Machine learning model parameters \n",
    "C_values = [1e-3, 1e-1, 1, 100]\n",
    "\n",
    "param_grid = [\n",
    "    # Tfidf permutations first\n",
    "    # With and without oversampling, ngrams\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    # Bag of words permutations next\n",
    "    # With and without oversampling, ngrams\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 500)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [CountVectorizer(max_features = 500)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'machine': [LogisticRegression(random_state=1)],\n",
    "        'machine__C': C_values\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run grid search with cross validation\n",
    "print('running grid search with logistic regression...')\n",
    "grid = GridSearchCVProgressBar(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')\n",
    "grid.fit(X, Y)\n",
    "print(f'best score: \\n {grid.best_score_}')\n",
    "print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have run the grid search, we will export our findings to a csv so that we can access them without having to run the expensive grid search every time. We will create a function that can be re-used throughout the notebook to export all the grid search findings from each machine learning model type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort and export all results for grid search to csv\n",
    "def export_gridsearch_to_csv(gs_clf, export_file):\n",
    "    with open(export_file, 'w') as outfile:\n",
    "        csvwriter = csv.writer(outfile, delimiter=',')\n",
    "\n",
    "        # Create the header using the parameter names \n",
    "        header = [\"mean\",\"std\", \"params\"]\n",
    "\n",
    "        csvwriter.writerow(header)\n",
    "\n",
    "        sorted_by_score = sorted(gs_clf.grid_scores_, key = itemgetter(1), reverse=True)\n",
    "\n",
    "        for config in sorted_by_score:\n",
    "            # Get mean and standard deviation\n",
    "            mean = np.abs(config[1])\n",
    "            std = np.std(config[2])\n",
    "            row = [mean,std, str(config[0])]\n",
    "\n",
    "            csvwriter.writerow(row)\n",
    "\n",
    "filename = datetime.datetime.utcnow().strftime('logistic_gridresults_%Y%m%d_%H:%M.csv')       \n",
    "export_gridsearch_to_csv(grid, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "#    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 5)),\n",
    "    ('machine', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Feature generation parameters\n",
    "maxdf = [0.25, 0.5, 0.75]\n",
    "mindf = [0.01, 0.05, 0.1]\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "# Machine learning model parameters \n",
    "rf_max_depth = [30, 60, 100]\n",
    "rf_min_samples_split = [430]\t# 430 represents 5% of the dataset after oversampling\n",
    "rf_n_estimators = [10]\n",
    "\n",
    "param_grid = [\n",
    "    # Tfidf permutations with and without n-grams\n",
    "    # Removed permutations with CountVectorizer after logistic regression demonstrated poor log loss scores\n",
    "    # Removed permutations without oversampling after logistic regression demonstrated poor log loss scores\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        #'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [RandomForestClassifier(random_state = 1)],\n",
    "        'machine__max_depth': rf_max_depth,\n",
    "        'machine__min_samples_split': rf_min_samples_split,\n",
    "        'machine__n_estimators': rf_n_estimators\n",
    "    },\n",
    "        {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500, ngram_range=(2,2))],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        #'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [RandomForestClassifier(random_state = 1)],\n",
    "        'machine__max_depth': rf_max_depth,\n",
    "        'machine__min_samples_split': rf_min_samples_split,\n",
    "        'machine__n_estimators': rf_n_estimators\n",
    "    }\n",
    "]\n",
    "# Run grid search with cross validation\n",
    "print('running grid search with random forest...')\n",
    "grid = GridSearchCVProgressBar(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')\n",
    "grid.fit(X, Y)\n",
    "print(f'best score: \\n {grid.best_score_}')\n",
    "print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = datetime.datetime.utcnow().strftime('rf_gridresults_%Y%m%d_%H:%M.csv')       \n",
    "export_gridsearch_to_csv(grid, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "#    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 5)),\n",
    "    ('machine', XGBClassifier())\n",
    "])\n",
    "\n",
    "# Feature generation parameters\n",
    "# max_df and min_df values reduced to best-performing values from logistic/random forest grid search\n",
    "maxdf = [0.75]\n",
    "mindf = [0.05]\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "# Machine learning model parameters \n",
    "xg_booster = ['gbtree','gblinear']\n",
    "xg_learning_rate = [0.01, 0.1, 0.3]\n",
    "xg_max_depth = [30, 60, 100]\n",
    "xg_subsample = [0.5, 1]\n",
    "\n",
    "param_grid = [\n",
    "# Tfidf permutations with and without n-grams\n",
    "# Removed permutations with CountVectorizer after logistic regression demonstrated poor log loss scores\n",
    "# Removed permutations without oversampling after logistic regression demonstrated poor log loss scores\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        #'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [XGBClassifier(random_state = 1)],\n",
    "        'machine__booster': xg_booster,\n",
    "        'machine__learning_rate': xg_learning_rate,\n",
    "        'machine__max_depth': xg_max_depth,\n",
    "        'machine__subsample': xg_subsample\n",
    "    },\n",
    "        {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500, ngram_range=(2,2))],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        #'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine__booster': xg_booster,\n",
    "        'machine__learning_rate': xg_learning_rate,\n",
    "        'machine__max_depth': xg_max_depth,\n",
    "        'machine__subsample': xg_subsample\n",
    "    }\n",
    "]\n",
    "# Run grid search with cross validation\n",
    "print('running grid search with XGBoost...')\n",
    "grid = GridSearchCVProgressBar(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')\n",
    "grid.fit(X, Y)\n",
    "print(f'best score: \\n {grid.best_score_}')\n",
    "print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = datetime.datetime.utcnow().strftime('xg_gridresults_%Y%m%d_%H:%M.csv')       \n",
    "export_gridsearch_to_csv(grid, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Pipeline (Multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "#    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 5)),\n",
    "    ('machine', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Feature generation parameters\n",
    "# max_df and min_df values reduced to best-performing values from logistic/random forest grid search\n",
    "maxdf = [0.75]\n",
    "mindf = [0.05]\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [500]\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "# Default machine learning model parameters \n",
    "\n",
    "param_grid = [\n",
    "# Tfidf permutations with and without n-grams\n",
    "# Removed permutations with CountVectorizer after logistic regression demonstrated poor log loss scores\n",
    "# Removed permutations without oversampling after logistic regression demonstrated poor log loss scores\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        #'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [MultinomialNB()],\n",
    "    },\n",
    "        {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 500, ngram_range=(2,2))],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        #'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [MultinomialNB()],\n",
    "    }\n",
    "]\n",
    "# Run grid search with cross validation\n",
    "print('running grid search with Naive Bayes...')\n",
    "grid = GridSearchCVProgressBar(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')\n",
    "grid.fit(X, Y)\n",
    "print(f'best score: \\n {grid.best_score_}')\n",
    "print(f'best params:\\n {grid.best_params_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = datetime.datetime.utcnow().strftime('nb_gridresults_%Y%m%d_%H:%M.csv')       \n",
    "export_gridsearch_to_csv(grid, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all of our preliminary pipelines have been run, we can take a look at the resulting log loss scores and judge which machine learning model was the most effective on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating grid search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_results = pd.read_csv('scripts/logistic_gridresults_20180629_00:02.csv')\n",
    "nb_results = pd.read_csv('scripts/nb_gridresults_20180703_18:04.csv')\n",
    "rf_results = pd.read_csv('scripts/rf_gridresults_20180701_22:50.csv')\n",
    "xg_results = pd.read_csv('scripts/xg_gridresults_20180630_17:46.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      classifier      mean       std  \\\n",
      "2  random forest  1.832495  0.011121   \n",
      "1    naive bayes  1.839022  0.044531   \n",
      "3        xgboost  1.863806  0.055248   \n",
      "0       logistic  1.888993  0.061842   \n",
      "\n",
      "                                              params  \n",
      "2  {'feat_gen': TfidfVectorizer(analyzer='word', ...  \n",
      "1  {'feat_gen': TfidfVectorizer(analyzer='word', ...  \n",
      "3  {'feat_gen': TfidfVectorizer(analyzer='word', ...  \n",
      "0  {'feat_gen': TfidfVectorizer(analyzer='word', ...  \n"
     ]
    }
   ],
   "source": [
    "best_logistic = pd.Series(['logistic', logistic_results['mean'][0], logistic_results['std'][0], logistic_results['params'][0]])\n",
    "best_nb = pd.Series(['naive bayes', nb_results['mean'][0], nb_results['std'][0], nb_results['params'][0]])\n",
    "best_rf = pd.Series(['random forest', rf_results['mean'][0], rf_results['std'][0], rf_results['params'][0]])\n",
    "best_xg = pd.Series(['xgboost', xg_results['mean'][0], xg_results['std'][0], xg_results['params'][0]])\n",
    "best_gridresults = pd.DataFrame().append([best_logistic, best_nb, best_rf, best_xg], ignore_index=True)\n",
    "best_gridresults.columns = ['classifier','mean','std','params']\n",
    "best_gridresults = best_gridresults.sort_values(by=['mean'], ascending=True)\n",
    "print(best_gridresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n"
     ]
    }
   ],
   "source": [
    "print(best_gridresults['params'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that with the lowest log loss score of 1.83, the random forest classifier performed the best on our training dataset. Decision trees/random forests are known to perform well on imbalanced datasets, which we are currently working with. This model used bigrams as features with max_depth = 30 and min_samples_split = 430. One option now is to use the hyperparameters determined from this grid search to test new methods of feature generation such as word2vec, a common unsupervised neural network approach for NLP. The resulting log loss scores from other methods of feature generation will be compared to this baseline score of 1.83. The hyperparameters are as follows: <br>\n",
    "- Feature Generator: TfidfVectorizer\n",
    "- max_df = 0.75\n",
    "- max_features = 500\n",
    "- min_df = 0.05\n",
    "- ngram_range = (2,2)\n",
    "- oversample kind = regular\n",
    "- Classifier: RandomForestClassifier\n",
    "- max_depth = 30\n",
    "- min_samples_split = 430\n",
    "- n_estimators = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.05,\n",
      "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=30, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'borderline1'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.5, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.1, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 60, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.25, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.01, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 30, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=500, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.75, 'feat_gen__min_df': 0.05, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=100, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 100, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular'}\n"
     ]
    }
   ],
   "source": [
    "for element in rf_results['params']:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "Let's see how this set of hyperparameters performs on our external validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in training/testing dataframes\n"
     ]
    }
   ],
   "source": [
    "# Load in training dataframe\n",
    "print(\"loading in training/testing dataframes\")\n",
    "train = pd.read_csv('~/tumor-mutation-classification/scripts/lemmatized_train.csv')\n",
    "X_train = train['lemmatized']\n",
    "Y_train = train['Class']\n",
    "test = pd.read_csv('~/tumor-mutation-classification/scripts/lemmatized_test.csv')\n",
    "X_test = test['lemmas']\n",
    "Y_test = test['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in training/testing dataframes\n",
      "Log Loss:  1.71745463541\n",
      "Accuracy:  0.371428571429\n",
      "\n",
      "Confusion Matrix:\n",
      " [[30  5  2 14 16 18  3  7 12]\n",
      " [ 2 48  3  4  1  7 20 11  2]\n",
      " [ 3  3 11  2  1  1  1  0  0]\n",
      " [21  8  8 45 13 14  5  7 20]\n",
      " [ 5  1  4  4 20  7  3  0  1]\n",
      " [ 3  2  1  2  3 35  2  0  0]\n",
      " [14 55 28  7  8  5 50 13 12]\n",
      " [ 0  0  0  0  0  0  2  1  1]\n",
      " [ 0  0  0  0  0  0  1  0  7]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.38      0.28      0.32       107\n",
      "          2       0.39      0.49      0.44        98\n",
      "          3       0.19      0.50      0.28        22\n",
      "          4       0.58      0.32      0.41       141\n",
      "          5       0.32      0.44      0.37        45\n",
      "          6       0.40      0.73      0.52        48\n",
      "          7       0.57      0.26      0.36       192\n",
      "          8       0.03      0.25      0.05         4\n",
      "          9       0.13      0.88      0.22         8\n",
      "\n",
      "avg / total       0.47      0.37      0.38       665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up pipeline\n",
    "# Parameters reduced to best-performing parameters from prior grid search\n",
    "def random_forest_validation(X_train, Y_train, X_test, Y_test):\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('feat_gen', TfidfVectorizer(max_df = 0.75, min_df = 0.05, max_features = 500)),\n",
    "        ('oversample', SMOTE(k_neighbors = 5, kind = 'regular', random_state = 1)),\n",
    "        ('machine', RandomForestClassifier(max_depth = 30, min_samples_split = 430, n_estimators = 10, random_state = 1))\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X_train, Y_train)\n",
    "    Y_pred = pipe.predict(X_test)\n",
    "    Y_pred_prob = pipe.predict_proba(X_test)\n",
    "\n",
    "    print('Log Loss: ', log_loss(Y_test, Y_pred_prob))\n",
    "    print('Accuracy: ', accuracy_score(Y_test, Y_pred))\n",
    "    print('\\nConfusion Matrix:\\n', confusion_matrix(Y_test, Y_pred))\n",
    "    print('\\nClassification Report:\\n', classification_report(Y_test, Y_pred))\n",
    "    \n",
    "random_forest_validation(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running validation on our external dataset, we can see that we have a baseline of **1.72 log loss score** and **37% accuracy** with **0.38 f1-score** (combining precision and recall). Let us now alter a few steps of the process and feed these altered dataframes into the training model to see if we can get it to perform any better. First, we will try increasing the number of features in the vectorizer. Currently, the model is trained with the 500 most common words, but the corpus includes 205,136 unique words. We will experiment with increasing the number of words considered for the feature generator. After we find a set a changes that improves the training log loss score, we will validate it again using the external validation dataset. \n",
    "\n",
    "# Fine tuning the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrying the random forest with 2000 most common words instead of 500\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer(max_df = 0.75, min_df = 0.05, max_features = 2000)),\n",
    "    ('oversample', SMOTE(k_neighbors = 5, kind = 'regular', random_state = 1)),\n",
    "    ('machine', RandomForestClassifier(max_depth = 30, min_samples_split = 430, n_estimators = 10, random_state = 1))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = pipe.predict(X_train)\n",
    "Y_pred_prob = pipe.predict_proba(X_train)\n",
    "\n",
    "print('Log Loss: ', log_loss(Y_train, Y_pred_prob))\n",
    "print('Accuracy: ', accuracy_score(Y_train, Y_pred))\n",
    "print('\\nConfusion Matrix:\\n', confusion_matrix(Y_test, Y_pred))\n",
    "print('\\nClassification Report:\\n', classification_report(Y_test, Y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss:  1.57133355091\n"
     ]
    }
   ],
   "source": [
    "# Retrying the random forest with 2000 most common words instead of 500\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer(max_df = 0.75, min_df = 0.05, max_features = 2000)),\n",
    "    ('oversample', SMOTE(k_neighbors = 5, kind = 'regular', random_state = 1)),\n",
    "    ('machine', RandomForestClassifier(max_depth = 30, min_samples_split = 430, n_estimators = 10, random_state = 1))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "Y_pred = pipe.predict(X_train)\n",
    "Y_pred_prob = pipe.predict_proba(X_train)\n",
    "\n",
    "print('Log Loss: ', log_loss(Y_train, Y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the words in our vectorizer lowered the training log loss score from 1.83 to **1.57**. This is a change we will keep -- from now on, the max_features specified in the training model will be 2000. This also means that we will have to expand our testing documents to include include the 2000 common words instead of the original 500. \n",
    "\n",
    "Let's try altering the pipeline to include a step for dimensionality reduction, which was neglected in the initial run of the grid searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss:  1.67777067324\n"
     ]
    }
   ],
   "source": [
    "pipe_dimred = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer(max_df = 0.75, min_df = 0.05, max_features = 2000)),\n",
    "    ('reduce_dim', decomposition.TruncatedSVD(n_components=250, random_state = 1)),\n",
    "    ('oversample', SMOTE(k_neighbors = 5, kind = 'regular', random_state = 1)),\n",
    "    ('machine', RandomForestClassifier(max_depth = 30, min_samples_split = 430, n_estimators = 10, random_state = 1))\n",
    "])\n",
    "\n",
    "pipe_dimred.fit(X_train, Y_train)\n",
    "Y_pred = pipe_dimred.predict(X_train)\n",
    "Y_pred_prob = pipe_dimred.predict_proba(X_train)\n",
    "\n",
    "print('Log Loss: ', log_loss(Y_train, Y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a step in the pipeline for dimensionality reduction via TruncatedSVD did not demonstrate an improvement from the new baseline log loss score of 1.57, though we did see an improvement from the initial training score of 1.83 that resulted from keeping 500 common words. In this instance, we reduced the number of features by one half using 2000 words -- perhaps this eliminated too much information? Let's continue fine tuning this in future efforts, since dimensionality reduction can be very beneficial for reducing the time and storage space required of this large dataset.\n",
    "\n",
    "** Conclusions: <br>\n",
    "Higher max features in vectorizer/vocabulary for testing documents. <br>\n",
    "Dimensionality reduction using Truncated SVD may or may not be beneficial.\n",
    "**\n",
    "\n",
    "Now that we know that increasing the number of words considered in our vectorizer benefits our model, we can experiment with the number of words we choose. We will wrap this section of experimentation in a function for efficiency. The dimensionality reduction step will be omitted for now and added again later when we have determined the best number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_max_features(X_train, Y_train, max_ft):\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('feat_gen', TfidfVectorizer(max_df = 0.75, min_df = 0.05, max_features = max_ft)),\n",
    "        ('oversample', SMOTE(k_neighbors = 5, kind = 'regular', random_state = 1)),\n",
    "        ('machine', RandomForestClassifier(max_depth = 30, min_samples_split = 430, n_estimators = 10, random_state = 1))\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X_train, Y_train)\n",
    "    Y_pred = pipe.predict(X_train)\n",
    "    Y_pred_prob = pipe.predict_proba(X_train)\n",
    "\n",
    "    print('Log Loss: ', log_loss(Y_train, Y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss:  1.5337523908\n"
     ]
    }
   ],
   "source": [
    "change_max_features(X_train, Y_train, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss:  1.5337523908\n"
     ]
    }
   ],
   "source": [
    "change_max_features(X_train, Y_train, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss:  1.5337523908\n"
     ]
    }
   ],
   "source": [
    "change_max_features(X_train, Y_train, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss:  1.5337523908\n"
     ]
    }
   ],
   "source": [
    "change_max_features(X_train, Y_train, 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss:  1.5337523908\n"
     ]
    }
   ],
   "source": [
    "change_max_features(X_train, Y_train, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss:  1.5337523908\n"
     ]
    }
   ],
   "source": [
    "change_max_features(X_train, Y_train, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss:  1.57133355091\n"
     ]
    }
   ],
   "source": [
    "change_max_features(X_train, Y_train, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After iterating through several values for how many word features to include in our vectorized dataframe, it appears that any number between 5000 words and all 2,000,000+ words yields the best log loss score at 1.53. We will move forward using 5000 words for simplicity's sake. At this time, we will reduce the vocabulary of the testing documents to 5000 words for our future validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identifying 5000 most common words per training documents to reduce the vocabulary of the testing documents\n",
    "counter = Counter()\n",
    "\n",
    "for document in train['lemmatized']:\n",
    "    words = document.split(' ')\n",
    "    counter.update(words)\n",
    "\n",
    "word_frequencies = sorted(\n",
    "                        [[key, value] for key, value in counter.items()],\n",
    "                        key = lambda x: x[1],\n",
    "                        reverse = True\n",
    "                        )\n",
    "\n",
    "common_words_5000 = [frequency_pair[0] for frequency_pair in word_frequencies[:5000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating new test dataframe with 5000 most common words per training set\n",
    "test_5k = test.copy()\n",
    "reduced_documents_5000 = []\n",
    "\n",
    "for document in test['lemmatized'].values.tolist():\n",
    "    words = document.split(' ')\n",
    "    reduced_document = [word for word in words\n",
    "                       if word in common_words_5000]\n",
    "    reduced_document = ' '.join(reduced_document)\n",
    "    reduced_documents_5000.append(reduced_document)\n",
    "    \n",
    "test_5k['lemmatized'] = reduced_documents_5000\n",
    "\n",
    "# Exporting updated testing dataframe to csv for future reference\n",
    "test_5k.to_csv('lemmatized_test_5000.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 5000 words in our corpus, we will now re-run the grid search with more finely tuned parameters for the random forest model. Based on the best performing parameters from the initial random forest grid serach, we are narrowing down the range of values for max_df and min_df in the word vectorizer, and max_depth and min_samples_split for the random forest. I also opted to include dimensionality reduction parameters as options here to fine-tune the model further. Note: The grid search will be run in a script outside of this Jupyter notebook but the code will be pasted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in training dataframe\n",
    "train = pd.read_csv('~/tumor-mutation-classification/lemmatized_train.csv')\n",
    "X = train['lemmatized']\n",
    "Y = train['Class']\n",
    "\n",
    "# Set up pipeline\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer()),\n",
    "    ('reduce_dim', decomposition.TruncatedSVD()),\n",
    "    ('oversample', SMOTE(k_neighbors = 5)),\n",
    "    ('machine', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Feature generation parameters\n",
    "maxdf = [0.65, 0.75, 0.85]\n",
    "mindf = [0.025, 0.05, .075]\n",
    "# Dimensionality reduction parameters\n",
    "n_components = [100, 500, 1000]\n",
    "# Oversampling parameters\n",
    "smote_kind = ['regular','borderline1']\n",
    "# Machine learning model parameters \n",
    "rf_max_depth = [20, 30, 40]\n",
    "rf_min_samples_split = [430, 860]\t# 430 represents 5% of the dataset after oversampling\n",
    "rf_n_estimators = [10]\n",
    "\n",
    "param_grid = [\n",
    "# Tfidf permutations with and without n-grams\n",
    "# Removed permutations with CountVectorizer after logistic regression demonstrated poor log loss scores\n",
    "# Removed permutations without oversampling after logistic regression demonstrated poor log loss scores\n",
    "    {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 5000)],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [RandomForestClassifier(random_state = 1)],\n",
    "        'machine__max_depth': rf_max_depth,\n",
    "        'machine__min_samples_split': rf_min_samples_split,\n",
    "        'machine__n_estimators': rf_n_estimators\n",
    "    },\n",
    "        {\n",
    "        'feat_gen': [TfidfVectorizer(max_features = 5000, ngram_range=(2,2))],\n",
    "        'feat_gen__max_df': maxdf,\n",
    "        'feat_gen__min_df': mindf,\n",
    "        'reduce_dim__n_components': n_components,\n",
    "        'oversample__kind': smote_kind,\n",
    "        'machine': [RandomForestClassifier(random_state = 1)],\n",
    "        'machine__max_depth': rf_max_depth,\n",
    "        'machine__min_samples_split': rf_min_samples_split,\n",
    "        'machine__n_estimators': rf_n_estimators\n",
    "    }\n",
    "]\n",
    "# Run grid search with cross validation\n",
    "print('running grid search with random forest...')\n",
    "grid = GridSearchCVProgressBar(pipe, cv=3, param_grid=param_grid, verbose=2, scoring = 'log_loss')\n",
    "grid.fit(X, Y)\n",
    "print(f'best score: \\n {grid.best_score_}')\n",
    "print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in results from the grid search run externally in a script..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                        random forest\n",
      "1                                              1.67208\n",
      "2                                            0.0287829\n",
      "3    {'feat_gen': TfidfVectorizer(analyzer='word', ...\n",
      "dtype: object\n",
      "{'feat_gen': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.65, max_features=5000, min_df=0.025,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), 'feat_gen__max_df': 0.65, 'feat_gen__min_df': 0.025, 'machine': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=430,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False), 'machine__max_depth': 20, 'machine__min_samples_split': 430, 'machine__n_estimators': 10, 'oversample__kind': 'regular', 'reduce_dim__n_components': 100}\n"
     ]
    }
   ],
   "source": [
    "rf2_results = pd.read_csv('scripts/rf2_gridresults_20180713_13:08.csv')\n",
    "best_rf2 = pd.Series(['random forest', rf2_results['mean'][0], rf2_results['std'][0], rf2_results['params'][0]])\n",
    "print(best_rf2)\n",
    "print(best_rf2[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified the best performing parameters for our narrowed-down grid search, we can test them on our validation dataset. Our new set of parameters is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading in training/testing dataframes\n",
      "Log Loss:  1.63802579801\n",
      "Accuracy:  0.406015037594\n",
      "\n",
      "Confusion Matrix:\n",
      " [[53  2  2 14 16  0  9  9  2]\n",
      " [10 40  6  6  0  2 22 12  0]\n",
      " [ 4  1 11  2  2  0  2  0  0]\n",
      " [48  0  4 56 17  1  6  9  0]\n",
      " [10  1 11  3 10  1  9  0  0]\n",
      " [11  0  1  2  9 17  7  1  0]\n",
      " [13 32 31 14  0  3 78 19  2]\n",
      " [ 0  0  0  1  0  0  1  2  0]\n",
      " [ 1  0  0  0  0  0  0  4  3]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          1       0.35      0.50      0.41       107\n",
      "          2       0.53      0.41      0.46        98\n",
      "          3       0.17      0.50      0.25        22\n",
      "          4       0.57      0.40      0.47       141\n",
      "          5       0.19      0.22      0.20        45\n",
      "          6       0.71      0.35      0.47        48\n",
      "          7       0.58      0.41      0.48       192\n",
      "          8       0.04      0.50      0.07         4\n",
      "          9       0.43      0.38      0.40         8\n",
      "\n",
      "avg / total       0.50      0.41      0.43       665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in training dataframe\n",
    "print(\"loading in training/testing dataframes\")\n",
    "train = pd.read_csv('~/tumor-mutation-classification/scripts/lemmatized_train.csv')\n",
    "X_train = train['lemmatized']\n",
    "Y_train = train['Class']\n",
    "test = pd.read_csv('~/tumor-mutation-classification/lemmatized_test_5000.csv')\n",
    "X_test = test['lemmas']\n",
    "Y_test = test['Class']\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feat_gen', TfidfVectorizer(max_df = 0.65, min_df = 0.025, max_features = 5000, ngram_range = (1,1))),\n",
    "    ('oversample', SMOTE(k_neighbors = 5, kind = 'regular', random_state = 1)),\n",
    "    ('reduce_dim', decomposition.TruncatedSVD(n_components = 100)),\n",
    "    ('machine', RandomForestClassifier(max_depth = 20, min_samples_split = 430, n_estimators = 10, random_state = 1))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "Y_pred = pipe.predict(X_test)\n",
    "Y_pred_prob = pipe.predict_proba(X_test)\n",
    "\n",
    "print('Log Loss: ', log_loss(Y_test, Y_pred_prob))\n",
    "print('Accuracy: ', accuracy_score(Y_test, Y_pred))\n",
    "print('\\nConfusion Matrix:\\n', confusion_matrix(Y_test, Y_pred))\n",
    "print('\\nClassification Report:\\n', classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running a second grid search in which the hyper-parameters were tuned to narrow in on the best-performing parameters from earlier, we found a set of parameters that improved our validation log loss score from 1.72 to **1.64**. The accuracy also improved from 37% to 41%, and the f1-score improved from 0.38 to 0.43. While these are significant improvements, the accuracy remains less than 50%, indicating that this product is not satisfactory for professional use. We will now explore other methods of feature representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "So far, our methods of feature generation have been reliant on one-hot word vectors, which is a sparse representation with no information regarding the meaning or context of the word. We will now experiment with using Word2Vec, a package from Google that relies on distributed representation. Distributed representation means that each word is represented in several places, both as the \"center\" word of a phrase and as a \"context\" word for other words. The resulting word vectors are dense because they carry information about the surrounding words. This is how Word2Vec is able to retain information about word semantics and meaning. One implementation of Word2Vec is gensim's Doc2Vec, which uses similary technology to tag each document and work with the resulting tag vectors to determine document similarity.\n",
    "\n",
    "\n",
    "'Essentially by lemmatization you make the input space sparser, which can help if you don't have enough training data.\n",
    "\n",
    "But since Word2Vec is fairly big, if you have big enough training data, lemmatization shouldn't gain you much.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add tag to each document\n",
    "def tag_docs(df):\n",
    "    docs = []\n",
    "    for row in df.iterrows():\n",
    "        docs.append(TaggedDocument(words = simple_preprocess(row[1]['text']), tags=[row[1]['Class']]))\n",
    "        # simple preprocess converts a document into a list of tokens.\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train doc2vec model\n",
    "def train_doc2vec_model(tagged_docs, window, size):\n",
    "    sents = tagged_docs\n",
    "    doc2vec_model = Doc2Vec(sents, size=size, window=window, iter=20, dm=1)\n",
    "    return doc2vec_model\n",
    "\n",
    "# Other parameter options: min_count=5, workers=1, epochs=20\n",
    "# model.build_vocab(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the final vector feature for the classifier\n",
    "def vec_for_learning(doc2vec_model, tagged_docs):\n",
    "    sents = tagged_docs\n",
    "    Y, X = zip(*[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return Y, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:362: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('~/tumor-mutation-classification/lemmatized_train.csv')\n",
    "test = pd.read_csv('~/tumor-mutation-classification/lemmatized_test_full.csv')\n",
    "\n",
    "train_tagged = tag_docs(train)\n",
    "test_tagged = tag_docs(test)\n",
    "model = train_doc2vec_model(train_tagged, 5, 300)\n",
    "\n",
    "y_train, X_train = vec_for_learning(model, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export vectorized dataframes\n",
    "X_train_np = np.asarray(X_train)\n",
    "y_train_np = np.asarray(y_train)\n",
    "X_test_np = np.asarray(X_test)\n",
    "y_test_np = np.asarray(y_test)\n",
    "\n",
    "np.savetxt(\"d2v_X_train.csv\", X_train_np, delimiter=\",\")\n",
    "np.savetxt(\"d2v_y_train.csv\", y_train_np, delimiter=\",\")\n",
    "np.savetxt(\"d2v_X_test.csv\", X_test_np, delimiter=\",\")\n",
    "np.savetxt(\"d2v_y_test.csv\", y_test_np, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our preprocessed dataframes exported and ready, we can feed them into our classifiers to get a basic picture of how they perform. We will run them through various external grid searches to determine the best hyperparameters to feed into our machine learning models and subsequently validate them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      classifier      mean       std  \\\n",
      "0       logistic  0.786146  0.037976   \n",
      "2        xgboost  0.885314  0.051294   \n",
      "1  random forest  1.429836  0.032435   \n",
      "\n",
      "                                              params  \n",
      "0  {'machine': LogisticRegression(C=0.1, class_we...  \n",
      "2  {'machine': XGBClassifier(base_score=0.5, boos...  \n",
      "1  {'machine': RandomForestClassifier(bootstrap=T...  \n"
     ]
    }
   ],
   "source": [
    "d2v_logistic_results = pd.read_csv('scripts/d2v_logistic_results_20180730_18:43.csv')\n",
    "#d2v_nb_results = pd.read_csv('scripts/nb_gridresults_20180703_18:04.csv')\n",
    "d2v_rf_results = pd.read_csv('scripts/d2v_rf_gridresults_20180730_18:39.csv')\n",
    "d2v_xg_results = pd.read_csv('scripts/d2v_xg_gridresults_20180730_05:53.csv')\n",
    "\n",
    "best_logistic = pd.Series(['logistic', d2v_logistic_results['mean'][0], d2v_logistic_results['std'][0], d2v_logistic_results['params'][0]])\n",
    "#best_nb = pd.Series(['naive bayes', d2v_nb_results['mean'][0], d2v_nb_results['std'][0], d2v_nb_results['params'][0]])\n",
    "best_rf = pd.Series(['random forest', d2v_rf_results['mean'][0], d2v_rf_results['std'][0], d2v_rf_results['params'][0]])\n",
    "best_xg = pd.Series(['xgboost', d2v_xg_results['mean'][0], d2v_xg_results['std'][0], d2v_xg_results['params'][0]])\n",
    "best_gridresults = pd.DataFrame().append([best_logistic, best_rf, best_xg], ignore_index=True)\n",
    "best_gridresults.columns = ['classifier','mean','std','params']\n",
    "best_gridresults = best_gridresults.sort_values(by=['mean'], ascending=True)\n",
    "print(best_gridresults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it appears that using a Doc2Vec feature representation is highly beneficial to our log loss score. The logistic regression model performed best here, while the random forest performed best in the first round of grid searches. Let's confirm this on our validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'machine': LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False), 'machine__C': 0.1, 'reduce_dim__n_components': 50}\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('~/tumor-mutation-classification/scripts/d2v_X_train.csv')\n",
    "X_test = pd.read_csv('~/tumor-mutation-classification/scripts/d2v_X_test.csv')\n",
    "y_train = pd.read_csv('~/tumor-mutation-classification/scripts/d2v_y_train.csv')\n",
    "y_test = pd.read_csv('~/tumor-mutation-classification/scripts/d2v_y_test.csv')\n",
    "\n",
    "print(best_gridresults['params'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss:  1.62763458216\n",
      "Accuracy:  0.573795180723\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 51   3   0  22  13   6  10   1   0]\n",
      " [  6  42   2   4   1   3  38   1   1]\n",
      " [  1   0  10   5   2   1   3   0   0]\n",
      " [ 29   3   4  84  15   2   3   1   0]\n",
      " [  6   0   1   3  24   5   6   0   0]\n",
      " [  5   3   1   0   5  29   5   0   0]\n",
      " [  6  26   9   2  13   2 133   0   1]\n",
      " [  0   1   0   0   0   0   1   1   1]\n",
      " [  0   0   0   0   0   0   0   1   7]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        1.0       0.49      0.48      0.49       106\n",
      "        2.0       0.54      0.43      0.48        98\n",
      "        3.0       0.37      0.45      0.41        22\n",
      "        4.0       0.70      0.60      0.64       141\n",
      "        5.0       0.33      0.53      0.41        45\n",
      "        6.0       0.60      0.60      0.60        48\n",
      "        7.0       0.67      0.69      0.68       192\n",
      "        8.0       0.20      0.25      0.22         4\n",
      "        9.0       0.70      0.88      0.78         8\n",
      "\n",
      "avg / total       0.59      0.57      0.58       664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up pipeline\n",
    "# Parameters reduced to best-performing parameters from prior grid search\n",
    "def logistic_validation(X_train, Y_train, X_test, Y_test):\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('reduce_dim', decomposition.TruncatedSVD(n_components = 50)),\n",
    "        ('oversample', SMOTE(k_neighbors = 5, kind = 'regular', random_state = 1)),\n",
    "        ('machine', LogisticRegression(C = 0.1, penalty = 'l2', random_state = 1))\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X_train, Y_train)\n",
    "    Y_pred = pipe.predict(X_test)\n",
    "    Y_pred_prob = pipe.predict_proba(X_test)\n",
    "\n",
    "    print('Log Loss: ', log_loss(Y_test, Y_pred_prob))\n",
    "    print('Accuracy: ', accuracy_score(Y_test, Y_pred))\n",
    "    print('\\nConfusion Matrix:\\n', confusion_matrix(Y_test, Y_pred))\n",
    "    print('\\nClassification Report:\\n', classification_report(Y_test, Y_pred))\n",
    "    \n",
    "logistic_validation(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a log loss score of **1.63**, this method of feature representation performs slightly better than tf-idf vectorization, which had a validated log loss score of 1.64. The f1-score went up to 0.58 (compared to 0.43 with tf-idf and random forest) but the f1-score is less meaningful than the log loss score because it doesn't reflect the model's confidence in its predictions. Any accuracy gained in the f1-score is likely the result of chance, while an improvement in the log loss score indicates that the model was more confident in its correct predictions (or less confident in its wrong ones). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "At this point, we have tried several methods of NLP feature representation for this text dataset. What started as a list of peer-reviewed journal articles became a matrix of numbers and one-hot vectors as we experimented with tf-idf and bag-of-words vectorization. Unfortunately, neither were sufficient to classify our test documents into their rightful classes. <br>\n",
    "\n",
    "In our first round of feature engineering, we created a pipeline to test several different hyperparameters to tune the vectorization process and ran each resulting dataframe through a grid search to determine the best machine learning model parameters. Logistic regression, random forest, XGBoost, and Naive Bayes were implemented; of these models, random forest performed the best with our tf-idf vectorized dataset. After some more fine tuning of the feature generation/machine learning model, we were able to come up with a log loss score of 1.64. This involved using tf-idf vectorization and reducing our feature set down to our 5000 most common words (after filtering out stopwords and lemmatizing). One potential issue here is that the 5000 most common words may not actually be our best features, but we reduced the dataset to these words for the sake of computational and conceptual simplicity. <br>\n",
    "\n",
    "It became clear that tf-idf and bag-of-words vectorization were not performing well through inspection of the f1-score, a reflection of both precision and recall. Although log loss is the objective scoring measure we are using to judge our models against eachother, there is no standard way of interpreting a lone log loss score. For more interpretability we looked into the f1-score, conveniently ranked between 0 and 1. The best model we could ascertain had an f1-score of 0.43, with average precision at 0.50 and recall at 0.41. This is a poor classifier for our models, as only half of its positive predictions are correct. <br>\n",
    "\n",
    "This led us to seek out a more nuanced method of feature representation. I chose the Doc2Vec feature of Google's Word2Vec for its ability to incorporate semantics and concepts into the word vectors. With only one iteration of hyperparameters for training the Doc2Vec model, we re-ran all of our machine learning classifiers and found that logistic regression was the best way to model this feature set. Although our F1-score improved significantly (from 0.43 to 0.58), our log loss score only improved by 0.01. This indicates that any improvement in our accuracy was likely due to chance, as the model did not gain any confidence in its correct predictions. <br>\n",
    "\n",
    "For the sake of brevity we did not optimize the hyperparameters for the Doc2Vec model the same way that we did for the first round of feature engineering, though this is certainly something we could explore further in the future. However, our model will only ever be as good as the data we give it, and this may be our biggest limitation. NLP projects tend to work best when distinguishing categories or writing styles, but since we are working with a body of technical writing, much of the structure and vocabulary remains consistent across the board. Technical writing abides by certain guidelines of how sentences are meant to be formed and there is little room left for personal touch or artistic interpretation. <br>\n",
    "\n",
    "Additionally, the severe class imbalance is likely not helping our models. We found through cross-validation that oversampling is indeed beneficial for this dataset, especially when paired with significant dimensionality reduction, but these two techniques together are still not enough to save the model's overall performance. More data (especially in the minority classes) would certainly help our case, but if the style of technical writing remains so cut-and-dried it may give credence to the notion that this dataset is not suitable for a supervised learning model.<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
