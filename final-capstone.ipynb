{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tumor Mutation Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will analyze the dataset which contains tumor gene mutations and their risk category, which corresponds to their risk of malignancy in the human condition. The dataset was hand-labeled and released by the team of clinical pathologists at Memorial Sloan Kettering in 2018. Our objective of this project is to fit the dataset into our machine learning models to predict the risk category while accounting for highly unbalanced classes. Several methods for text feature generation will be explored and the resulting features will be reduced using principle component analysis (PCA). We will then use the synthetic minority over-sampling technique (SMOTE) to resample the dataset to make the numbers of categories more even. The last step is to compare the machine learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "#nlp = spacy.blank('en')\n",
    "#nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In this section we will load in all our data and format them to the appropriate data types. The text also needs to be cleaned of all non-legitimate words, such as figure references and parentheses. This will be accomplished using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up packages for loading in data\n",
    "client = boto3.client('s3') #low-level functional API\n",
    "\n",
    "resource = boto3.resource('s3') #high-level object-oriented API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in training data labels\n",
    "obj = client.get_object(Bucket='thinkful-capstone', Key='training_variants')\n",
    "stream = io.BytesIO(obj['Body'].read())\n",
    "training_variants = pd.read_csv(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID    Gene             Variation  Class\n",
      "0   0  FAM58A  Truncating Mutations      1\n",
      "1   1     CBL                 W802*      2\n",
      "2   2     CBL                 Q249E      2\n",
      "3   3     CBL                 N454D      3\n",
      "4   4     CBL                 L399V      4\n",
      "(3321, 4)\n"
     ]
    }
   ],
   "source": [
    "print(training_variants.head())\n",
    "print(training_variants.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID||Text\n",
      "0||Cyclin-dependent kinases (CDKs) regulate a variety of fundamental cellular processes. CDK10 stands out as one of the last orphan CDKs for which no activating cyclin has been identified and no kinase activity revealed. Previous work has shown that CDK10 silencing increases ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2)-driven activation of the MAPK pathway, which confers tamoxifen resistance to breast cancer cells. The precise mechanisms by which CDK10 modulates ETS2 activity, and more generally the functions of CDK10, remain elusive. Here we demonstrate that CDK10 is a cyclin-dependent kinase by identifying cyclin M as an activating cyclin. Cyclin M, an orphan cyclin, is the product of FAM58A, whose mutations cause STAR syndrome, a human developmental anomaly whose features include toe syndactyly, telecanthus, and anogenital and renal malformations. We show that STAR syndrome-associated cyclin M mutants are unable to interact with CDK10. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and in conferring tamoxifen resistance to breast cancer cells. CDK10/cyclin M phosphorylates ETS2 in vitro, and in cells it positively controls ETS2 degradation by the proteasome. ETS2 protein levels are increased in cells derived from a STAR patient, and this increase is attributable to decreased cyclin M levels. Altogether, our results reveal an additional regulatory mechanism for ETS2, which plays key roles in cancer and development. They also shed light on the molecular mechanisms underlying STAR syndrome.Cyclin-dependent kinases (CDKs) play a pivotal role in the control of a number of fundamental cellular processes (1). The human genome contains 21 genes encoding proteins that can be considered as members of the CDK family owing to their sequence similarity with bona fide CDKs, those known to be activated by cyclins (2). Although discovered almost 20 y ago (3, 4), CDK10 remains one of the two CDKs without an identified cyclin partner. This knowledge gap has largely impeded the exploration of its biological functions. CDK10 can act as a positive cell cycle regulator in some cells (5, 6) or as a tumor suppressor in others (7, 8). CDK10 interacts with the ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2) transcription factor and inhibits its transcriptional activity through an unknown mechanism (9). CDK10 knockdown derepresses ETS2, which increases the expression of the c-Raf protein kinase, activates the MAPK pathway, and induces resistance of MCF7 cells to tamoxifen (6).Here, we deorphanize CDK10 by identifying cyclin M, the product of FAM58A, as a binding partner. Mutations in this gene that predict absence or truncation of cyclin M are associated with STAR syndrome, whose features include toe syndactyly, telecanthus, and anogenital and renal malformations in heterozygous females (10). However, both the functions of cyclin M and the pathogenesis of STAR syndrome remain unknown. We show that a recombinant CDK10/cyclin M heterodimer is an active protein kinase that phosphorylates ETS2 in vitro. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and phospho-ERK expression levels and in inducing tamoxifen resistance in estrogen receptor (ER)+ breast cancer cells. We show that CDK10/cyclin M positively controls ETS2 degradation by the proteasome, through the phosphorylation of two neighboring serines. Finally, we detect an increased ETS2 expression level in cells derived from a STAR patient, and we demonstrate that it is attributable to the decreased cyclin M expression level observed in these cells.Previous SectionNext SectionResultsA yeast two-hybrid (Y2H) screen unveiled an interaction signal between CDK10 and a mouse protein whose C-terminal half presents a strong sequence homology with the human FAM58A gene product [whose proposed name is cyclin M (11)]. We thus performed Y2H mating assays to determine whether human CDK10 interacts with human cyclin M (Fig. 1 A–C). The longest CDK10 isoform (P1) expressed as a bait protein produced a strong interaction phenotype with full-length cyclin M (expressed as a prey protein) but no detectable phenotype with cyclin D1, p21 (CIP1), and Cdi1 (KAP), which are known binding partners of other CDKs (Fig. 1B). CDK1 and CDK3 also produced Y2H signals with cyclin M, albeit notably weaker than that observed with CDK10 (Fig. 1B). An interaction phenotype was also observed between full-length cyclin M and CDK10 proteins expressed as bait and prey, respectively (Fig. S1A). We then tested different isoforms of CDK10 and cyclin M originating from alternative gene splicing, and two truncated cyclin M proteins corresponding to the hypothetical products of two mutated FAM58A genes found in STAR syndrome patients (10). None of these shorter isoforms produced interaction phenotypes (Fig. 1 A and C and Fig. S1A).Fig. 1.In a new window Download PPTFig. 1.CDK10 and cyclin M form an interaction complex. (A) Schematic representation of the different protein isoforms analyzed by Y2H assays. Amino acid numbers are indicated. Black boxes indicate internal deletions. The red box indicates a differing amino acid sequence compared with CDK10 P1. (B) Y2H assay between a set of CDK proteins expressed as baits (in fusion to the LexA DNA binding domain) and CDK interacting proteins expressed as preys (in fusion to the B42 transcriptional activator). pEG202 and pJG4-5 are the empty bait and prey plasmids expressing LexA and B42, respectively. lacZ was used as a reporter gene, and blue yeast are indicative of a Y2H interaction phenotype. (C) Y2H assay between the different CDK10 and cyclin M isoforms. The amino-terminal region of ETS2, known to interact with CDK10 (9), was also assayed. (D) Western blot analysis of Myc-CDK10 (wt or kd) and CycM-V5-6His expression levels in transfected HEK293 cells. (E) Western blot analysis of Myc-CDK10 (wt or kd) immunoprecipitates obtained using the anti-Myc antibody. “Inputs” correspond to 10 μg total lysates obtained from HEK293 cells coexpressing Myc-CDK10 (wt or kd) and CycM-V5-6His. (F) Western blot analysis of immunoprecipitates obtained using the anti-CDK10 antibody or a control goat antibody, from human breast cancer MCF7 cells. “Input” corresponds to 30 μg MCF7 total cell lysates. The lower band of the doublet observed on the upper panel comigrates with the exogenously expressed untagged CDK10 and thus corresponds to endogenous CDK10. The upper band of the doublet corresponds to a nonspecific signal, as demonstrated by it insensitivity to either overexpression of CDK10 (as seen on the left lane) or silencing of CDK10 (Fig. S2B). Another experiment with a longer gel migration is shown in Fig. S1D.Next we examined the ability of CDK10 and cyclin M to interact when expressed in human cells (Fig. 1 D and E). We tested wild-type CDK10 (wt) and a kinase dead (kd) mutant bearing a D181A amino acid substitution that abolishes ATP binding (12). We expressed cyclin M-V5-6His and/or Myc-CDK10 (wt or kd) in a human embryonic kidney cell line (HEK293). The expression level of cyclin M-V5-6His was significantly increased upon coexpression with Myc-CDK10 (wt or kd) and, to a lesser extent, that of Myc-CDK10 (wt or kd) was increased upon coexpression with cyclin M-V5-6His (Fig. 1D). We then immunoprecipitated Myc-CDK10 proteins and detected the presence of cyclin M in the CDK10 (wt) and (kd) immunoprecipitates only when these proteins were coexpressed pair-wise (Fig. 1E). We confirmed these observations by detecting the presence of Myc-CDK10 in cyclin M-V5-6His immunoprecipitates (Fig. S1B). These experiments confirmed the lack of robust interaction between the CDK10.P2 isoform and cyclin M (Fig. S1C). To detect the interaction between endogenous proteins, we performed immunoprecipitations on nontransfected MCF7 cells derived from a human breast cancer. CDK10 and cyclin M antibodies detected their cognate endogenous proteins by Western blotting. We readily detected cyclin M in immunoprecipitates obtained with the CDK10 antibody but not with a control antibody (Fig. 1F). These results confirm the physical interaction between CDK10 and cyclin M in human cells.To unveil a hypothesized CDK10/cyclin M protein kinase activity, we produced GST-CDK10 and StrepII-cyclin M fusion proteins in insect cells, either individually or in combination. We observed that GST-CDK10 and StrepII-cyclin M copurified, thus confirming their interaction in yet another cellular model (Fig. 2A). We then performed in vitro kinase assays with purified proteins, using histone H1 as a generic substrate. Histone H1 phosphorylation was detected only from lysates of cells coexpressing GST-CDK10 and StrepII-cyclin M. No phosphorylation was detected when GST-CDK10 or StrepII-cyclin M were expressed alone, or when StrepII-cyclin M was coexpressed with GST-CDK10(kd) (Fig. 2A). Next we investigated whether ETS2, which is known to interact with CDK10 (9) (Fig. 1C), is a phosphorylation substrate of CDK10/cyclin M. We detected strong phosphorylation of ETS2 by the GST-CDK10/StrepII-cyclin M purified heterodimer, whereas no phosphorylation was detected using GST-CDK10 alone or GST-CDK10(kd)/StrepII-cyclin M heterodimer (Fig. 2B).Fig. 2.In a new window Download PPTFig. 2.CDK10 is a cyclin M-dependent protein kinase. (A) In vitro protein kinase assay on histone H1. Lysates from insect cells expressing different proteins were purified on a glutathione Sepharose matrix to capture GST-CDK10(wt or kd) fusion proteins alone, or in complex with STR-CycM fusion protein. Purified protein expression levels were analyzed by Western blots (Top and Upper Middle). The kinase activity was determined by autoradiography of histone H1, whose added amounts were visualized by Coomassie staining (Lower Middle and Bottom). (B) Same as in A, using purified recombinant 6His-ETS2 as a substrate.CDK10 silencing has been shown to increase ETS2-driven c-RAF transcription and to activate the MAPK \n"
     ]
    }
   ],
   "source": [
    "# Load in training data text articles\n",
    "obj = client.get_object(Bucket=\"thinkful-capstone\",Key=\"training_text\")\n",
    "raw_training_text = obj[\"Body\"].read()\n",
    "training_text = raw_training_text.decode('utf-8')\n",
    "print(training_text[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211296707\n",
      "205598350\n",
      "ID||Text\n",
      "0||Cyclin-dependent kinases (CDKs) regulate a variety of fundamental cellular processes. CDK10 stands out as one of the last orphan CDKs for which no activating cyclin has been identified and no kinase activity revealed. Previous work has shown that CDK10 silencing increases ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2)-driven activation of the MAPK pathway, which confers tamoxifen resistance to breast cancer cells. The precise mechanisms by which CDK10 modulates ETS2 activity, and more generally the functions of CDK10, remain elusive. Here we demonstrate that CDK10 is a cyclin-dependent kinase by identifying cyclin M as an activating cyclin. Cyclin M, an orphan cyclin, is the product of FAM58A, whose mutations cause STAR syndrome, a human developmental anomaly whose features include toe syndactyly, telecanthus, and anogenital and renal malformations. We show that STAR syndrome-associated cyclin M mutants are unable to interact with CDK10. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and in conferring tamoxifen resistance to breast cancer cells. CDK10/cyclin M phosphorylates ETS2 in vitro, and in cells it positively controls ETS2 degradation by the proteasome. ETS2 protein levels are increased in cells derived from a STAR patient, and this increase is attributable to decreased cyclin M levels. Altogether, our results reveal an additional regulatory mechanism for ETS2, which plays key roles in cancer and development. They also shed light on the molecular mechanisms underlying STAR syndrome.Cyclin-dependent kinases (CDKs) play a pivotal role in the control of a number of fundamental cellular processes. The human genome contains 21 genes encoding proteins that can be considered as members of the CDK family owing to their sequence similarity with bona fide CDKs, those known to be activated by cyclins. Although discovered almost 20 y ago, CDK10 remains one of the two CDKs without an identified cyclin partner. This knowledge gap has largely impeded the exploration of its biological functions. CDK10 can act as a positive cell cycle regulator in some cells or as a tumor suppressor in others. CDK10 interacts with the ETS2 (v-ets erythroblastosis virus E26 oncogene homolog 2) transcription factor and inhibits its transcriptional activity through an unknown mechanism. CDK10 knockdown derepresses ETS2, which increases the expression of the c-Raf protein kinase, activates the MAPK pathway, and induces resistance of MCF7 cells to tamoxifen.Here, we deorphanize CDK10 by identifying cyclin M, the product of FAM58A, as a binding partner. Mutations in this gene that predict absence or truncation of cyclin M are associated with STAR syndrome, whose features include toe syndactyly, telecanthus, and anogenital and renal malformations in heterozygous females. However, both the functions of cyclin M and the pathogenesis of STAR syndrome remain unknown. We show that a recombinant CDK10/cyclin M heterodimer is an active protein kinase that phosphorylates ETS2 in vitro. Cyclin M silencing phenocopies CDK10 silencing in increasing c-Raf and phospho-ERK expression levels and in inducing tamoxifen resistance in estrogen receptor (ER)+ breast cancer cells. We show that CDK10/cyclin M positively controls ETS2 degradation by the proteasome, through the phosphorylation of two neighboring serines. Finally, we detect an increased ETS2 expression level in cells derived from a STAR patient, and we demonstrate that it is attributable to the decreased cyclin M expression level observed in these cells.Previous SectionNext SectionResultsA yeast two-hybrid (Y2H) screen unveiled an interaction signal between CDK10 and a mouse protein whose C-terminal half presents a strong sequence homology with the human FAM58A gene product [whose proposed name is cyclin M]. We thus performed Y2H mating assays to determine whether human CDK10 interacts with human cyclin M. The longest CDK10 isoform (P1) expressed as a bait protein produced a\n"
     ]
    }
   ],
   "source": [
    "# Eliminate references and abbreviations within parentheses\n",
    "print(len(training_text))\n",
    "training_text = re.sub(' \\(Fig \\d+.+?\\)', '', training_text)\n",
    "training_text = re.sub(' \\(Fig\\. \\d+.+?\\)', '', training_text)\n",
    "training_text = re.sub(' \\(\\d.*?\\)', '', training_text)\n",
    "training_text = re.sub(' \\([A-Z]\\)', '', training_text)\n",
    "print(len(training_text))\n",
    "print(training_text[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3321\n"
     ]
    }
   ],
   "source": [
    "# Split text file into list of documents\n",
    "training_list = training_text.split('||')\n",
    "training_list = training_list[2:]\n",
    "print(len(training_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID    Gene             Variation  Class  \\\n",
      "0   0  FAM58A  Truncating Mutations      1   \n",
      "1   1     CBL                 W802*      2   \n",
      "2   2     CBL                 Q249E      2   \n",
      "3   3     CBL                 N454D      3   \n",
      "4   4     CBL                 L399V      4   \n",
      "\n",
      "                                                text  \n",
      "0  Cyclin-dependent kinases (CDKs) regulate a var...  \n",
      "1   Abstract Background  Non-small cell lung canc...  \n",
      "2   Abstract Background  Non-small cell lung canc...  \n",
      "3  Recent evidence has demonstrated that acquired...  \n",
      "4  Oncogenic mutations in the monomeric Casitas B...  \n"
     ]
    }
   ],
   "source": [
    "# Load training text into dataframe\n",
    "texts_df = pd.DataFrame(training_list, columns = ['text'])\n",
    "\n",
    "# Merge text dataframe with labels dataframe\n",
    "train = pd.concat([training_variants, texts_df], axis=1)\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is loaded into a dataframe, let's do some preliminary data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3321\n",
      "7    953\n",
      "4    686\n",
      "1    568\n",
      "2    452\n",
      "6    275\n",
      "5    242\n",
      "3     89\n",
      "9     37\n",
      "8     19\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3321 total datapoints to work with, and it looks like we are dealing with significant class imbalance. Class 7 has 953 datapoints, while Class 8 has only 19. We will have to address this class imbalance with our experiment design. Additionally, the labels have been anonymized, which means we cannot draw any insight about what these classes might signify. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Design\n",
    "\n",
    "The prevalence of class imbalance has serious implications for our analysis. First and foremost, we must establish our scoring metric. The purpose here is to use the relevant clinical texts to predict the mutation category for each gene/mutation pair. While we want the predictions to be as accurate as possible, simple classification accuracy is not a representative way to judge models that are built on class imbalance, as they may achieve high accuracy by simply predicting the most common class every time. <br>\n",
    "\n",
    "Given that we are working with a multi-label classifier, the most appropriate scoring metric is Cohen's Kappa coefficient (K). This statistic measures inter-rater agreement for categorical items. Contraty to conventional classification accuracy, the K statistic accounts for the possibility of the agreement occurring by chance. <br>\n",
    "\n",
    "The Kappa statistic varies from 0 to 1, where 0 = agreement equivalent to chance, and 1 = perfect agreement. We will be choosing models with Kappa statistics closer to 1. We will also look at the precision and recall via the F1 score. Though these are not optimized for multi-label classification, they will be interesting to consider. <br>\n",
    "\n",
    "We will also try oversampling the lesser-represented categories and apply our machine learning models on the oversampled datasets, judging by their Kappa statistic. Oversampling can be achieved by generating duplicate datapoints or by generating new synthetic datapoints via SMOTE, the Synthetic Minority Oversampling Technique. <br>\n",
    "\n",
    "I will use various methods of feature generation including classic NLP techniques such as bag-of-words, tf-idf, and n-grams. These methods of feature generation will be applied to both the original and oversampled datasets. They will then be subjected to various machine learning models. Decision trees are known to perform well on unbalanced datasets, so this model may prevail on the original data. However, Naive Bayes is known to perform well on natural langauge, so once the dataset is oversampled it is possible that Naive Bayes will perform the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is relatively clean already, and containts no NaN values. It needs to be tokenized so it can be processed into readable pieces of data. We will use spaCy to tokenize the data and create a new column with a list of the tokens for each row. Furthermore, we will convert all tokens that are not stop words or punctuation to lemmas to reduce the noise from unnecessary words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### The following will not run due to computing restraints (programmatic spacy tokenization/lemma conversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train['spacy_tokens'] = train['text'].apply(lambda x: nlp(x))\n",
    "\n",
    "def lemmatize(x):\n",
    "    return [token.lemma_ for token in x\n",
    "                      if not token.is_stop\n",
    "                      and not token.is_punct\n",
    "                      and not token.lemma_ == \"-PRON-\"\n",
    "                      and not token.lemma_ == \" \" or \"  \"\n",
    "    ]\n",
    "\n",
    "train['lemmas'] = train['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def lemmatize(x):\n",
    "    return [token.lemma_ for token in x\n",
    "                      if not token.is_stop\n",
    "                      and not token.is_punct\n",
    "                      and not token.lemma_ == \"-PRON-\"\n",
    "                      and not token.lemma_ == \" \" or \"  \"\n",
    "    ]\n",
    "\n",
    "segment_size = 50\n",
    "num_segments = int(len(train)/segment_size)\n",
    "print('segment size', segment_size)\n",
    "print(num_segments, 'segments')\n",
    "\n",
    "train_token = pd.DataFrame()\n",
    "    \n",
    "for current_segment in range(num_segments - 1):\n",
    "    start = current_segment * segment_size\n",
    "    if current_segment == num_segments - 1:\n",
    "        end = len(train)\n",
    "    else:\n",
    "        end = start + (segment_size - 1)\n",
    "\n",
    "    segment_df = train[start:end]\n",
    "    segment_df['spacy_tokens'] = segment_df['text'].apply(lambda x: nlp(x))\n",
    "    train_token.append(segment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also suggested: <br>\n",
    "nlp = spacy.blank('de') <br>\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### Manual workaround"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this manual workaround doesn't work...it wasn't removing the stop words (which i wanted to do to reduce features/processing time for tf-idf vectorization) so i changed the language from spacy.blank to normal spacy.load and now i'm getting memory error here. attempted to add swap space using the following method: http://blog.nateharada.com/adding-swap-ec2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ubuntu/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Create a function to filter out stopwords/punctuation and convert to lemma\n",
    "def lemmatize(x):\n",
    "    return [token.lemma_ for token in x\n",
    "                      if not token.is_stop\n",
    "                      and not token.is_punct\n",
    "                      and not token.lemma_ == \"-PRON-\"\n",
    "                      and not token.lemma_ == \" \" or \"  \"\n",
    "    ]\n",
    "# we have a problem here because it is separating hyphenated words and it shouldn't be doing that\n",
    "# also we probably want to keep the original PRONs\n",
    "# so let's move forward not using lemmas. not worth the hassle\n",
    "\n",
    "# Create an empty DataFrame to later add spacy tokens\n",
    "train_token = pd.DataFrame()\n",
    "\n",
    "df_0 = train[0:100]\n",
    "df_0['spacy_tokens'] = df_0['text'].apply(lambda x: nlp(x))\n",
    "df_0['lemmas'] = df_0['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_0, ignore_index=True)\n",
    "\n",
    "df_100 = train[100:200]\n",
    "df_100['spacy_tokens'] = df_100['text'].apply(lambda x: nlp(x))\n",
    "df_100['lemmas'] = df_100['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_100, ignore_index=True)\n",
    "\n",
    "df_200 = train[200:300]\n",
    "df_200['spacy_tokens'] = df_200['text'].apply(lambda x: nlp(x))\n",
    "df_200['lemmas'] = df_200['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_200, ignore_index=True)\n",
    "\n",
    "df_300 = train[300:400]\n",
    "df_300['spacy_tokens'] = df_300['text'].apply(lambda x: nlp(x))\n",
    "df_300['lemmas'] = df_300['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_300, ignore_index=True)\n",
    "\n",
    "df_400 = train[400:500]\n",
    "df_400['spacy_tokens'] = df_400['text'].apply(lambda x: nlp(x))\n",
    "df_400['lemmas'] = df_400['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_400, ignore_index=True)\n",
    "\n",
    "df_500 = train[500:600]\n",
    "df_500['spacy_tokens'] = df_500['text'].apply(lambda x: nlp(x))\n",
    "df_500['lemmas'] = df_500['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_500, ignore_index=True)\n",
    "\n",
    "df_600 = train[600:700]\n",
    "df_600['spacy_tokens'] = df_600['text'].apply(lambda x: nlp(x))\n",
    "df_600['lemmas'] = df_600['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_600, ignore_index=True)\n",
    "\n",
    "df_700 = train[700:800]\n",
    "df_700['spacy_tokens'] = df_700['text'].apply(lambda x: nlp(x))\n",
    "df_700['lemmas'] = df_700['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_700, ignore_index=True)\n",
    "\n",
    "df_800 = train[800:900]\n",
    "df_800['spacy_tokens'] = df_800['text'].apply(lambda x: nlp(x))\n",
    "df_800['lemmas'] = df_800['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_800, ignore_index=True)\n",
    "\n",
    "df_900 = train[900:1000]\n",
    "df_900['spacy_tokens'] = df_900['text'].apply(lambda x: nlp(x))\n",
    "df_900['lemmas'] = df_900['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_900, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_1000 = train[1000:1100]\n",
    "df_1000['spacy_tokens'] = df_1000['text'].apply(lambda x: nlp(x))\n",
    "df_1000['lemmas'] = df_1000['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_1000, ignore_index=True)\n",
    "\n",
    "df_1100 = train[1100:1200]\n",
    "df_1100['spacy_tokens'] = df_1100['text'].apply(lambda x: nlp(x))\n",
    "df_1100['lemmas'] = df_1100['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_1100, ignore_index=True)\n",
    "\n",
    "df_1200 = train[1200:1300]\n",
    "df_1200['spacy_tokens'] = df_1200['text'].apply(lambda x: nlp(x))\n",
    "df_1200['lemmas'] = df_1200['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_1200, ignore_index=True)\n",
    "\n",
    "df_1300 = train[1300:1400]\n",
    "df_1300['spacy_tokens'] = df_1300['text'].apply(lambda x: nlp(x))\n",
    "df_1300['lemmas'] = df_1300['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_1300, ignore_index=True)\n",
    "\n",
    "df_1400 = train[1400:1500]\n",
    "df_1400['spacy_tokens'] = df_1400['text'].apply(lambda x: nlp(x))\n",
    "df_1400['lemmas'] = df_1400['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_1400, ignore_index=True)\n",
    "\n",
    "df_1500 = train[1500:1600]\n",
    "df_1500['spacy_tokens'] = df_1500['text'].apply(lambda x: nlp(x))\n",
    "df_1500['lemmas'] = df_1500['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_1500, ignore_index=True)\n",
    "\n",
    "df_1600 = train[1600:1700]\n",
    "df_1600['spacy_tokens'] = df_1600['text'].apply(lambda x: nlp(x))\n",
    "df_1600['lemmas'] = df_1600['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_1600, ignore_index=True)\n",
    "\n",
    "df_1700 = train[1700:1800]\n",
    "df_1700['spacy_tokens'] = df_1700['text'].apply(lambda x: nlp(x))\n",
    "df_1700['lemmas'] = df_1700['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_1700, ignore_index=True)\n",
    "\n",
    "df_1800 = train[1800:1900]\n",
    "df_1800['spacy_tokens'] = df_1800['text'].apply(lambda x: nlp(x))\n",
    "df_1800['lemmas'] = df_1800['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_1800, ignore_index=True)\n",
    "\n",
    "df_1900 = train[1900:2000]\n",
    "df_1900['spacy_tokens'] = df_1900['text'].apply(lambda x: nlp(x))\n",
    "df_1900['lemmas'] = df_1900['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_1900, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_2000 = train[2000:2100]\n",
    "df_2000['spacy_tokens'] = df_2000['text'].apply(lambda x: nlp(x))\n",
    "df_2000['lemmas'] = df_2000['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_2000, ignore_index=True)\n",
    "\n",
    "df_2100 = train[2100:2200]\n",
    "df_2100['spacy_tokens'] = df_2100['text'].apply(lambda x: nlp(x))\n",
    "df_2100['lemmas'] = df_2100['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_2100, ignore_index=True)\n",
    "\n",
    "df_2200 = train[2200:2300]\n",
    "df_2200['spacy_tokens'] = df_2200['text'].apply(lambda x: nlp(x))\n",
    "df_2200['lemmas'] = df_2200['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_2200, ignore_index=True)\n",
    "\n",
    "df_2300 = train[2300:2400]\n",
    "df_2300['spacy_tokens'] = df_2300['text'].apply(lambda x: nlp(x))\n",
    "df_2300['lemmas'] = df_2300['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_2300, ignore_index=True)\n",
    "\n",
    "df_2400 = train[2400:2500]\n",
    "df_2400['spacy_tokens'] = df_2400['text'].apply(lambda x: nlp(x))\n",
    "df_2400['lemmas'] = df_2400['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_2400, ignore_index=True)\n",
    "\n",
    "df_2500 = train[2500:2600]\n",
    "df_2500['spacy_tokens'] = df_2500['text'].apply(lambda x: nlp(x))\n",
    "df_2500['lemmas'] = df_2500['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_2500, ignore_index=True)\n",
    "\n",
    "df_2600 = train[2600:2700]\n",
    "df_2600['spacy_tokens'] = df_2600['text'].apply(lambda x: nlp(x))\n",
    "df_2600['lemmas'] = df_2600['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_2600, ignore_index=True)\n",
    "\n",
    "df_2700 = train[2700:2800]\n",
    "df_2700['spacy_tokens'] = df_2700['text'].apply(lambda x: nlp(x))\n",
    "df_2700['lemmas'] = df_2700['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_2700, ignore_index=True)\n",
    "\n",
    "df_2800 = train[2800:2900]\n",
    "df_2800['spacy_tokens'] = df_2800['text'].apply(lambda x: nlp(x))\n",
    "df_2800['lemmas'] = df_2800['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_2800, ignore_index=True)\n",
    "\n",
    "df_2900 = train[2900:3000]\n",
    "df_2900['spacy_tokens'] = df_2900['text'].apply(lambda x: nlp(x))\n",
    "df_2900['lemmas'] = df_2900['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_2900, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_3000 = train[3000:3100]\n",
    "df_3000['spacy_tokens'] = df_3000['text'].apply(lambda x: nlp(x))\n",
    "df_3000['lemmas'] = df_3000['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_3000, ignore_index=True)\n",
    "\n",
    "df_3100 = train[3100:3200]\n",
    "df_3100['spacy_tokens'] = df_3100['text'].apply(lambda x: nlp(x))\n",
    "df_3100['lemmas'] = df_3100['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_3100, ignore_index=True)\n",
    "\n",
    "df_3200 = train[3200:3300]\n",
    "df_3200['spacy_tokens'] = df_3200['text'].apply(lambda x: nlp(x))\n",
    "df_3200['lemmas'] = df_3200['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_3200, ignore_index=True)\n",
    "\n",
    "df_3300 = train[3300:]\n",
    "df_3300['spacy_tokens'] = df_3300['text'].apply(lambda x: nlp(x))\n",
    "df_3300['lemmas'] = df_3300['spacy_tokens'].apply(lambda x: lemmatize(x))\n",
    "train_token = train_token.append(df_3300, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train_token.head())\n",
    "print(train_token.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Did not successfully remove stop words\n",
    "# try with NLTK\n",
    "\n",
    "# tokenizes much faster and removes stop words but doesn't really lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID    Gene             Variation  Class  \\\n",
      "0   0  FAM58A  Truncating Mutations      1   \n",
      "1   1     CBL                 W802*      2   \n",
      "2   2     CBL                 Q249E      2   \n",
      "3   3     CBL                 N454D      3   \n",
      "4   4     CBL                 L399V      4   \n",
      "\n",
      "                                                text  \\\n",
      "0  Cyclin-dependent kinases (CDKs) regulate a var...   \n",
      "1   Abstract Background  Non-small cell lung canc...   \n",
      "2   Abstract Background  Non-small cell lung canc...   \n",
      "3  Recent evidence has demonstrated that acquired...   \n",
      "4  Oncogenic mutations in the monomeric Casitas B...   \n",
      "\n",
      "                                         nltk_tokens  \\\n",
      "0  [Cyclin-dependent, kinases, (, CDKs, ), regula...   \n",
      "1  [Abstract, Background, Non-small, cell, lung, ...   \n",
      "2  [Abstract, Background, Non-small, cell, lung, ...   \n",
      "3  [Recent, evidence, has, demonstrated, that, ac...   \n",
      "4  [Oncogenic, mutations, in, the, monomeric, Cas...   \n",
      "\n",
      "                                        lower_tokens  \\\n",
      "0  [cyclin-dependent, kinases, (, cdks, ), regula...   \n",
      "1  [abstract, background, non-small, cell, lung, ...   \n",
      "2  [abstract, background, non-small, cell, lung, ...   \n",
      "3  [recent, evidence, has, demonstrated, that, ac...   \n",
      "4  [oncogenic, mutations, in, the, monomeric, cas...   \n",
      "\n",
      "                                      no_stop_tokens  \\\n",
      "0  [cyclin-dependent, kinases, (, cdks, ), regula...   \n",
      "1  [abstract, background, non-small, cell, lung, ...   \n",
      "2  [abstract, background, non-small, cell, lung, ...   \n",
      "3  [recent, evidence, demonstrated, acquired, uni...   \n",
      "4  [oncogenic, mutations, monomeric, casitas, b-l...   \n",
      "\n",
      "                                              lemmas  \n",
      "0  [cyclin-dependent, kinase, (, cdks, ), regulat...  \n",
      "1  [abstract, background, non-small, cell, lung, ...  \n",
      "2  [abstract, background, non-small, cell, lung, ...  \n",
      "3  [recent, evidence, demonstrated, acquired, uni...  \n",
      "4  [oncogenic, mutation, monomeric, casitas, b-li...  \n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def clean_text_nltk(df):\n",
    "    '''\n",
    "    # split into words\n",
    "    tokens = word_tokenize(text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    ??? table = str.maketrans('', '', string.punctuation)\n",
    "    ??? stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "\n",
    "    stemmed = [wordnet.lemmatize(word) for word in words]\n",
    "\n",
    "    return ' '.join(stemmed)\n",
    "    '''\n",
    "    \n",
    "    # split into tokens\n",
    "    nltk_tokens = df['text'].apply(lambda x: word_tokenize(x))\n",
    "    \n",
    "    # convert to lowercase\n",
    "    lower_tokens = nltk_tokens.apply(lambda x: [word.lower() for word in x])\n",
    "    \n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    no_stop_tokens = lower_tokens.apply(lambda x: [word for word in x if not word in stop_words])\n",
    "    \n",
    "    # lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['lemmas'] = no_stop_tokens.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    \n",
    "    return df\n",
    "    \n",
    "train_nltk = clean_text_nltk(train)\n",
    "print(train_nltk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK: Convert into a list of strings to feed to tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train_nltk['lemmas']\n",
    "Y = train_nltk['Class']\n",
    "\n",
    "X_lemma_strings = [\n",
    "    ' '.join([str(word) for word in text])\n",
    "    for text in X.values.tolist()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-ab36102c576f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# flatten into list of lists insted of nested lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_lemma_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# took over an hour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_lemma_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# flatten into list of strings insted of nested lists\n",
    "merged = set(itertools.chain.from_iterable(X_lemma_strings))\n",
    "# took over an hour \n",
    "print(len(X_lemma_strings))\n",
    "print(len(merged))\n",
    "\n",
    "common_unigrams = [Counter(merged).most_common(100000)]\n",
    "print(common_unigrams[:10])\n",
    "#tfidf_unigram = tfidf_vectorizer(common_unigrams,Y)\n",
    "#print(tfidf_unigram.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK: Oversample now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-30a85021224b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Resampled dataset shape {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_sample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \"\"\"\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_hash_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_hash_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhash_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, Y_res = sm.fit_sample(X, Y)\n",
    "print('Resampled dataset shape {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-e452028462b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# oversample now?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mX_resample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_resample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The number of transactions after resampling : '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_resample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_resample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_sample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \"\"\"\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_hash_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_hash_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhash_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/home/ubuntu/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "X = train_nltk.drop('Class', axis=1).values\n",
    "Y = train_nltk['Class'].values\n",
    "\n",
    "print(type(X))\n",
    "\n",
    "# oversample now?\n",
    "X_resample, Y_resample = SMOTE().fit_sample(X, Y)\n",
    "print('The number of transactions after resampling : ' + str(len(X_resample)))\n",
    "print(X_resample.value_count())\n",
    "\n",
    "X_lemma_strings = [\n",
    "    ' '.join([str(word) for word in text])\n",
    "    for text in X.values.tolist()\n",
    "]\n",
    "\n",
    "print(len(X_lemma_strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It doesn't like setting array elements with a sequence but i'm using array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({1: 900, 0: 100})\n",
      "Resampled dataset shape Counter({0: 900, 1: 900})\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(n_classes=2, class_sep=2,\n",
    "weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "print('Original dataset shape {}'.format(Counter(y)))\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_sample(X, y)\n",
    "print('Resampled dataset shape {}'.format(Counter(y_res)))\n",
    "\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Establishing our test datasets\n",
    "# using full spacy tokens converted to strings because lemmas are problematic\n",
    "\n",
    "#X = train_token['spacy_tokens']\n",
    "#Y = train_token['Class']\n",
    "\n",
    "#X_strings = [\n",
    "#    ' '.join([str(word) for word in text])\n",
    "#    for text in X.values.tolist()\n",
    "#]\n",
    "\n",
    "#print(len(X_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Establishing our test datasets\n",
    "# using lemmas to make processing time shorter\n",
    "\n",
    "X = train_token['lemmas']\n",
    "Y = train_token['Class']\n",
    "\n",
    "# oversample now?\n",
    "X_resample, Y_resample = SMOTE().fit_sample(X, Y)\n",
    "print 'The number of transactions after resampling : ' + str(len(X_resample))\n",
    "print(X_resample.value_count())\n",
    "\n",
    "X_lemma_strings = [\n",
    "    ' '.join([str(word) for word in text])\n",
    "    for text in X.values.tolist()\n",
    "]\n",
    "\n",
    "print(len(X_lemma_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X_lemma_strings[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-fd7745c9fafb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# flatten into list of lists insted of nested lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_lemma_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# took over an hour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_lemma_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "# flatten into list of lists insted of nested lists\n",
    "merged = list(itertools.chain.from_iterable(X_lemma_strings))\n",
    "# took over an hour \n",
    "print(len(X_lemma_strings))\n",
    "print(len(merged))\n",
    "\n",
    "common_unigrams = [Counter(X_strings).most_common(10000)]\n",
    "print(common_unigrams[:10])\n",
    "#tfidf_unigram = tfidf_vectorizer(common_unigrams,Y)\n",
    "#print(tfidf_unigram.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### Oversample now?\n",
    "\n",
    "If we oversample now, we reduce processing time from having to tokenize/lemmatize duplicates, we can just duplicate now? But then it will add more to the tf-idf processing time? Do I only oversample one category? Will SMOTE even work with non-numerical data? <br>\n",
    "\n",
    "maybe i have to oversample after vectorization? <br>\n",
    "\n",
    "Using SMOTE technique for oversampling\n",
    "\n",
    "approach: vectorize, drop features...then oversample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import imbalanced_learn as imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = train_token['lemmas']\n",
    "Y = train_token['Class']\n",
    "\n",
    "X_resampled, y_resampled = SMOTE().fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.microsoft.com/en-us/machine-learning-server/python-reference/microsoftml/featurize-text \n",
    "\n",
    "from microsoftml import rx_logistic_regression, featurize_text, rx_predict\n",
    "from microsoftml.entrypoints._stopwordsremover_predefined import predefined\n",
    "\n",
    "makes it incredibly easy but won't install\n",
    "\n",
    "what about tSNE dimensionality reduction before tf-idf vectorization? they aren't technically features yet... (use PCA first) <br>\n",
    "https://lvdmaaten.github.io/tsne/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating n-grams\n",
    "\n",
    "Beyond using singular words or lemmas as features for classification, we can use groupings of words that appear together, as they may convey more meaning than each word isolated by itself. We will create features for bigrams and trigrams, as any groupings larger than 3 words will likely be too specific and may create unnecessary noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ngrams(input_text, n):\n",
    "  input_text = input_text.split(' ')\n",
    "  output = []\n",
    "  for i in range(len(input_text)-n+1):\n",
    "    output.append(input_text[i:i+n])\n",
    "  return output\n",
    "\n",
    "bigrams = []\n",
    "\n",
    "for text in X_lemma_strings:\n",
    "    text_bigrams = ngrams(text, 2)\n",
    "    for bigram in text_bigrams:\n",
    "        bigrams.append(bigram)\n",
    "    \n",
    "print(bigrams[:1])\n",
    "# needs punctuation and stopwords removed\n",
    "# currently returns as nested, can we flatten into one list of lists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(bigrams[:10])\n",
    "print(len(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "common_bigrams = [Counter(bigrams).most_common(10000)]\n",
    "print(common_bigrams[:10])\n",
    "#tfidf_unigram = tfidf_vectorizer(common_unigrams,Y)\n",
    "#print(tfidf_unigram.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trigrams = []\n",
    "\n",
    "#for text in X_strings:\n",
    "#    text_trigrams = ngrams(text, 3)\n",
    "#    trigrams.append(text_trigrams)\n",
    "    # original syntax that returns triple nested lists\n",
    "    \n",
    "#print(trigrams[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(X,Y):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                                 min_df=2,\n",
    "                                 stop_words='english', \n",
    "                                 lowercase=True,\n",
    "                                 norm=u'l2',\n",
    "                                 smooth_idf=True,\n",
    "                                )\n",
    "\n",
    "    sparse_tfidf_matrix=vectorizer.fit_transform(X_strings)\n",
    "    print(f'Number of features: {sparse_tfidf_matrix.get_shape()[1]}')\n",
    "    \n",
    "    # Densify matrix so we can convert it to a conventional dataframe to extract X/Y\n",
    "    dense_tfidf_matrix = sparse_tfidf_matrix.todense()\n",
    "    df_tfidf = pd.DataFrame(dense_tfidf_matrix)\n",
    "    df_tfidf['lemma_tokens'] = X\n",
    "    df_tfidf['class'] = Y\n",
    "    \n",
    "    return df_tfidf\n",
    "\n",
    "tfidf_unigram = tfidf_vectorizer(X_strings,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_bigram = tfidf_vectorizer(bigrams, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning Methods\n",
    "\n",
    "Here, we will attempt to classify the training data using several different machine learning classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(tX_train, tY_train)\n",
    "test_pred = gnb.predict(tX_test)\n",
    "print(f'Testing Accuracy: {accuracy_score(tY_test, test_pred)}')\n",
    "print(f'Cross Val Score: {cross_val_score(gnb, tX_train, tY_train, cv=3).mean()}')\n",
    "print(pd.crosstab(tY_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = tfidf_vectorizer(\n",
    "                            stop_words='english', \n",
    "                            lowercase=True,\n",
    "                            norm=u'l2',\n",
    "                            smooth_idf=True\n",
    "                        )\n",
    "nb = GaussianNB()\n",
    "\n",
    "pipe = Pipeline(steps=[('tfidf', tfidf), ('nb', nb)])\n",
    "\n",
    "tfidf_maxdf = [0.25, 0.5, 0.75]\n",
    "tfidf_mindf = [2, 5, 20]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'tfidf__max_df': tfidf_maxdf,\n",
    "        'tfidf__min_df': tfidf_mindf,\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, cv=5, n_jobs=1, param_grid=param_grid)\n",
    "grid.fit(X_train, Y_train)\n",
    "#print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, decomposition, datasets\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feat_gen', tfidf_vectorizer()),\n",
    "    ('reduce_dim', decomposition.PCA())\n",
    "    ('classify', GaussianNB())\n",
    "])\n",
    "\n",
    "tfidf_maxdf = [0.25, 0.5, 0.75]\n",
    "tfidf_mindf = [2, 5, 20]\n",
    "\n",
    "param_grid_1 = [\n",
    "    {\n",
    "        'feat_gen': [tfidf_vectorizer(                            \n",
    "                                        stop_words='english', \n",
    "                                        lowercase=True,\n",
    "                                        norm=u'l2',\n",
    "                                        smooth_idf=True\n",
    "                                    )],\n",
    "        'feat_gen__max_df': tfidf_maxdf,\n",
    "        'feat_gen__min_df': tfidf_mindf,\n",
    "        'classify__C': C_OPTIONS\n",
    "    },\n",
    "    {\n",
    "        'feat_gen': [SelectKBest(chi2)],\n",
    "        'feat_gen__k': N_FEATURES_OPTIONS,\n",
    "        'classify__C': C_OPTIONS\n",
    "    },\n",
    "]\n",
    "\n",
    "param_grid_2 = dict(feat_gen=[tfidf_vectorizer(), count_vectorizer()],\n",
    "                    reduce_dim = [PCA(), tSNE()]\n",
    "                    clf = [GaussianNB()])\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, n_jobs=1, param_grid=param_grid)\n",
    "grid.fit(X, Y)\n",
    "\n",
    "##### needs to use a real class not a custom function\n",
    "##### input must be X list of strings (each string corresponds to a single document) and Y of same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
